{
    "docs": [
        {
            "location": "/index.html", 
            "text": "mlr Tutorial\n\n\nThis web page provides an in-depth introduction on how to\nuse the \nmlr\n framework for machine learning experiments in \nR\n.\n\n\nWe focus on the comprehension of the basic functions and applications.\nMore detailed technical information can be found in the \nmanual pages\n which\nare regularly updated and reflect the documentation of the \ncurrent package version on CRAN\n.\n\n\nAn offline version of this of this tutorial is available for download \nhere\n.\n\n\nThe tutorial explains the basic analysis of a data set step by step.\nPlease refer to sections of the menu above: Basics, Advanced, Extend and Appendix.\n\n\nDuring the tutorial we present various simple examples from classification, regression, cluster and\nsurvival analysis to illustrate the main features of the package.\n\n\nEnjoy reading!\n\n\nQuick start\n\n\nA simple stratified cross-validation of \nlinear discriminant analysis\n with \nmlr\n.\n\n\nlibrary(mlr)\ndata(iris)\n\n## Define the task\ntask = makeClassifTask(id = \ntutorial\n, data = iris, target = \nSpecies\n)\n\n## Define the learner\nlrn = makeLearner(\nclassif.lda\n)\n\n## Define the resampling strategy\nrdesc = makeResampleDesc(method = \nCV\n, stratify = TRUE)\n\n## Do the resampling\nr = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE)\n\n## Get the mean misclassification error\nr$aggr\n#\n mmce.test.mean \n#\n           0.02", 
            "title": "Home"
        }, 
        {
            "location": "/index.html#mlr-tutorial", 
            "text": "This web page provides an in-depth introduction on how to\nuse the  mlr  framework for machine learning experiments in  R .  We focus on the comprehension of the basic functions and applications.\nMore detailed technical information can be found in the  manual pages  which\nare regularly updated and reflect the documentation of the  current package version on CRAN .  An offline version of this of this tutorial is available for download  here .  The tutorial explains the basic analysis of a data set step by step.\nPlease refer to sections of the menu above: Basics, Advanced, Extend and Appendix.  During the tutorial we present various simple examples from classification, regression, cluster and\nsurvival analysis to illustrate the main features of the package.  Enjoy reading!", 
            "title": "mlr Tutorial"
        }, 
        {
            "location": "/index.html#quick-start", 
            "text": "A simple stratified cross-validation of  linear discriminant analysis  with  mlr .  library(mlr)\ndata(iris)\n\n## Define the task\ntask = makeClassifTask(id =  tutorial , data = iris, target =  Species )\n\n## Define the learner\nlrn = makeLearner( classif.lda )\n\n## Define the resampling strategy\nrdesc = makeResampleDesc(method =  CV , stratify = TRUE)\n\n## Do the resampling\nr = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE)\n\n## Get the mean misclassification error\nr$aggr\n#  mmce.test.mean \n#            0.02", 
            "title": "Quick start"
        }, 
        {
            "location": "/task/index.html", 
            "text": "Learning Tasks\n\n\nLearning tasks encapsulate the data set and further relevant information about a machine\nlearning problem, for example the name of the target variable.\n\n\nTask types and creation\n\n\nThe tasks are organized in a hierarchy, with the generic \nTask\n at the top.\nThe following tasks can be instantiated and all inherit from the virtual superclass \nTask\n:\n\n\n\n\nClassifTask\n for binary and multi-class classification problems\n  (normal cost-sensitive classification can be handled as well),\n\n\nRegrTask\n for regression problems,\n\n\nSurvTask\n for survival analysis,\n\n\nCostSensTask\n for general cost-sensitive classification\n  (with example specific costs),\n\n\nClusterTask\n for cluster analysis.\n\n\n\n\nTo create a task, just call \nmake\nTaskType\n, e.g., \nmakeClassifTask\n.\nAll tasks require an identifier (argument \nid\n) and a \ndata.frame\n (argument \ndata\n).\nIf no ID is provided it is automatically generated using the variable name of the data.\nIt will be later used to name results, for example of\n\nbenchmark experiments\n, or in generated plots.\nDepending on the nature of the learning problem, additional arguments may be required and\nare discussed in the following subsections.\n\n\nRegression\n\n\nFor supervised learning like regression (as well as classification and\nsurvival analysis) we, in addition to \ndata\n, have to specify the name of the \ntarget\n\nvariable.\n\n\ndata(BostonHousing, package = \nmlbench\n)\nregr.task = makeRegrTask(id = \nbh\n, data = BostonHousing, target = \nmedv\n)\nregr.task\n#\n Supervised task: bh\n#\n Type: regr\n#\n Target: medv\n#\n Observations: 506\n#\n Features:\n#\n numerics  factors  ordered \n#\n       12        1        0 \n#\n Missings: FALSE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n\n\n\n\nAs you can see, the \nTask\n records the type of the learning problem and basic information about\nthe data set, e.g., the types of the features (numeric vectors, factors or ordered factors),\nthe number of observations, or whether missing values are present.\n\n\nCreating tasks for classification problems and survival analysis follows the same scheme,\nthe data type of the target variables included in \ndata\n is simply different.\nFor each of these learning problems, some specifics are described below.\n\n\nClassification\n\n\nFor classification the target variable has to be a \nfactor\n.\n\n\nIn binary classification it is customary to refer to the two classes as \npositive\n and\n\nnegative\n class.\nThis is for example relevant for certain \nperformance measures\n\nlike the \ntrue positive rate\n.\nBy default the first factor level of the target variable is selected as the positive class.\n\n\nIn the following example, we define a classification task for the\n\nBreastCancer\n data set and exclude the variable \nId\n from all further\nmodel fitting and evaluation.\n\n\ndata(BreastCancer, package = \nmlbench\n)\ndf = BreastCancer\ndf$Id = NULL\nclassif.task = makeClassifTask(id = \nBreastCancer\n, data = df, target = \nClass\n)\nclassif.task\n#\n Supervised task: BreastCancer\n#\n Type: classif\n#\n Target: Class\n#\n Observations: 699\n#\n Features:\n#\n numerics  factors  ordered \n#\n        0        4        5 \n#\n Missings: TRUE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n#\n Classes: 2\n#\n    benign malignant \n#\n       458       241 \n#\n Positive class: benign\n\n\n\n\nThe positive class is \nbenign\n.\nClass \nmalignant\n can be manually selected as the positive class:\n\n\nclassif.task = makeClassifTask(id = \nBreastCancer\n, data = df, target = \nClass\n, positive = \nmalignant\n)\n\n\n\n\nCluster analysis\n\n\nAs cluster analysis is unsupervised, the only mandatory argument to construct a cluster analysis task is the \ndata\n.\nBelow we create a learning task from the data set \nmtcars\n.\n\n\ndata(mtcars, package = \ndatasets\n)\ncluster.task = makeClusterTask(data = mtcars)\ncluster.task\n#\n Unsupervised task: mtcars\n#\n Type: cluster\n#\n Observations: 32\n#\n Features:\n#\n numerics  factors  ordered \n#\n       11        0        0 \n#\n Missings: FALSE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n\n\n\n\nSurvival analysis\n\n\nSurvival tasks use two target columns.\nFor left and right censored problems these consist of the survival time and a binary event indicator.\nFor interval censored data the two target columns must be specified in the \n\"interval2\"\n format (see \nSurv\n).\n\n\ndata(lung, package = \nsurvival\n)\nlung$status = (lung$status == 2) # convert to logical\nsurv.task = makeSurvTask(data = lung, target = c(\ntime\n, \nstatus\n))\nsurv.task\n#\n Supervised task: lung\n#\n Type: surv\n#\n Target: time,status\n#\n Observations: 228\n#\n Features:\n#\n numerics  factors  ordered \n#\n        8        0        0 \n#\n Missings: TRUE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n\n\n\n\nThe type of censoring can be specified via the argument \ncensoring\n, which defaults to\n\n\"rcens\"\n for right censored data.\n\n\nCost-sensitive classification\n\n\nThe standard objective in classification is to obtain a high prediction accuracy, i.e., to\nminimize the number of errors. Thereby, all types of misclassification errors are deemed\nequally severe. However, in many applications different kinds of errors cause different costs.\n\n\nIn case of \nclass-dependent costs\n, that depend on the actual and predicted class labels, it\nis sufficient to create an ordinary \nClassifTask\n.\n\n\nIn order to handle \nexample-specific costs\n it is necessary to generate a \nCostSensTask\n.\nIn this scenario, each example (\nx\n, \ny\n) is associated with an individual cost vector of length\n\nK\n where \nK\n denotes the number of classes. The \nk\n-th component indicates the cost of assigning\n\nx\n to class \nk\n. Naturally, it is assumed that the cost of the intended class label \ny\n is\nminimal.\n\n\nAs the cost vector contains all relevant information about the intended class label \ny\n, only\nthe feature values \nx\n and a \ncost\n matrix, which contains the cost vectors for all examples in the data\nset, are required to create the \nCostSensTask\n.\n\n\nIn the following example we use the \niris data\n and generate an artificial cost matrix\n(following \nBeygelzimer et al., 2005\n):\n\n\ndf = iris\ncost = matrix(runif(150 * 3, 0, 2000), 150) * (1 - diag(3))[df$Species,]\ndf$Species = NULL\n\ncostsens.task = makeCostSensTask(data = df, cost = cost)\ncostsens.task\n#\n Supervised task: df\n#\n Type: costsens\n#\n Observations: 150\n#\n Features:\n#\n numerics  factors  ordered \n#\n        4        0        0 \n#\n Missings: FALSE\n#\n Has blocking: FALSE\n#\n Classes: 3\n#\n y1, y2, y3\n\n\n\n\nFor more details see the section about \ncost-sensitive classification\n.\n\n\nFurther settings\n\n\nThe \nTask\n help page also lists several other arguments to describe further details of the\nlearning problem.\n\n\nFor example, we could include a \nblocking\n factor into the task.\nThis would tell the task that some observations \"belong together\" and should\nnot be separated when splitting into training and test sets during a resampling iteration.\n\n\nAnother possibility is to assign \nweights\n to observations according to their importance.\n\n\nAccessing a learning task\n\n\nWe provide many operators to access the elements stored in a \nTask\n.\nFor example, to access a \ntask description\n you can use\n\n\ngetTaskDescription(regr.task)\n#\n $id\n#\n [1] \nbh\n\n#\n \n#\n $type\n#\n [1] \nregr\n\n#\n \n#\n $target\n#\n [1] \nmedv\n\n#\n \n#\n $size\n#\n [1] 506\n#\n \n#\n $n.feat\n#\n numerics  factors  ordered \n#\n       12        1        0 \n#\n \n#\n $has.missings\n#\n [1] FALSE\n#\n \n#\n $has.weights\n#\n [1] FALSE\n#\n \n#\n $has.blocking\n#\n [1] FALSE\n#\n \n#\n attr(,\nclass\n)\n#\n [1] \nTaskDescRegr\n \nTaskDesc\n\n\n\n\n\nThe most important operations are listed in the documentation of \nTask\n.\nHere are some more examples.\n\n\n## Accessing the data set in classif.task\nstr(getTaskData(classif.task))\n#\n 'data.frame':    699 obs. of  10 variables:\n#\n  $ Cl.thickness   : Ord.factor w/ 10 levels \n1\n2\n3\n4\n..: 5 5 3 6 4 8 1 2 2 4 ...\n#\n  $ Cell.size      : Ord.factor w/ 10 levels \n1\n2\n3\n4\n..: 1 4 1 8 1 10 1 1 1 2 ...\n#\n  $ Cell.shape     : Ord.factor w/ 10 levels \n1\n2\n3\n4\n..: 1 4 1 8 1 10 1 2 1 1 ...\n#\n  $ Marg.adhesion  : Ord.factor w/ 10 levels \n1\n2\n3\n4\n..: 1 5 1 1 3 8 1 1 1 1 ...\n#\n  $ Epith.c.size   : Ord.factor w/ 10 levels \n1\n2\n3\n4\n..: 2 7 2 3 2 7 2 2 2 2 ...\n#\n  $ Bare.nuclei    : Factor w/ 10 levels \n1\n,\n2\n,\n3\n,\n4\n,..: 1 10 2 4 1 10 10 1 1 1 ...\n#\n  $ Bl.cromatin    : Factor w/ 10 levels \n1\n,\n2\n,\n3\n,\n4\n,..: 3 3 3 3 3 9 3 3 1 2 ...\n#\n  $ Normal.nucleoli: Factor w/ 10 levels \n1\n,\n2\n,\n3\n,\n4\n,..: 1 2 1 7 1 7 1 1 1 1 ...\n#\n  $ Mitoses        : Factor w/ 9 levels \n1\n,\n2\n,\n3\n,\n4\n,..: 1 1 1 1 1 1 1 1 5 1 ...\n#\n  $ Class          : Factor w/ 2 levels \nbenign\n,\nmalignant\n: 1 1 1 1 1 2 1 1 1 1 ...\n\n## Get the number of observations in classif.task\ngetTaskSize(classif.task)\n#\n [1] 699\n\n## Get the number of input variables in cluster.task\ngetTaskNFeats(cluster.task)\n#\n [1] 11\n\n## Get the names of the input variables in cluster.task\ngetTaskFeatureNames(cluster.task)\n#\n  [1] \nmpg\n  \ncyl\n  \ndisp\n \nhp\n   \ndrat\n \nwt\n   \nqsec\n \nvs\n   \nam\n   \ngear\n\n#\n [11] \ncarb\n\n\n## Get the names of the target columns\ngetTaskTargetNames(surv.task)\n#\n [1] \ntime\n   \nstatus\n\n\n## Get the values of the target variable in regr.task\nhead(getTaskTargets(regr.task))\n#\n [1] 24.0 21.6 34.7 33.4 36.2 28.7\n\n## Get the cost matrix in costsens.task\nhead(getTaskCosts(costsens.task))\n#\n      y1        y2         y3\n#\n [1,]  0 1589.5664  674.44434\n#\n [2,]  0 1173.4364  828.40682\n#\n [3,]  0  942.7611 1095.33713\n#\n [4,]  0 1049.5562  477.82496\n#\n [5,]  0 1121.8899   90.85237\n#\n [6,]  0 1819.9830  841.06686\n\n\n\n\nNote the many options that \ngetTaskData\n provides to convert the data set into a convenient format.\nThis especially comes in handy when you \nintegrate a learner\n from another \nR\n\npackage into \nmlr\n.\nIn this regard the functions \ngetTaskFormula\n and \ngetTaskFormulaAsString\n\nare also useful.\n\n\nModifying a learning task\n\n\nmlr\n provides several functions to alter an existing \nTask\n which is often more\nconvenient than creating a new \nTask\n from scratch.\nHere are some examples.\n\n\n## Select observations and features\ncluster.task = subsetTask(cluster.task, subset = 4:17)\n\n## It may happen, especially after selecting observations, that features are constant.\n## These should be removed.\nremoveConstantFeatures(cluster.task)\n#\n Removing 1 columns: am\n#\n Unsupervised task: mtcars\n#\n Type: cluster\n#\n Observations: 14\n#\n Features:\n#\n numerics  factors  ordered \n#\n       10        0        0 \n#\n Missings: FALSE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n\n## Remove selected features\ndropFeatures(surv.task, c(\nmeal.cal\n, \nwt.loss\n))\n#\n Supervised task: lung\n#\n Type: surv\n#\n Target: time,status\n#\n Observations: 228\n#\n Features:\n#\n numerics  factors  ordered \n#\n        6        0        0 \n#\n Missings: TRUE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n\n## Standardize numerical features\ntask = normalizeFeatures(cluster.task, method = \nrange\n)\nsummary(getTaskData(task))\n#\n       mpg              cyl              disp              hp        \n#\n  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n#\n  1st Qu.:0.3161   1st Qu.:0.5000   1st Qu.:0.1242   1st Qu.:0.2801  \n#\n  Median :0.5107   Median :1.0000   Median :0.4076   Median :0.6311  \n#\n  Mean   :0.4872   Mean   :0.7143   Mean   :0.4430   Mean   :0.5308  \n#\n  3rd Qu.:0.6196   3rd Qu.:1.0000   3rd Qu.:0.6618   3rd Qu.:0.7473  \n#\n  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n#\n       drat              wt              qsec              vs        \n#\n  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n#\n  1st Qu.:0.2672   1st Qu.:0.1275   1st Qu.:0.2302   1st Qu.:0.0000  \n#\n  Median :0.3060   Median :0.1605   Median :0.3045   Median :0.0000  \n#\n  Mean   :0.4544   Mean   :0.3268   Mean   :0.3752   Mean   :0.4286  \n#\n  3rd Qu.:0.7026   3rd Qu.:0.3727   3rd Qu.:0.4908   3rd Qu.:1.0000  \n#\n  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n#\n        am           gear             carb       \n#\n  Min.   :0.5   Min.   :0.0000   Min.   :0.0000  \n#\n  1st Qu.:0.5   1st Qu.:0.0000   1st Qu.:0.3333  \n#\n  Median :0.5   Median :0.0000   Median :0.6667  \n#\n  Mean   :0.5   Mean   :0.2857   Mean   :0.6429  \n#\n  3rd Qu.:0.5   3rd Qu.:0.7500   3rd Qu.:1.0000  \n#\n  Max.   :0.5   Max.   :1.0000   Max.   :1.0000\n\n\n\n\nSome of these functions are explained in more detail in the \ndata preprocessing\n section.\n\n\nExample tasks\n\n\nFor your convenience \nmlr\n provides pre-defined \ntasks\n for each type of learning problem.\nThese are used throughout this tutorial in order to get shorter and more readable code.\nA list of all \ntasks\n can be found \nhere\n.", 
            "title": "Tasks"
        }, 
        {
            "location": "/task/index.html#learning-tasks", 
            "text": "Learning tasks encapsulate the data set and further relevant information about a machine\nlearning problem, for example the name of the target variable.", 
            "title": "Learning Tasks"
        }, 
        {
            "location": "/task/index.html#task-types-and-creation", 
            "text": "The tasks are organized in a hierarchy, with the generic  Task  at the top.\nThe following tasks can be instantiated and all inherit from the virtual superclass  Task :   ClassifTask  for binary and multi-class classification problems\n  (normal cost-sensitive classification can be handled as well),  RegrTask  for regression problems,  SurvTask  for survival analysis,  CostSensTask  for general cost-sensitive classification\n  (with example specific costs),  ClusterTask  for cluster analysis.   To create a task, just call  make TaskType , e.g.,  makeClassifTask .\nAll tasks require an identifier (argument  id ) and a  data.frame  (argument  data ).\nIf no ID is provided it is automatically generated using the variable name of the data.\nIt will be later used to name results, for example of benchmark experiments , or in generated plots.\nDepending on the nature of the learning problem, additional arguments may be required and\nare discussed in the following subsections.  Regression  For supervised learning like regression (as well as classification and\nsurvival analysis) we, in addition to  data , have to specify the name of the  target \nvariable.  data(BostonHousing, package =  mlbench )\nregr.task = makeRegrTask(id =  bh , data = BostonHousing, target =  medv )\nregr.task\n#  Supervised task: bh\n#  Type: regr\n#  Target: medv\n#  Observations: 506\n#  Features:\n#  numerics  factors  ordered \n#        12        1        0 \n#  Missings: FALSE\n#  Has weights: FALSE\n#  Has blocking: FALSE  As you can see, the  Task  records the type of the learning problem and basic information about\nthe data set, e.g., the types of the features (numeric vectors, factors or ordered factors),\nthe number of observations, or whether missing values are present.  Creating tasks for classification problems and survival analysis follows the same scheme,\nthe data type of the target variables included in  data  is simply different.\nFor each of these learning problems, some specifics are described below.  Classification  For classification the target variable has to be a  factor .  In binary classification it is customary to refer to the two classes as  positive  and negative  class.\nThis is for example relevant for certain  performance measures \nlike the  true positive rate .\nBy default the first factor level of the target variable is selected as the positive class.  In the following example, we define a classification task for the BreastCancer  data set and exclude the variable  Id  from all further\nmodel fitting and evaluation.  data(BreastCancer, package =  mlbench )\ndf = BreastCancer\ndf$Id = NULL\nclassif.task = makeClassifTask(id =  BreastCancer , data = df, target =  Class )\nclassif.task\n#  Supervised task: BreastCancer\n#  Type: classif\n#  Target: Class\n#  Observations: 699\n#  Features:\n#  numerics  factors  ordered \n#         0        4        5 \n#  Missings: TRUE\n#  Has weights: FALSE\n#  Has blocking: FALSE\n#  Classes: 2\n#     benign malignant \n#        458       241 \n#  Positive class: benign  The positive class is  benign .\nClass  malignant  can be manually selected as the positive class:  classif.task = makeClassifTask(id =  BreastCancer , data = df, target =  Class , positive =  malignant )  Cluster analysis  As cluster analysis is unsupervised, the only mandatory argument to construct a cluster analysis task is the  data .\nBelow we create a learning task from the data set  mtcars .  data(mtcars, package =  datasets )\ncluster.task = makeClusterTask(data = mtcars)\ncluster.task\n#  Unsupervised task: mtcars\n#  Type: cluster\n#  Observations: 32\n#  Features:\n#  numerics  factors  ordered \n#        11        0        0 \n#  Missings: FALSE\n#  Has weights: FALSE\n#  Has blocking: FALSE  Survival analysis  Survival tasks use two target columns.\nFor left and right censored problems these consist of the survival time and a binary event indicator.\nFor interval censored data the two target columns must be specified in the  \"interval2\"  format (see  Surv ).  data(lung, package =  survival )\nlung$status = (lung$status == 2) # convert to logical\nsurv.task = makeSurvTask(data = lung, target = c( time ,  status ))\nsurv.task\n#  Supervised task: lung\n#  Type: surv\n#  Target: time,status\n#  Observations: 228\n#  Features:\n#  numerics  factors  ordered \n#         8        0        0 \n#  Missings: TRUE\n#  Has weights: FALSE\n#  Has blocking: FALSE  The type of censoring can be specified via the argument  censoring , which defaults to \"rcens\"  for right censored data.  Cost-sensitive classification  The standard objective in classification is to obtain a high prediction accuracy, i.e., to\nminimize the number of errors. Thereby, all types of misclassification errors are deemed\nequally severe. However, in many applications different kinds of errors cause different costs.  In case of  class-dependent costs , that depend on the actual and predicted class labels, it\nis sufficient to create an ordinary  ClassifTask .  In order to handle  example-specific costs  it is necessary to generate a  CostSensTask .\nIn this scenario, each example ( x ,  y ) is associated with an individual cost vector of length K  where  K  denotes the number of classes. The  k -th component indicates the cost of assigning x  to class  k . Naturally, it is assumed that the cost of the intended class label  y  is\nminimal.  As the cost vector contains all relevant information about the intended class label  y , only\nthe feature values  x  and a  cost  matrix, which contains the cost vectors for all examples in the data\nset, are required to create the  CostSensTask .  In the following example we use the  iris data  and generate an artificial cost matrix\n(following  Beygelzimer et al., 2005 ):  df = iris\ncost = matrix(runif(150 * 3, 0, 2000), 150) * (1 - diag(3))[df$Species,]\ndf$Species = NULL\n\ncostsens.task = makeCostSensTask(data = df, cost = cost)\ncostsens.task\n#  Supervised task: df\n#  Type: costsens\n#  Observations: 150\n#  Features:\n#  numerics  factors  ordered \n#         4        0        0 \n#  Missings: FALSE\n#  Has blocking: FALSE\n#  Classes: 3\n#  y1, y2, y3  For more details see the section about  cost-sensitive classification .", 
            "title": "Task types and creation"
        }, 
        {
            "location": "/task/index.html#further-settings", 
            "text": "The  Task  help page also lists several other arguments to describe further details of the\nlearning problem.  For example, we could include a  blocking  factor into the task.\nThis would tell the task that some observations \"belong together\" and should\nnot be separated when splitting into training and test sets during a resampling iteration.  Another possibility is to assign  weights  to observations according to their importance.", 
            "title": "Further settings"
        }, 
        {
            "location": "/task/index.html#accessing-a-learning-task", 
            "text": "We provide many operators to access the elements stored in a  Task .\nFor example, to access a  task description  you can use  getTaskDescription(regr.task)\n#  $id\n#  [1]  bh \n#  \n#  $type\n#  [1]  regr \n#  \n#  $target\n#  [1]  medv \n#  \n#  $size\n#  [1] 506\n#  \n#  $n.feat\n#  numerics  factors  ordered \n#        12        1        0 \n#  \n#  $has.missings\n#  [1] FALSE\n#  \n#  $has.weights\n#  [1] FALSE\n#  \n#  $has.blocking\n#  [1] FALSE\n#  \n#  attr(, class )\n#  [1]  TaskDescRegr   TaskDesc   The most important operations are listed in the documentation of  Task .\nHere are some more examples.  ## Accessing the data set in classif.task\nstr(getTaskData(classif.task))\n#  'data.frame':    699 obs. of  10 variables:\n#   $ Cl.thickness   : Ord.factor w/ 10 levels  1 2 3 4 ..: 5 5 3 6 4 8 1 2 2 4 ...\n#   $ Cell.size      : Ord.factor w/ 10 levels  1 2 3 4 ..: 1 4 1 8 1 10 1 1 1 2 ...\n#   $ Cell.shape     : Ord.factor w/ 10 levels  1 2 3 4 ..: 1 4 1 8 1 10 1 2 1 1 ...\n#   $ Marg.adhesion  : Ord.factor w/ 10 levels  1 2 3 4 ..: 1 5 1 1 3 8 1 1 1 1 ...\n#   $ Epith.c.size   : Ord.factor w/ 10 levels  1 2 3 4 ..: 2 7 2 3 2 7 2 2 2 2 ...\n#   $ Bare.nuclei    : Factor w/ 10 levels  1 , 2 , 3 , 4 ,..: 1 10 2 4 1 10 10 1 1 1 ...\n#   $ Bl.cromatin    : Factor w/ 10 levels  1 , 2 , 3 , 4 ,..: 3 3 3 3 3 9 3 3 1 2 ...\n#   $ Normal.nucleoli: Factor w/ 10 levels  1 , 2 , 3 , 4 ,..: 1 2 1 7 1 7 1 1 1 1 ...\n#   $ Mitoses        : Factor w/ 9 levels  1 , 2 , 3 , 4 ,..: 1 1 1 1 1 1 1 1 5 1 ...\n#   $ Class          : Factor w/ 2 levels  benign , malignant : 1 1 1 1 1 2 1 1 1 1 ...\n\n## Get the number of observations in classif.task\ngetTaskSize(classif.task)\n#  [1] 699\n\n## Get the number of input variables in cluster.task\ngetTaskNFeats(cluster.task)\n#  [1] 11\n\n## Get the names of the input variables in cluster.task\ngetTaskFeatureNames(cluster.task)\n#   [1]  mpg    cyl    disp   hp     drat   wt     qsec   vs     am     gear \n#  [11]  carb \n\n## Get the names of the target columns\ngetTaskTargetNames(surv.task)\n#  [1]  time     status \n\n## Get the values of the target variable in regr.task\nhead(getTaskTargets(regr.task))\n#  [1] 24.0 21.6 34.7 33.4 36.2 28.7\n\n## Get the cost matrix in costsens.task\nhead(getTaskCosts(costsens.task))\n#       y1        y2         y3\n#  [1,]  0 1589.5664  674.44434\n#  [2,]  0 1173.4364  828.40682\n#  [3,]  0  942.7611 1095.33713\n#  [4,]  0 1049.5562  477.82496\n#  [5,]  0 1121.8899   90.85237\n#  [6,]  0 1819.9830  841.06686  Note the many options that  getTaskData  provides to convert the data set into a convenient format.\nThis especially comes in handy when you  integrate a learner  from another  R \npackage into  mlr .\nIn this regard the functions  getTaskFormula  and  getTaskFormulaAsString \nare also useful.", 
            "title": "Accessing a learning task"
        }, 
        {
            "location": "/task/index.html#modifying-a-learning-task", 
            "text": "mlr  provides several functions to alter an existing  Task  which is often more\nconvenient than creating a new  Task  from scratch.\nHere are some examples.  ## Select observations and features\ncluster.task = subsetTask(cluster.task, subset = 4:17)\n\n## It may happen, especially after selecting observations, that features are constant.\n## These should be removed.\nremoveConstantFeatures(cluster.task)\n#  Removing 1 columns: am\n#  Unsupervised task: mtcars\n#  Type: cluster\n#  Observations: 14\n#  Features:\n#  numerics  factors  ordered \n#        10        0        0 \n#  Missings: FALSE\n#  Has weights: FALSE\n#  Has blocking: FALSE\n\n## Remove selected features\ndropFeatures(surv.task, c( meal.cal ,  wt.loss ))\n#  Supervised task: lung\n#  Type: surv\n#  Target: time,status\n#  Observations: 228\n#  Features:\n#  numerics  factors  ordered \n#         6        0        0 \n#  Missings: TRUE\n#  Has weights: FALSE\n#  Has blocking: FALSE\n\n## Standardize numerical features\ntask = normalizeFeatures(cluster.task, method =  range )\nsummary(getTaskData(task))\n#        mpg              cyl              disp              hp        \n#   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n#   1st Qu.:0.3161   1st Qu.:0.5000   1st Qu.:0.1242   1st Qu.:0.2801  \n#   Median :0.5107   Median :1.0000   Median :0.4076   Median :0.6311  \n#   Mean   :0.4872   Mean   :0.7143   Mean   :0.4430   Mean   :0.5308  \n#   3rd Qu.:0.6196   3rd Qu.:1.0000   3rd Qu.:0.6618   3rd Qu.:0.7473  \n#   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n#        drat              wt              qsec              vs        \n#   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n#   1st Qu.:0.2672   1st Qu.:0.1275   1st Qu.:0.2302   1st Qu.:0.0000  \n#   Median :0.3060   Median :0.1605   Median :0.3045   Median :0.0000  \n#   Mean   :0.4544   Mean   :0.3268   Mean   :0.3752   Mean   :0.4286  \n#   3rd Qu.:0.7026   3rd Qu.:0.3727   3rd Qu.:0.4908   3rd Qu.:1.0000  \n#   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n#         am           gear             carb       \n#   Min.   :0.5   Min.   :0.0000   Min.   :0.0000  \n#   1st Qu.:0.5   1st Qu.:0.0000   1st Qu.:0.3333  \n#   Median :0.5   Median :0.0000   Median :0.6667  \n#   Mean   :0.5   Mean   :0.2857   Mean   :0.6429  \n#   3rd Qu.:0.5   3rd Qu.:0.7500   3rd Qu.:1.0000  \n#   Max.   :0.5   Max.   :1.0000   Max.   :1.0000  Some of these functions are explained in more detail in the  data preprocessing  section.", 
            "title": "Modifying a learning task"
        }, 
        {
            "location": "/task/index.html#example-tasks", 
            "text": "For your convenience  mlr  provides pre-defined  tasks  for each type of learning problem.\nThese are used throughout this tutorial in order to get shorter and more readable code.\nA list of all  tasks  can be found  here .", 
            "title": "Example tasks"
        }, 
        {
            "location": "/learner/index.html", 
            "text": "Learners\n\n\nThe following classes provide a unified interface to all popular machine learning methods in \nR\n:\n(cost-sensitive) classification, regression, survival analysis, and clustering.\nMany are already integrated in \nmlr\n, others are not, but the package is specifically designed to make extensions simple.\n\n\nSection \nintegrated learners\n shows the already\nimplemented machine learning methods and their properties.\nIf your favorite method is missing, either \nopen an issue\n or take\na look at \nhow to integrate a learning method yourself\n.\nThis basic introduction demonstrates how to use already implemented learners.\n\n\nConstructing a learner\n\n\nA learner in \nmlr\n is generated by calling \nmakeLearner\n.\nIn the constructor you need to specify which learning method you want to use.\nMoreover, you can:\n\n\n\n\nSet hyperparameters.\n\n\nControl the output for later prediction, e.g., for classification\n  whether you want a factor of predicted class labels or probabilities.\n\n\nSet an ID to name the object (some methods will later use this ID to name results or annotate plots).\n\n\n\n\n## Classification tree, set it up for predicting probabilities\nclassif.lrn = makeLearner(\nclassif.randomForest\n, predict.type = \nprob\n, fix.factors.prediction = TRUE)\n\n## Regression gradient boosting machine, specify hyperparameters via a list\nregr.lrn = makeLearner(\nregr.gbm\n, par.vals = list(n.trees = 500, interaction.depth = 3))\n\n## Cox proportional hazards model with custom name\nsurv.lrn = makeLearner(\nsurv.coxph\n, id = \ncph\n)\n\n## K-means with 5 clusters\ncluster.lrn = makeLearner(\ncluster.SimpleKMeans\n, N = 5)\n\n\n\n\nThe first argument specifies which algorithm to use.\nThe naming convention is \nclassif.\nR_method_name\n for\nclassification methods, \nregr.\nR_method_name\n for regression methods,\n\nsurv.\nR_method_name\n for survival analysis, and \ncluster.\nR_method_name\n\nfor clustering methods.\n\n\nHyperparameter values can be specified either via the \n...\n argument or as a \nlist\n\nvia \npar.vals\n.\n\n\nOccasionally, \nfactor\n features may cause problems when fewer levels are present in the\ntest data set than in the training data.\nBy setting \nfix.factors.prediction = TRUE\n these are avoided by adding a factor level for missing data in the test data set.\n\n\nLet's have a look at two of the learners created above.\n\n\nclassif.lrn\n#\n Learner classif.randomForest from package randomForest\n#\n Type: classif\n#\n Name: Random Forest; Short name: rf\n#\n Class: classif.randomForest\n#\n Properties: twoclass,multiclass,numerics,factors,ordered,prob\n#\n Predict-Type: prob\n#\n Hyperparameters:\n\nsurv.lrn\n#\n Learner cph from package survival\n#\n Type: surv\n#\n Name: Cox Proportional Hazard Model; Short name: coxph\n#\n Class: surv.coxph\n#\n Properties: missings,numerics,factors,weights,prob,rcens\n#\n Predict-Type: response\n#\n Hyperparameters:\n\n\n\n\nAll generated learners are objects of class \nLearner\n.\nThis class contains the properties of the method, e.g., which types of features it can handle,\nwhat kind of output is possible during prediction, and whether multi-class problems,\nobservations weights or missing values are supported.\n\n\nAs you might have noticed, there is currently no special learner class for cost-sensitive classification.\nFor ordinary misclassification costs you can use standard classification methods.\nFor example-dependent costs there are several ways to generate cost-sensitive learners from ordinary\nregression and classification learners. This is explained in greater detail in the section about\n\ncost-sensitive classification\n.\n\n\nAccessing a learner\n\n\nThe \nLearner\n object is a \nlist\n and the following elements contain\ninformation regarding the hyperparameters and the type of prediction.\n\n\n## Get the configured hyperparameter settings that deviate from the defaults\ncluster.lrn$par.vals\n#\n $N\n#\n [1] 5\n\n## Get the set of hyperparameters\nclassif.lrn$par.set\n#\n                     Type  len   Def   Constr Req Tunable Trafo\n#\n ntree            integer    -   500 1 to Inf   -    TRUE     -\n#\n mtry             integer    -     - 1 to Inf   -    TRUE     -\n#\n replace          logical    -  TRUE        -   -    TRUE     -\n#\n classwt    numericvector \nNA\n     - 0 to Inf   -    TRUE     -\n#\n cutoff     numericvector \nNA\n     -   0 to 1   -    TRUE     -\n#\n sampsize   integervector \nNA\n     - 0 to Inf   -    TRUE     -\n#\n nodesize         integer    -     1 1 to Inf   -    TRUE     -\n#\n maxnodes         integer    -     - 1 to Inf   -    TRUE     -\n#\n importance       logical    - FALSE        -   -    TRUE     -\n#\n localImp         logical    - FALSE        -   -    TRUE     -\n#\n norm.votes       logical    -  TRUE        -   -    TRUE     -\n#\n keep.inbag       logical    - FALSE        -   -    TRUE     -\n\n## Get the type of prediction\nregr.lrn$predict.type\n#\n [1] \nresponse\n\n\n\n\n\nSlot \n$par.set\n is an object of class \nParamSet\n.\nIt contains, among others, the type of hyperparameters (e.g., numeric, logical), potential\ndefault values and the range of allowed values.\n\n\nMoreover, \nmlr\n provides function \ngetHyperPars\n to access the current hyperparameter setting\nof a \nLearner\n and \ngetParamSet\n to get a description of all possible settings.\nThese are particularly useful in case of wrapped \nLearner\ns,\nfor example if a learner is fused with a feature selection strategy, and both, the learner\nas well the feature selection method, have hyperparameters.\nFor details see the section on \nwrapped learners\n.\n\n\n## Get current hyperparameter settings\ngetHyperPars(cluster.lrn)\n#\n $N\n#\n [1] 5\n\n## Get a description of all possible hyperparameter settings\ngetParamSet(classif.lrn)\n#\n                     Type  len   Def   Constr Req Tunable Trafo\n#\n ntree            integer    -   500 1 to Inf   -    TRUE     -\n#\n mtry             integer    -     - 1 to Inf   -    TRUE     -\n#\n replace          logical    -  TRUE        -   -    TRUE     -\n#\n classwt    numericvector \nNA\n     - 0 to Inf   -    TRUE     -\n#\n cutoff     numericvector \nNA\n     -   0 to 1   -    TRUE     -\n#\n sampsize   integervector \nNA\n     - 0 to Inf   -    TRUE     -\n#\n nodesize         integer    -     1 1 to Inf   -    TRUE     -\n#\n maxnodes         integer    -     - 1 to Inf   -    TRUE     -\n#\n importance       logical    - FALSE        -   -    TRUE     -\n#\n localImp         logical    - FALSE        -   -    TRUE     -\n#\n norm.votes       logical    -  TRUE        -   -    TRUE     -\n#\n keep.inbag       logical    - FALSE        -   -    TRUE     -\n\n\n\n\nWe can also use \ngetParamSet\n to get a quick overview about the available hyperparameters\nand defaults of a learning method without explicitly constructing it (by calling \nmakeLearner\n).\n\n\ngetParamSet(\nclassif.randomForest\n)\n#\n                     Type  len   Def   Constr Req Tunable Trafo\n#\n ntree            integer    -   500 1 to Inf   -    TRUE     -\n#\n mtry             integer    -     - 1 to Inf   -    TRUE     -\n#\n replace          logical    -  TRUE        -   -    TRUE     -\n#\n classwt    numericvector \nNA\n     - 0 to Inf   -    TRUE     -\n#\n cutoff     numericvector \nNA\n     -   0 to 1   -    TRUE     -\n#\n sampsize   integervector \nNA\n     - 0 to Inf   -    TRUE     -\n#\n nodesize         integer    -     1 1 to Inf   -    TRUE     -\n#\n maxnodes         integer    -     - 1 to Inf   -    TRUE     -\n#\n importance       logical    - FALSE        -   -    TRUE     -\n#\n localImp         logical    - FALSE        -   -    TRUE     -\n#\n norm.votes       logical    -  TRUE        -   -    TRUE     -\n#\n keep.inbag       logical    - FALSE        -   -    TRUE     -\n\n\n\n\nModifying a learner\n\n\nThere are also some functions that enable you to change certain aspects\nof a \nLearner\n without needing to create a new \nLearner\n from scratch.\nHere are some examples.\n\n\n## Change the ID\nsurv.lrn = setId(surv.lrn, \nCoxModel\n)\nsurv.lrn\n#\n Learner CoxModel from package survival\n#\n Type: surv\n#\n Name: Cox Proportional Hazard Model; Short name: coxph\n#\n Class: surv.coxph\n#\n Properties: missings,numerics,factors,weights,prob,rcens\n#\n Predict-Type: response\n#\n Hyperparameters:\n\n## Change the prediction type, predict a factor with class labels instead of probabilities\nclassif.lrn = setPredictType(classif.lrn, \nresponse\n)\n\n## Change hyperparameter values\ncluster.lrn = setHyperPars(cluster.lrn, N = 4)\n\n## Go back to default hyperparameter values\nregr.lrn = removeHyperPars(regr.lrn, c(\nn.trees\n, \ninteraction.depth\n))\n\n\n\n\nListing learners\n\n\nA list of all learners integrated in \nmlr\n and their respective properties is shown in the \nAppendix\n.\n\n\nIf you would like a list of available learners, maybe only with certain properties or suitable for a\ncertain learning \nTask\n use function \nlistLearners\n.\n\n\n## List everything in mlr\nhead(listLearners())\n#\n           classif.ada   classif.bartMachine           classif.bdk \n#\n         \nclassif.ada\n \nclassif.bartMachine\n         \nclassif.bdk\n \n#\n      classif.binomial    classif.blackboost      classif.boosting \n#\n    \nclassif.binomial\n  \nclassif.blackboost\n    \nclassif.boosting\n\n\n## List classifiers that can output probabilities\nhead(listLearners(\nclassif\n, properties = \nprob\n))\n#\n           classif.ada   classif.bartMachine           classif.bdk \n#\n         \nclassif.ada\n \nclassif.bartMachine\n         \nclassif.bdk\n \n#\n      classif.binomial    classif.blackboost      classif.boosting \n#\n    \nclassif.binomial\n  \nclassif.blackboost\n    \nclassif.boosting\n\n\n## List classifiers that can be applied to iris (i.e., multiclass) and output probabilities\nhead(listLearners(iris.task, properties = \nprob\n))\n#\n          classif.bdk     classif.boosting      classif.cforest \n#\n        \nclassif.bdk\n   \nclassif.boosting\n    \nclassif.cforest\n \n#\n        classif.ctree   classif.extraTrees          classif.gbm \n#\n      \nclassif.ctree\n \nclassif.extraTrees\n        \nclassif.gbm\n\n\n## The calls above return character vectors, but you can also create learner objects\nhead(listLearners(\ncluster\n, create = TRUE), 2)\n#\n $cluster.cmeans\n#\n Learner cluster.cmeans from package e1071,clue\n#\n Type: cluster\n#\n Name: Fuzzy C-Means Clustering; Short name: cmeans\n#\n Class: cluster.cmeans\n#\n Properties: numerics,prob\n#\n Predict-Type: response\n#\n Hyperparameters: centers=\ninteger\n\n#\n \n#\n \n#\n $cluster.EM\n#\n Learner cluster.EM from package RWeka\n#\n Type: cluster\n#\n Name: Expectation-Maximization Clustering; Short name: em\n#\n Class: cluster.EM\n#\n Properties: numerics\n#\n Predict-Type: response\n#\n Hyperparameters:", 
            "title": "Learners"
        }, 
        {
            "location": "/learner/index.html#learners", 
            "text": "The following classes provide a unified interface to all popular machine learning methods in  R :\n(cost-sensitive) classification, regression, survival analysis, and clustering.\nMany are already integrated in  mlr , others are not, but the package is specifically designed to make extensions simple.  Section  integrated learners  shows the already\nimplemented machine learning methods and their properties.\nIf your favorite method is missing, either  open an issue  or take\na look at  how to integrate a learning method yourself .\nThis basic introduction demonstrates how to use already implemented learners.", 
            "title": "Learners"
        }, 
        {
            "location": "/learner/index.html#constructing-a-learner", 
            "text": "A learner in  mlr  is generated by calling  makeLearner .\nIn the constructor you need to specify which learning method you want to use.\nMoreover, you can:   Set hyperparameters.  Control the output for later prediction, e.g., for classification\n  whether you want a factor of predicted class labels or probabilities.  Set an ID to name the object (some methods will later use this ID to name results or annotate plots).   ## Classification tree, set it up for predicting probabilities\nclassif.lrn = makeLearner( classif.randomForest , predict.type =  prob , fix.factors.prediction = TRUE)\n\n## Regression gradient boosting machine, specify hyperparameters via a list\nregr.lrn = makeLearner( regr.gbm , par.vals = list(n.trees = 500, interaction.depth = 3))\n\n## Cox proportional hazards model with custom name\nsurv.lrn = makeLearner( surv.coxph , id =  cph )\n\n## K-means with 5 clusters\ncluster.lrn = makeLearner( cluster.SimpleKMeans , N = 5)  The first argument specifies which algorithm to use.\nThe naming convention is  classif. R_method_name  for\nclassification methods,  regr. R_method_name  for regression methods, surv. R_method_name  for survival analysis, and  cluster. R_method_name \nfor clustering methods.  Hyperparameter values can be specified either via the  ...  argument or as a  list \nvia  par.vals .  Occasionally,  factor  features may cause problems when fewer levels are present in the\ntest data set than in the training data.\nBy setting  fix.factors.prediction = TRUE  these are avoided by adding a factor level for missing data in the test data set.  Let's have a look at two of the learners created above.  classif.lrn\n#  Learner classif.randomForest from package randomForest\n#  Type: classif\n#  Name: Random Forest; Short name: rf\n#  Class: classif.randomForest\n#  Properties: twoclass,multiclass,numerics,factors,ordered,prob\n#  Predict-Type: prob\n#  Hyperparameters:\n\nsurv.lrn\n#  Learner cph from package survival\n#  Type: surv\n#  Name: Cox Proportional Hazard Model; Short name: coxph\n#  Class: surv.coxph\n#  Properties: missings,numerics,factors,weights,prob,rcens\n#  Predict-Type: response\n#  Hyperparameters:  All generated learners are objects of class  Learner .\nThis class contains the properties of the method, e.g., which types of features it can handle,\nwhat kind of output is possible during prediction, and whether multi-class problems,\nobservations weights or missing values are supported.  As you might have noticed, there is currently no special learner class for cost-sensitive classification.\nFor ordinary misclassification costs you can use standard classification methods.\nFor example-dependent costs there are several ways to generate cost-sensitive learners from ordinary\nregression and classification learners. This is explained in greater detail in the section about cost-sensitive classification .", 
            "title": "Constructing a learner"
        }, 
        {
            "location": "/learner/index.html#accessing-a-learner", 
            "text": "The  Learner  object is a  list  and the following elements contain\ninformation regarding the hyperparameters and the type of prediction.  ## Get the configured hyperparameter settings that deviate from the defaults\ncluster.lrn$par.vals\n#  $N\n#  [1] 5\n\n## Get the set of hyperparameters\nclassif.lrn$par.set\n#                      Type  len   Def   Constr Req Tunable Trafo\n#  ntree            integer    -   500 1 to Inf   -    TRUE     -\n#  mtry             integer    -     - 1 to Inf   -    TRUE     -\n#  replace          logical    -  TRUE        -   -    TRUE     -\n#  classwt    numericvector  NA      - 0 to Inf   -    TRUE     -\n#  cutoff     numericvector  NA      -   0 to 1   -    TRUE     -\n#  sampsize   integervector  NA      - 0 to Inf   -    TRUE     -\n#  nodesize         integer    -     1 1 to Inf   -    TRUE     -\n#  maxnodes         integer    -     - 1 to Inf   -    TRUE     -\n#  importance       logical    - FALSE        -   -    TRUE     -\n#  localImp         logical    - FALSE        -   -    TRUE     -\n#  norm.votes       logical    -  TRUE        -   -    TRUE     -\n#  keep.inbag       logical    - FALSE        -   -    TRUE     -\n\n## Get the type of prediction\nregr.lrn$predict.type\n#  [1]  response   Slot  $par.set  is an object of class  ParamSet .\nIt contains, among others, the type of hyperparameters (e.g., numeric, logical), potential\ndefault values and the range of allowed values.  Moreover,  mlr  provides function  getHyperPars  to access the current hyperparameter setting\nof a  Learner  and  getParamSet  to get a description of all possible settings.\nThese are particularly useful in case of wrapped  Learner s,\nfor example if a learner is fused with a feature selection strategy, and both, the learner\nas well the feature selection method, have hyperparameters.\nFor details see the section on  wrapped learners .  ## Get current hyperparameter settings\ngetHyperPars(cluster.lrn)\n#  $N\n#  [1] 5\n\n## Get a description of all possible hyperparameter settings\ngetParamSet(classif.lrn)\n#                      Type  len   Def   Constr Req Tunable Trafo\n#  ntree            integer    -   500 1 to Inf   -    TRUE     -\n#  mtry             integer    -     - 1 to Inf   -    TRUE     -\n#  replace          logical    -  TRUE        -   -    TRUE     -\n#  classwt    numericvector  NA      - 0 to Inf   -    TRUE     -\n#  cutoff     numericvector  NA      -   0 to 1   -    TRUE     -\n#  sampsize   integervector  NA      - 0 to Inf   -    TRUE     -\n#  nodesize         integer    -     1 1 to Inf   -    TRUE     -\n#  maxnodes         integer    -     - 1 to Inf   -    TRUE     -\n#  importance       logical    - FALSE        -   -    TRUE     -\n#  localImp         logical    - FALSE        -   -    TRUE     -\n#  norm.votes       logical    -  TRUE        -   -    TRUE     -\n#  keep.inbag       logical    - FALSE        -   -    TRUE     -  We can also use  getParamSet  to get a quick overview about the available hyperparameters\nand defaults of a learning method without explicitly constructing it (by calling  makeLearner ).  getParamSet( classif.randomForest )\n#                      Type  len   Def   Constr Req Tunable Trafo\n#  ntree            integer    -   500 1 to Inf   -    TRUE     -\n#  mtry             integer    -     - 1 to Inf   -    TRUE     -\n#  replace          logical    -  TRUE        -   -    TRUE     -\n#  classwt    numericvector  NA      - 0 to Inf   -    TRUE     -\n#  cutoff     numericvector  NA      -   0 to 1   -    TRUE     -\n#  sampsize   integervector  NA      - 0 to Inf   -    TRUE     -\n#  nodesize         integer    -     1 1 to Inf   -    TRUE     -\n#  maxnodes         integer    -     - 1 to Inf   -    TRUE     -\n#  importance       logical    - FALSE        -   -    TRUE     -\n#  localImp         logical    - FALSE        -   -    TRUE     -\n#  norm.votes       logical    -  TRUE        -   -    TRUE     -\n#  keep.inbag       logical    - FALSE        -   -    TRUE     -", 
            "title": "Accessing a learner"
        }, 
        {
            "location": "/learner/index.html#modifying-a-learner", 
            "text": "There are also some functions that enable you to change certain aspects\nof a  Learner  without needing to create a new  Learner  from scratch.\nHere are some examples.  ## Change the ID\nsurv.lrn = setId(surv.lrn,  CoxModel )\nsurv.lrn\n#  Learner CoxModel from package survival\n#  Type: surv\n#  Name: Cox Proportional Hazard Model; Short name: coxph\n#  Class: surv.coxph\n#  Properties: missings,numerics,factors,weights,prob,rcens\n#  Predict-Type: response\n#  Hyperparameters:\n\n## Change the prediction type, predict a factor with class labels instead of probabilities\nclassif.lrn = setPredictType(classif.lrn,  response )\n\n## Change hyperparameter values\ncluster.lrn = setHyperPars(cluster.lrn, N = 4)\n\n## Go back to default hyperparameter values\nregr.lrn = removeHyperPars(regr.lrn, c( n.trees ,  interaction.depth ))", 
            "title": "Modifying a learner"
        }, 
        {
            "location": "/learner/index.html#listing-learners", 
            "text": "A list of all learners integrated in  mlr  and their respective properties is shown in the  Appendix .  If you would like a list of available learners, maybe only with certain properties or suitable for a\ncertain learning  Task  use function  listLearners .  ## List everything in mlr\nhead(listLearners())\n#            classif.ada   classif.bartMachine           classif.bdk \n#           classif.ada   classif.bartMachine           classif.bdk  \n#       classif.binomial    classif.blackboost      classif.boosting \n#      classif.binomial    classif.blackboost      classif.boosting \n\n## List classifiers that can output probabilities\nhead(listLearners( classif , properties =  prob ))\n#            classif.ada   classif.bartMachine           classif.bdk \n#           classif.ada   classif.bartMachine           classif.bdk  \n#       classif.binomial    classif.blackboost      classif.boosting \n#      classif.binomial    classif.blackboost      classif.boosting \n\n## List classifiers that can be applied to iris (i.e., multiclass) and output probabilities\nhead(listLearners(iris.task, properties =  prob ))\n#           classif.bdk     classif.boosting      classif.cforest \n#          classif.bdk     classif.boosting      classif.cforest  \n#         classif.ctree   classif.extraTrees          classif.gbm \n#        classif.ctree   classif.extraTrees          classif.gbm \n\n## The calls above return character vectors, but you can also create learner objects\nhead(listLearners( cluster , create = TRUE), 2)\n#  $cluster.cmeans\n#  Learner cluster.cmeans from package e1071,clue\n#  Type: cluster\n#  Name: Fuzzy C-Means Clustering; Short name: cmeans\n#  Class: cluster.cmeans\n#  Properties: numerics,prob\n#  Predict-Type: response\n#  Hyperparameters: centers= integer \n#  \n#  \n#  $cluster.EM\n#  Learner cluster.EM from package RWeka\n#  Type: cluster\n#  Name: Expectation-Maximization Clustering; Short name: em\n#  Class: cluster.EM\n#  Properties: numerics\n#  Predict-Type: response\n#  Hyperparameters:", 
            "title": "Listing learners"
        }, 
        {
            "location": "/train/index.html", 
            "text": "Training a Learner\n\n\nTraining a learner means fitting a model to a given data set.\nIn \nmlr\n this can be done by calling the function \ntrain\n\non a \nLearner\n and a suitable \nTask\n.\n\n\nTraining a learner works the same way for every type of \nTask\n.\nHere is a classification example using the data set \niris\n and an LDA learner.\n\n\nlrn = makeLearner(\nclassif.lda\n)\nmod = train(lrn, iris.task)\nmod\n#\n Model for learner.id=classif.lda; learner.class=classif.lda\n#\n Trained on: task.id = iris-example; obs = 150; features = 4\n#\n Hyperparameters:\n\n\n\n\nIn the above example, creating a \nLearner\n explicitly is not strictly necessary.\nAs a general rule, you have to create a \nLearner\n if you want to change any defaults\nby, e.g., setting hyperparameter values or changing the type of prediction.\nOtherwise, \ntrain\n and many other functions accept a character string naming the learning\nmethod.\n\n\nmod = train(\nclassif.lda\n, iris.task)\nmod\n#\n Model for learner.id=classif.lda; learner.class=classif.lda\n#\n Trained on: task.id = iris-example; obs = 150; features = 4\n#\n Hyperparameters:\n\n\n\n\nOptionally, only a subset of the data, specified by an index set, can be used to\ntrain the learner. This set is passed using the \nsubset\n argument of \ntrain\n.\n\n\nWe fit a simple linear regression model to the \nBostonHousing\n data set.\nThe object \nbh.task\n is the regression \nTask\n on the \nBostonHousing\n\ndata set provided by \nmlr\n.\n\n\n## Number of observations\nn = getTaskSize(bh.task)\n## Use 1/3 of the observations for training\ntrain.set = sample(n, size = n/3)\n## Train the learner\nmod = train(\nregr.lm\n, bh.task, subset = train.set)\nmod\n#\n Model for learner.id=regr.lm; learner.class=regr.lm\n#\n Trained on: task.id = BostonHousing-example; obs = 168; features = 13\n#\n Hyperparameters:\n\n\n\n\nNote, for later, that all standard \nresampling strategies\n are supported.\nTherefore you usually do not have to subset the data yourself.\n\n\nMoreover, if the \nLearner\n supports this, you can specify observation \nweights\n\nthat reflect the relevance of examples in the training process.\n\n\nFor example, in the \nBreastCancer\n data set class \nbenign\n is almost\ntwice as frequent as class malignant.\nIf both classes should have equal importance in training the classifier we can weight the\nexamples according to the class frequencies in the data set as shown in the following\n\nR\n code (see also the section about \nimbalanced classification problems\n).\n\n\n## Calculate the observation weights\ntarget = getTaskTargets(bc.task)\ntab = as.numeric(table(target))\nw = 1/tab[target]\n\ntrain(\nclassif.rpart\n, task = bc.task, weights = w)\n#\n Model for learner.id=classif.rpart; learner.class=classif.rpart\n#\n Trained on: task.id = BreastCancer-example; obs = 683; features = 9\n#\n Hyperparameters: xval=0\n\n\n\n\nAs you may recall, it is also possible to pass observation weights when creating the\n\nTask\n.\nNaturally, it makes sense to specify \nweights\n in \nmake\nTask\n if those weights\nshould always be used for the learning task and in \ntrain\n if this is not the case since, e.g.,\nsome of the learners you want to use cannot deal with weights or you want to try different weights.\nThe weights in \ntrain\n overwrite the weights in \nTask\n.\nAs a side remark for more advanced readers:\nBy varying the weights in the calls to train, you could also implement your own variant of\na general boosting type of algorithm on arbitrary mlr base learners.\n\n\nLet's finish with a survival analysis example and train a\n\nCox proportional hazards model\n\non the \nlung\n data set.\n\n\ndata(lung, package = \nsurvival\n)\nlung$status = (lung$status == 2)\ntask = makeSurvTask(data = lung, target = c(\ntime\n, \nstatus\n))\nlrn = makeLearner(\nsurv.coxph\n)\n\nmod = train(lrn, task)\nmod\n#\n Model for learner.id=surv.coxph; learner.class=surv.coxph\n#\n Trained on: task.id = lung; obs = 228; features = 8\n#\n Hyperparameters:\n\n\n\n\nWrapped models\n\n\ntrain\n returns an object of class \nWrappedModel\n, which wraps the\nparticular model of the underlying \nR\n learning method.\nThis object contains the actual fitted model fit as returned by the \nR\n external package and additionally some informations about the learner and task.\nIt can subsequently be used to perform a \nprediction\n for new observations.\n\n\nIn order to access the underlying model we can use the function \ngetLearnerModel\n.\nIn the following example we get an object of class \nlm\n.\n\n\nmod = train(\nregr.lm\n, bh.task, subset = train.set)\ngetLearnerModel(mod)\n#\n \n#\n Call:\n#\n stats::lm(formula = f, data = d)\n#\n \n#\n Coefficients:\n#\n (Intercept)         crim           zn        indus        chas1  \n#\n   35.593344     0.050105     0.058767    -0.010443     3.292679  \n#\n         nox           rm          age          dis          rad  \n#\n  -26.048276     3.964828     0.028116    -1.560925     0.199352  \n#\n         tax      ptratio            b        lstat  \n#\n   -0.005052    -0.798202     0.008841    -0.674258", 
            "title": "Train"
        }, 
        {
            "location": "/train/index.html#training-a-learner", 
            "text": "Training a learner means fitting a model to a given data set.\nIn  mlr  this can be done by calling the function  train \non a  Learner  and a suitable  Task .  Training a learner works the same way for every type of  Task .\nHere is a classification example using the data set  iris  and an LDA learner.  lrn = makeLearner( classif.lda )\nmod = train(lrn, iris.task)\nmod\n#  Model for learner.id=classif.lda; learner.class=classif.lda\n#  Trained on: task.id = iris-example; obs = 150; features = 4\n#  Hyperparameters:  In the above example, creating a  Learner  explicitly is not strictly necessary.\nAs a general rule, you have to create a  Learner  if you want to change any defaults\nby, e.g., setting hyperparameter values or changing the type of prediction.\nOtherwise,  train  and many other functions accept a character string naming the learning\nmethod.  mod = train( classif.lda , iris.task)\nmod\n#  Model for learner.id=classif.lda; learner.class=classif.lda\n#  Trained on: task.id = iris-example; obs = 150; features = 4\n#  Hyperparameters:  Optionally, only a subset of the data, specified by an index set, can be used to\ntrain the learner. This set is passed using the  subset  argument of  train .  We fit a simple linear regression model to the  BostonHousing  data set.\nThe object  bh.task  is the regression  Task  on the  BostonHousing \ndata set provided by  mlr .  ## Number of observations\nn = getTaskSize(bh.task)\n## Use 1/3 of the observations for training\ntrain.set = sample(n, size = n/3)\n## Train the learner\nmod = train( regr.lm , bh.task, subset = train.set)\nmod\n#  Model for learner.id=regr.lm; learner.class=regr.lm\n#  Trained on: task.id = BostonHousing-example; obs = 168; features = 13\n#  Hyperparameters:  Note, for later, that all standard  resampling strategies  are supported.\nTherefore you usually do not have to subset the data yourself.  Moreover, if the  Learner  supports this, you can specify observation  weights \nthat reflect the relevance of examples in the training process.  For example, in the  BreastCancer  data set class  benign  is almost\ntwice as frequent as class malignant.\nIf both classes should have equal importance in training the classifier we can weight the\nexamples according to the class frequencies in the data set as shown in the following R  code (see also the section about  imbalanced classification problems ).  ## Calculate the observation weights\ntarget = getTaskTargets(bc.task)\ntab = as.numeric(table(target))\nw = 1/tab[target]\n\ntrain( classif.rpart , task = bc.task, weights = w)\n#  Model for learner.id=classif.rpart; learner.class=classif.rpart\n#  Trained on: task.id = BreastCancer-example; obs = 683; features = 9\n#  Hyperparameters: xval=0  As you may recall, it is also possible to pass observation weights when creating the Task .\nNaturally, it makes sense to specify  weights  in  make Task  if those weights\nshould always be used for the learning task and in  train  if this is not the case since, e.g.,\nsome of the learners you want to use cannot deal with weights or you want to try different weights.\nThe weights in  train  overwrite the weights in  Task .\nAs a side remark for more advanced readers:\nBy varying the weights in the calls to train, you could also implement your own variant of\na general boosting type of algorithm on arbitrary mlr base learners.  Let's finish with a survival analysis example and train a Cox proportional hazards model \non the  lung  data set.  data(lung, package =  survival )\nlung$status = (lung$status == 2)\ntask = makeSurvTask(data = lung, target = c( time ,  status ))\nlrn = makeLearner( surv.coxph )\n\nmod = train(lrn, task)\nmod\n#  Model for learner.id=surv.coxph; learner.class=surv.coxph\n#  Trained on: task.id = lung; obs = 228; features = 8\n#  Hyperparameters:", 
            "title": "Training a Learner"
        }, 
        {
            "location": "/train/index.html#wrapped-models", 
            "text": "train  returns an object of class  WrappedModel , which wraps the\nparticular model of the underlying  R  learning method.\nThis object contains the actual fitted model fit as returned by the  R  external package and additionally some informations about the learner and task.\nIt can subsequently be used to perform a  prediction  for new observations.  In order to access the underlying model we can use the function  getLearnerModel .\nIn the following example we get an object of class  lm .  mod = train( regr.lm , bh.task, subset = train.set)\ngetLearnerModel(mod)\n#  \n#  Call:\n#  stats::lm(formula = f, data = d)\n#  \n#  Coefficients:\n#  (Intercept)         crim           zn        indus        chas1  \n#    35.593344     0.050105     0.058767    -0.010443     3.292679  \n#          nox           rm          age          dis          rad  \n#   -26.048276     3.964828     0.028116    -1.560925     0.199352  \n#          tax      ptratio            b        lstat  \n#    -0.005052    -0.798202     0.008841    -0.674258", 
            "title": "Wrapped models"
        }, 
        {
            "location": "/predict/index.html", 
            "text": "Predicting Outcomes for New Data\n\n\nPredicting the target values for new observations is\nimplemented the same way as most of the other predict methods in \nR\n.\nIn general, all you need to do is call \npredict\n on the object\nreturned by \ntrain\n and pass the data you want predictions for.\n\n\nThere are two ways to pass the data:\n\n\n\n\nEither pass the \nTask\n via the \ntask\n argument or\n\n\npass a \ndata frame\n via the \nnewdata\n argument.\n\n\n\n\nThe first way is preferable if you want predictions for data already included in a \nTask\n.\n\n\nJust as \ntrain\n, the \npredict\n function has a \nsubset\n argument,\nso you can set aside different portions of the data in \nTask\n for training and prediction\n(more advanced methods for splitting the data in train and test set are described in the\n\nsection on resampling\n).\n\n\nIn the following example we fit a \ngradient boosting machine\n to every second\nobservation of the \nBostonHousing\n data set and make predictions\non the remaining data in \nbh.task\n.\n\n\nn = getTaskSize(bh.task)\ntrain.set = seq(1, n, by = 2)\ntest.set = seq(2, n, by = 2)\nlrn = makeLearner(\nregr.gbm\n, n.trees = 100)\nmod = train(lrn, bh.task, subset = train.set)\n\ntask.pred = predict(mod, task = bh.task, subset = test.set)\ntask.pred\n#\n Prediction: 253 observations\n#\n predict.type: response\n#\n threshold: \n#\n time: 0.00\n#\n    id truth response\n#\n 2   2  21.6 22.28539\n#\n 4   4  33.4 23.33968\n#\n 6   6  28.7 22.40896\n#\n 8   8  27.1 22.12750\n#\n 10 10  18.9 22.12750\n#\n 12 12  18.9 22.12750\n\n\n\n\nThe second way is useful if you want to predict data not included in the \nTask\n.\n\n\nHere we cluster the \niris\n data set without the target variable.\nAll observations with an odd index are included in the \nTask\n and used for training.\nPredictions are made for the remaining observations.\n\n\nn = nrow(iris)\niris.train = iris[seq(1, n, by = 2), -5]\niris.test = iris[seq(2, n, by = 2), -5]\ntask = makeClusterTask(data = iris.train)\nmod = train(\ncluster.kmeans\n, task)\n\nnewdata.pred = predict(mod, newdata = iris.test)\nnewdata.pred\n#\n Prediction: 75 observations\n#\n predict.type: response\n#\n threshold: \n#\n time: 0.00\n#\n    response\n#\n 2         2\n#\n 4         2\n#\n 6         2\n#\n 8         2\n#\n 10        2\n#\n 12        2\n\n\n\n\nNote that for supervised learning you do not have to remove the target columns from the data.\nThese columns are automatically removed prior to calling the underlying \npredict\n method of the learner.\n\n\nAccessing the prediction\n\n\nThe returned \nPrediction\n object is a named \nlist\n. The most important element is \ndata\n which is a\n\ndata frame\n that contains columns with the true values of the target variable (in case of\nsupervised learning problems) and the predictions.\n\n\nIn the following the predictions on the \nBostonHousing\n and the\n\niris\n data sets are shown.\nAs you may recall, the predictions in the first case were made from a \nTask\n and in the\nsecond case from a \ndata frame\n.\n\n\n## Result of predict with data passed via task argument\nhead(task.pred$data)\n#\n    id truth response\n#\n 2   2  21.6 22.28539\n#\n 4   4  33.4 23.33968\n#\n 6   6  28.7 22.40896\n#\n 8   8  27.1 22.12750\n#\n 10 10  18.9 22.12750\n#\n 12 12  18.9 22.12750\n\n## Result of predict with data passed via newdata argument\nhead(newdata.pred$data)\n#\n    response\n#\n 2         2\n#\n 4         2\n#\n 6         2\n#\n 8         2\n#\n 10        2\n#\n 12        2\n\n\n\n\nAs you can see when predicting from a \nTask\n, the resulting \ndata frame\n\ncontains an additional column, called \nid\n, which tells us which element in the original data set\nthe prediction corresponds to.\n\n\nExtract Probabilities\n\n\nThe predicted probabilities can be extracted from the \nPrediction\n using the function\n\ngetProbabilities\n.\nHere is another cluster analysis example. We use \nfuzzy c-means clustering\n\non the \nmtcars\n data set.\n\n\nlrn = makeLearner(\ncluster.cmeans\n, predict.type = \nprob\n)\nmod = train(lrn, mtcars.task)\n\npred = predict(mod, task = mtcars.task)\nhead(getProbabilities(pred))\n#\n                            1           2\n#\n Mazda RX4         0.97959529 0.020404714\n#\n Mazda RX4 Wag     0.97963550 0.020364495\n#\n Datsun 710        0.99265984 0.007340164\n#\n Hornet 4 Drive    0.54292079 0.457079211\n#\n Hornet Sportabout 0.01870622 0.981293776\n#\n Valiant           0.75746556 0.242534444\n\n\n\n\nFor classification problems there are some more things worth mentioning.\nBy default, class labels are predicted.\n\n\n## Linear discriminant analysis on the iris data set\nmod = train(\nclassif.lda\n, task = iris.task)\n\npred = predict(mod, task = iris.task)\npred\n#\n Prediction: 150 observations\n#\n predict.type: response\n#\n threshold: \n#\n time: 0.00\n#\n   id  truth response\n#\n 1  1 setosa   setosa\n#\n 2  2 setosa   setosa\n#\n 3  3 setosa   setosa\n#\n 4  4 setosa   setosa\n#\n 5  5 setosa   setosa\n#\n 6  6 setosa   setosa\n\n\n\n\nA confusion matrix can be obtained by calling \ngetConfMatrix\n.\n\n\ngetConfMatrix(pred)\n#\n             predicted\n#\n true         setosa versicolor virginica -SUM-\n#\n   setosa         50          0         0     0\n#\n   versicolor      0         48         2     2\n#\n   virginica       0          1        49     1\n#\n   -SUM-           0          1         2     3\n\n\n\n\nIn order to get predicted posterior probabilities we have to create a \nLearner\n\nwith the appropriate \npredict.type\n.\n\n\nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n)\nmod = train(lrn, iris.task)\n\npred = predict(mod, newdata = iris)\nhead(pred$data)\n#\n    truth prob.setosa prob.versicolor prob.virginica response\n#\n 1 setosa           1               0              0   setosa\n#\n 2 setosa           1               0              0   setosa\n#\n 3 setosa           1               0              0   setosa\n#\n 4 setosa           1               0              0   setosa\n#\n 5 setosa           1               0              0   setosa\n#\n 6 setosa           1               0              0   setosa\n\n\n\n\nIn addition to the probabilities, class labels are predicted\nby choosing the class with the maximum probability and breaking ties at random.\n\n\nAs mentioned above, the predicted posterior probabilities can be accessed via the\n\ngetProbabilities\n function.\n\n\nhead(getProbabilities(pred))\n#\n   setosa versicolor virginica\n#\n 1      1          0         0\n#\n 2      1          0         0\n#\n 3      1          0         0\n#\n 4      1          0         0\n#\n 5      1          0         0\n#\n 6      1          0         0\n\n\n\n\nAdjusting the threshold\n\n\nWe can set the threshold value that is used to map the predicted posterior probabilities to class labels.\nNote that for this purpose we need to create a \nLearner\n that predicts probabilities.\nFor binary classification, the threshold determines when the \npositive\n class is predicted.\nThe default is 0.5.\nNow, we set the threshold for the positive class to 0.9 (that is, an example is assigned to the positive class if its posterior probability exceeds 0.9).\nWhich of the two classes is the positive one can be seen by accessing the \nTask\n.\nTo illustrate binary classification, we use the \nSonar\n data set from the \nmlbench\n package.\n\n\nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n)\nmod = train(lrn, task = sonar.task)\n\n## Label of the positive class\ngetTaskDescription(sonar.task)$positive\n#\n [1] \nM\n\n\n## Default threshold\npred1 = predict(mod, sonar.task)\npred1$threshold\n#\n   M   R \n#\n 0.5 0.5\n\n## Set the threshold value for the positive class\npred2 = setThreshold(pred1, 0.9)\npred2$threshold\n#\n   M   R \n#\n 0.9 0.1\npred2\n#\n Prediction: 208 observations\n#\n predict.type: prob\n#\n threshold: M=0.90,R=0.10\n#\n time: 0.01\n#\n   id truth    prob.M    prob.R response\n#\n 1  1     R 0.1060606 0.8939394        R\n#\n 2  2     R 0.7333333 0.2666667        R\n#\n 3  3     R 0.0000000 1.0000000        R\n#\n 4  4     R 0.1060606 0.8939394        R\n#\n 5  5     R 0.9250000 0.0750000        M\n#\n 6  6     R 0.0000000 1.0000000        R\n\n## We can also set the effect in the confusion matrix\ngetConfMatrix(pred1)\n#\n        predicted\n#\n true     M  R -SUM-\n#\n   M     95 16    16\n#\n   R     10 87    10\n#\n   -SUM- 10 16    26\ngetConfMatrix(pred2)\n#\n        predicted\n#\n true     M  R -SUM-\n#\n   M     84 27    27\n#\n   R      6 91     6\n#\n   -SUM-  6 27    33\n\n\n\n\nNote that in the binary case \ngetProbabilities\n by default extracts the posterior\nprobabilities of the positive class only.\n\n\nhead(getProbabilities(pred1))\n#\n [1] 0.1060606 0.7333333 0.0000000 0.1060606 0.9250000 0.0000000\n## But we can change that, too\nhead(getProbabilities(pred1, cl = c(\nM\n, \nR\n)))\n#\n           M         R\n#\n 1 0.1060606 0.8939394\n#\n 2 0.7333333 0.2666667\n#\n 3 0.0000000 1.0000000\n#\n 4 0.1060606 0.8939394\n#\n 5 0.9250000 0.0750000\n#\n 6 0.0000000 1.0000000\n\n\n\n\nIt works similarly for multiclass classification.\nThe threshold has to be given by a named vector specifying the values by which each probability will be divided.\nThe class with the maximum resulting value is then selected.\n\n\nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n)\nmod = train(lrn, iris.task)\npred = predict(mod, newdata = iris)\npred$threshold\n#\n     setosa versicolor  virginica \n#\n  0.3333333  0.3333333  0.3333333\ntable(as.data.frame(pred)$response)\n#\n \n#\n     setosa versicolor  virginica \n#\n         50         54         46\npred = setThreshold(pred, c(setosa = 0.01, versicolor = 50, virginica = 1))\npred$threshold\n#\n     setosa versicolor  virginica \n#\n       0.01      50.00       1.00\ntable(as.data.frame(pred)$response)\n#\n \n#\n     setosa versicolor  virginica \n#\n         50          0        100\n\n\n\n\nIf you are interested in tuning the threshold (vector) have a look at the section about\n\nperformance curves and threshold tuning\n.\n\n\nVisualizing the prediction\n\n\nThe function \nplotLearnerPrediction\n allows to visualize predictions, e.g., for teaching purposes\nor exploring models.\nIt trains the chosen learning method for 1 or 2 selected features and then displays the\npredictions with \nggplot\n.\n\n\nFor \nclassification\n, we get a scatter plot of 2 features (by default the first 2 in the data set).\nThe type of symbol shows the true class labels of the data points.\nSymbols with white border indicate misclassified observations.\nThe posterior probabilities (if the learner under consideration supports this)\nare represented by the background color where higher saturation means larger probabilities.\n\n\nThe plot title displays the ID of the \nLearner\n (in the following example CART),\nits parameters, its training performance and its cross-validation performance.\n\nmmce\n stands for \nmean misclassification error\n, i.e., the error rate.\nSee the sections on \nperformance\n and\n\nresampling\n for further explanations.\n\n\nlrn = makeLearner(\nclassif.rpart\n, id = \nCART\n)\nplotLearnerPrediction(lrn, task = iris.task)\n\n\n\n\n \n\n\nFor \nclustering\n we also get a scatter plot of two selected features.\nThe color of the points indicates the predicted cluster.\n\n\nlrn = makeLearner(\ncluster.SimpleKMeans\n)\nplotLearnerPrediction(lrn, task = mtcars.task, features = c(\ndisp\n, \ndrat\n), cv = 0)\n\n\n\n\n \n\n\nFor \nregression\n, there are two types of plots.\nThe 1D plot shows the target values in relation to a single feature, the regression curve and,\nif the chosen learner supports this, the estimated standard error.\n\n\nplotLearnerPrediction(\nregr.lm\n, features = \nlstat\n, task = bh.task)\n\n\n\n\n \n\n\nThe 2D variant, as in the classification case, generates a scatter plot of 2 features.\nThe fill color of the dots illustrates the value of the target variable \n\"medv\"\n, the\nbackground colors show the estimated mean.\nThe plot does not represent the estimated standard error.\n\n\nplotLearnerPrediction(\nregr.lm\n, features = c(\nlstat\n, \nrm\n), task = bh.task)", 
            "title": "Predict"
        }, 
        {
            "location": "/predict/index.html#predicting-outcomes-for-new-data", 
            "text": "Predicting the target values for new observations is\nimplemented the same way as most of the other predict methods in  R .\nIn general, all you need to do is call  predict  on the object\nreturned by  train  and pass the data you want predictions for.  There are two ways to pass the data:   Either pass the  Task  via the  task  argument or  pass a  data frame  via the  newdata  argument.   The first way is preferable if you want predictions for data already included in a  Task .  Just as  train , the  predict  function has a  subset  argument,\nso you can set aside different portions of the data in  Task  for training and prediction\n(more advanced methods for splitting the data in train and test set are described in the section on resampling ).  In the following example we fit a  gradient boosting machine  to every second\nobservation of the  BostonHousing  data set and make predictions\non the remaining data in  bh.task .  n = getTaskSize(bh.task)\ntrain.set = seq(1, n, by = 2)\ntest.set = seq(2, n, by = 2)\nlrn = makeLearner( regr.gbm , n.trees = 100)\nmod = train(lrn, bh.task, subset = train.set)\n\ntask.pred = predict(mod, task = bh.task, subset = test.set)\ntask.pred\n#  Prediction: 253 observations\n#  predict.type: response\n#  threshold: \n#  time: 0.00\n#     id truth response\n#  2   2  21.6 22.28539\n#  4   4  33.4 23.33968\n#  6   6  28.7 22.40896\n#  8   8  27.1 22.12750\n#  10 10  18.9 22.12750\n#  12 12  18.9 22.12750  The second way is useful if you want to predict data not included in the  Task .  Here we cluster the  iris  data set without the target variable.\nAll observations with an odd index are included in the  Task  and used for training.\nPredictions are made for the remaining observations.  n = nrow(iris)\niris.train = iris[seq(1, n, by = 2), -5]\niris.test = iris[seq(2, n, by = 2), -5]\ntask = makeClusterTask(data = iris.train)\nmod = train( cluster.kmeans , task)\n\nnewdata.pred = predict(mod, newdata = iris.test)\nnewdata.pred\n#  Prediction: 75 observations\n#  predict.type: response\n#  threshold: \n#  time: 0.00\n#     response\n#  2         2\n#  4         2\n#  6         2\n#  8         2\n#  10        2\n#  12        2  Note that for supervised learning you do not have to remove the target columns from the data.\nThese columns are automatically removed prior to calling the underlying  predict  method of the learner.", 
            "title": "Predicting Outcomes for New Data"
        }, 
        {
            "location": "/predict/index.html#accessing-the-prediction", 
            "text": "The returned  Prediction  object is a named  list . The most important element is  data  which is a data frame  that contains columns with the true values of the target variable (in case of\nsupervised learning problems) and the predictions.  In the following the predictions on the  BostonHousing  and the iris  data sets are shown.\nAs you may recall, the predictions in the first case were made from a  Task  and in the\nsecond case from a  data frame .  ## Result of predict with data passed via task argument\nhead(task.pred$data)\n#     id truth response\n#  2   2  21.6 22.28539\n#  4   4  33.4 23.33968\n#  6   6  28.7 22.40896\n#  8   8  27.1 22.12750\n#  10 10  18.9 22.12750\n#  12 12  18.9 22.12750\n\n## Result of predict with data passed via newdata argument\nhead(newdata.pred$data)\n#     response\n#  2         2\n#  4         2\n#  6         2\n#  8         2\n#  10        2\n#  12        2  As you can see when predicting from a  Task , the resulting  data frame \ncontains an additional column, called  id , which tells us which element in the original data set\nthe prediction corresponds to.  Extract Probabilities  The predicted probabilities can be extracted from the  Prediction  using the function getProbabilities .\nHere is another cluster analysis example. We use  fuzzy c-means clustering \non the  mtcars  data set.  lrn = makeLearner( cluster.cmeans , predict.type =  prob )\nmod = train(lrn, mtcars.task)\n\npred = predict(mod, task = mtcars.task)\nhead(getProbabilities(pred))\n#                             1           2\n#  Mazda RX4         0.97959529 0.020404714\n#  Mazda RX4 Wag     0.97963550 0.020364495\n#  Datsun 710        0.99265984 0.007340164\n#  Hornet 4 Drive    0.54292079 0.457079211\n#  Hornet Sportabout 0.01870622 0.981293776\n#  Valiant           0.75746556 0.242534444  For classification problems there are some more things worth mentioning.\nBy default, class labels are predicted.  ## Linear discriminant analysis on the iris data set\nmod = train( classif.lda , task = iris.task)\n\npred = predict(mod, task = iris.task)\npred\n#  Prediction: 150 observations\n#  predict.type: response\n#  threshold: \n#  time: 0.00\n#    id  truth response\n#  1  1 setosa   setosa\n#  2  2 setosa   setosa\n#  3  3 setosa   setosa\n#  4  4 setosa   setosa\n#  5  5 setosa   setosa\n#  6  6 setosa   setosa  A confusion matrix can be obtained by calling  getConfMatrix .  getConfMatrix(pred)\n#              predicted\n#  true         setosa versicolor virginica -SUM-\n#    setosa         50          0         0     0\n#    versicolor      0         48         2     2\n#    virginica       0          1        49     1\n#    -SUM-           0          1         2     3  In order to get predicted posterior probabilities we have to create a  Learner \nwith the appropriate  predict.type .  lrn = makeLearner( classif.rpart , predict.type =  prob )\nmod = train(lrn, iris.task)\n\npred = predict(mod, newdata = iris)\nhead(pred$data)\n#     truth prob.setosa prob.versicolor prob.virginica response\n#  1 setosa           1               0              0   setosa\n#  2 setosa           1               0              0   setosa\n#  3 setosa           1               0              0   setosa\n#  4 setosa           1               0              0   setosa\n#  5 setosa           1               0              0   setosa\n#  6 setosa           1               0              0   setosa  In addition to the probabilities, class labels are predicted\nby choosing the class with the maximum probability and breaking ties at random.  As mentioned above, the predicted posterior probabilities can be accessed via the getProbabilities  function.  head(getProbabilities(pred))\n#    setosa versicolor virginica\n#  1      1          0         0\n#  2      1          0         0\n#  3      1          0         0\n#  4      1          0         0\n#  5      1          0         0\n#  6      1          0         0", 
            "title": "Accessing the prediction"
        }, 
        {
            "location": "/predict/index.html#adjusting-the-threshold", 
            "text": "We can set the threshold value that is used to map the predicted posterior probabilities to class labels.\nNote that for this purpose we need to create a  Learner  that predicts probabilities.\nFor binary classification, the threshold determines when the  positive  class is predicted.\nThe default is 0.5.\nNow, we set the threshold for the positive class to 0.9 (that is, an example is assigned to the positive class if its posterior probability exceeds 0.9).\nWhich of the two classes is the positive one can be seen by accessing the  Task .\nTo illustrate binary classification, we use the  Sonar  data set from the  mlbench  package.  lrn = makeLearner( classif.rpart , predict.type =  prob )\nmod = train(lrn, task = sonar.task)\n\n## Label of the positive class\ngetTaskDescription(sonar.task)$positive\n#  [1]  M \n\n## Default threshold\npred1 = predict(mod, sonar.task)\npred1$threshold\n#    M   R \n#  0.5 0.5\n\n## Set the threshold value for the positive class\npred2 = setThreshold(pred1, 0.9)\npred2$threshold\n#    M   R \n#  0.9 0.1\npred2\n#  Prediction: 208 observations\n#  predict.type: prob\n#  threshold: M=0.90,R=0.10\n#  time: 0.01\n#    id truth    prob.M    prob.R response\n#  1  1     R 0.1060606 0.8939394        R\n#  2  2     R 0.7333333 0.2666667        R\n#  3  3     R 0.0000000 1.0000000        R\n#  4  4     R 0.1060606 0.8939394        R\n#  5  5     R 0.9250000 0.0750000        M\n#  6  6     R 0.0000000 1.0000000        R\n\n## We can also set the effect in the confusion matrix\ngetConfMatrix(pred1)\n#         predicted\n#  true     M  R -SUM-\n#    M     95 16    16\n#    R     10 87    10\n#    -SUM- 10 16    26\ngetConfMatrix(pred2)\n#         predicted\n#  true     M  R -SUM-\n#    M     84 27    27\n#    R      6 91     6\n#    -SUM-  6 27    33  Note that in the binary case  getProbabilities  by default extracts the posterior\nprobabilities of the positive class only.  head(getProbabilities(pred1))\n#  [1] 0.1060606 0.7333333 0.0000000 0.1060606 0.9250000 0.0000000\n## But we can change that, too\nhead(getProbabilities(pred1, cl = c( M ,  R )))\n#            M         R\n#  1 0.1060606 0.8939394\n#  2 0.7333333 0.2666667\n#  3 0.0000000 1.0000000\n#  4 0.1060606 0.8939394\n#  5 0.9250000 0.0750000\n#  6 0.0000000 1.0000000  It works similarly for multiclass classification.\nThe threshold has to be given by a named vector specifying the values by which each probability will be divided.\nThe class with the maximum resulting value is then selected.  lrn = makeLearner( classif.rpart , predict.type =  prob )\nmod = train(lrn, iris.task)\npred = predict(mod, newdata = iris)\npred$threshold\n#      setosa versicolor  virginica \n#   0.3333333  0.3333333  0.3333333\ntable(as.data.frame(pred)$response)\n#  \n#      setosa versicolor  virginica \n#          50         54         46\npred = setThreshold(pred, c(setosa = 0.01, versicolor = 50, virginica = 1))\npred$threshold\n#      setosa versicolor  virginica \n#        0.01      50.00       1.00\ntable(as.data.frame(pred)$response)\n#  \n#      setosa versicolor  virginica \n#          50          0        100  If you are interested in tuning the threshold (vector) have a look at the section about performance curves and threshold tuning .", 
            "title": "Adjusting the threshold"
        }, 
        {
            "location": "/predict/index.html#visualizing-the-prediction", 
            "text": "The function  plotLearnerPrediction  allows to visualize predictions, e.g., for teaching purposes\nor exploring models.\nIt trains the chosen learning method for 1 or 2 selected features and then displays the\npredictions with  ggplot .  For  classification , we get a scatter plot of 2 features (by default the first 2 in the data set).\nThe type of symbol shows the true class labels of the data points.\nSymbols with white border indicate misclassified observations.\nThe posterior probabilities (if the learner under consideration supports this)\nare represented by the background color where higher saturation means larger probabilities.  The plot title displays the ID of the  Learner  (in the following example CART),\nits parameters, its training performance and its cross-validation performance. mmce  stands for  mean misclassification error , i.e., the error rate.\nSee the sections on  performance  and resampling  for further explanations.  lrn = makeLearner( classif.rpart , id =  CART )\nplotLearnerPrediction(lrn, task = iris.task)     For  clustering  we also get a scatter plot of two selected features.\nThe color of the points indicates the predicted cluster.  lrn = makeLearner( cluster.SimpleKMeans )\nplotLearnerPrediction(lrn, task = mtcars.task, features = c( disp ,  drat ), cv = 0)     For  regression , there are two types of plots.\nThe 1D plot shows the target values in relation to a single feature, the regression curve and,\nif the chosen learner supports this, the estimated standard error.  plotLearnerPrediction( regr.lm , features =  lstat , task = bh.task)     The 2D variant, as in the classification case, generates a scatter plot of 2 features.\nThe fill color of the dots illustrates the value of the target variable  \"medv\" , the\nbackground colors show the estimated mean.\nThe plot does not represent the estimated standard error.  plotLearnerPrediction( regr.lm , features = c( lstat ,  rm ), task = bh.task)", 
            "title": "Visualizing the prediction"
        }, 
        {
            "location": "/performance/index.html", 
            "text": "Evaluating Learner Performance\n\n\nThe quality of the predictions of a model in \nmlr\n can be assessed with respect to a\nnumber of different performance measures.\nIn order to calculate the performance measures, call \nperformance\n on the object\nreturned by \npredict\n and specify the desired performance measures.\n\n\nAvailable performance measures\n\n\nmlr\n provides a large number of performance measures for all types of learning problems.\nTypical performance measures for \nclassification\n are the mean misclassification error (\nmmce\n),\naccuracy (\nacc\n) or measures based on \nROC analysis\n.\nFor \nregression\n the mean of squared errors (\nmse\n) or mean of absolute errors (\nmae\n)\nare usually considered.\nFor \nclustering\n tasks, measures such as the Dunn index (\ndunn\n) are provided,\nwhile for \nsurvival\n predictions, the Concordance Index (\ncindex\n) is\nsupported, and for \ncost-sensitive\n predictions the misclassification penalty\n(\nmcp\n) and others. It is also possible to access the time to train the\nlearner (\ntimetrain\n), the time to compute the prediction (\ntimepredict\n) and their\nsum (\ntimeboth\n) as performance measures.\n\n\nTo see which performance measures are implemented, have a look at the\n\ntable of performance measures\n and the \nmeasures\n documentation page.\n\n\nIf you want to implement an additional measure or include a measure with\nnon-standard misclassification costs, see the section on\n\ncreating custom measures\n.\n\n\nListing measures\n\n\nThe properties and requirements of the individual measures are shown in the \ntable of performance measures\n.\n\n\nIf you would like a list of available measures with certain properties or suitable for a\ncertain learning \nTask\n use the function \nlistMeasures\n.\n\n\n## Performance measures for classification with multiple classes\nlistMeasures(\nclassif\n, properties = \nclassif.multi\n)\n#\n [1] \nfeatperc\n       \nmmce\n           \ntimeboth\n       \nacc\n           \n#\n [5] \nmulticlass.auc\n \nber\n            \ntimepredict\n    \ntimetrain\n\n## Performance measure suitable for the iris classification task\nlistMeasures(iris.task)\n#\n [1] \nfeatperc\n       \nmmce\n           \ntimeboth\n       \nacc\n           \n#\n [5] \nmulticlass.auc\n \nber\n            \ntimepredict\n    \ntimetrain\n\n\n\n\n\nCalculate performance measures\n\n\nIn the following example we fit a \ngradient boosting machine\n on a subset of the\n\nBostonHousing\n data set and calculate the mean squared error\n(\nmse\n) on the remaining observations.\n\n\nn = getTaskSize(bh.task)\nlrn = makeLearner(\nregr.gbm\n, n.trees = 1000)\nmod = train(lrn, task = bh.task, subset = seq(1, n, 2))\npred = predict(mod, task = bh.task, subset = seq(2, n, 2))\n\n# mse is the default measure for regression, we do not have to specify\n# it here\nperformance(pred)\n#\n      mse \n#\n 42.68414\n\n\n\n\nThe following code computes the median of squared errors (\nmedse\n) instead.\n\n\nperformance(pred, measures = medse)\n#\n    medse \n#\n 9.134965\n\n\n\n\nOf course, we can also calculate multiple performance measures at once by simply passing a\nlist of measures which can also include \nyour own measure\n.\n\n\nCalculate the mean squared error, median squared error and mean absolute error (\nmae\n).\n\n\nperformance(pred, measures = list(mse, medse, mae))\n#\n       mse     medse       mae \n#\n 42.684141  9.134965  4.536750\n\n\n\n\nFor the other types of learning problems and measures, calculating the performance basically\nworks in the same way.\n\n\nRequirements of performance measures\n\n\nNote that in order to calculate some performance measures it is required that you pass the\n\nTask\n or the \nfitted model\n in addition to the \nPrediction\n.\n\n\nFor example in order to assess the time needed for training (\ntimetrain\n), the fitted\nmodel has to be passed.\n\n\nperformance(pred, measures = timetrain, model = mod)\n#\n timetrain \n#\n     0.158\n\n\n\n\nFor many performance measures in cluster analysis the \nTask\n is required.\n\n\nlrn = makeLearner(\ncluster.kmeans\n, centers = 3)\nmod = train(lrn, mtcars.task)\npred = predict(mod, task = mtcars.task)\n\n## Calculate the Dunn index\nperformance(pred, measures = dunn, task = mtcars.task)\n#\n      dunn \n#\n 0.1462919\n\n\n\n\nMoreover, some measures require a certain type of prediction.\nFor example in binary classification in order to calculate the AUC (\nauc\n) -- the area\nunder the ROC (receiver operating characteristic) curve -- we have to make sure that posterior\nprobabilities are predicted.\nFor more information on ROC analysis, see the section on \nROC analysis\n.\n\n\nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n)\nmod = train(lrn, task = sonar.task)\npred = predict(mod, task = sonar.task)\n\nperformance(pred, measures = auc)\n#\n       auc \n#\n 0.9224018\n\n\n\n\nAlso bear in mind that many of the performance measures that are available for classification,\ne.g., the false positive rate (\nfpr\n), are only suitable for binary problems.\n\n\nAccess a performance measure\n\n\nPerformance measures in \nmlr\n are objects of class \nMeasure\n.\nIf you are interested in the properties or requirements of a single measure you can access it directly.\nSee the help page of \nMeasure\n for information on the individual slots.\n\n\n## Mean misclassification error\nstr(mmce)\n#\n List of 10\n#\n  $ id        : chr \nmmce\n\n#\n  $ minimize  : logi TRUE\n#\n  $ properties: chr [1:4] \nclassif\n \nclassif.multi\n \nreq.pred\n \nreq.truth\n\n#\n  $ fun       :function (task, model, pred, feats, extra.args)  \n#\n  $ extra.args: list()\n#\n  $ best      : num 0\n#\n  $ worst     : num 1\n#\n  $ name      : chr \nMean misclassification error\n\n#\n  $ note      : chr \n\n#\n  $ aggr      :List of 2\n#\n   ..$ id : chr \ntest.mean\n\n#\n   ..$ fun:function (task, perf.test, perf.train, measure, group, pred)  \n#\n   ..- attr(*, \nclass\n)= chr \nAggregation\n\n#\n  - attr(*, \nclass\n)= chr \nMeasure\n\n\n\n\n\nBinary classification: Plot performance versus threshold\n\n\nAs you may recall (see the previous section on \nmaking predictions\n)\nin binary classification we can adjust the threshold used to map probabilities to class labels.\nHelpful in this regard is are the functions \ngenerateThreshVsPerfData\n and \nplotThreshVsPerf\n, which generate and plot, respectively, the learner performance versus the threshold.\n\n\nFor more performance plots and automatic threshold tuning see \nhere\n.\n\n\nIn the following example we consider the \nSonar\n data set and\nplot the false positive rate (\nfpr\n), the false negative rate (\nfnr\n)\nas well as the misclassification rate (\nmmce\n) for all possible threshold values.\n\n\nlrn = makeLearner(\nclassif.lda\n, predict.type = \nprob\n)\nn = getTaskSize(sonar.task)\nmod = train(lrn, task = sonar.task, subset = seq(1, n, by = 2))\npred = predict(mod, task = sonar.task, subset = seq(2, n, by = 2))\n\n## Performance for the default threshold 0.5\nperformance(pred, measures = list(fpr, fnr, mmce))\n#\n       fpr       fnr      mmce \n#\n 0.2500000 0.3035714 0.2788462\n## Plot false negative and positive rates as well as the error rate versus the threshold\nd = generateThreshVsPerfData(pred, measures = list(fpr, fnr, mmce))\nplotThreshVsPerf(d)\n\n\n\n\n \n\n\nThere is an experimental \nggvis\n plotting function \nplotThreshVsPerfGGVIS\n which performs similarly\nto \nplotThreshVsPerf\n but instead of creating facetted subplots to visualize multiple learners and/or\nmultiple measures, one of them is mapped to an interactive sidebar which selects what to display.\n\n\nplotThreshVsPerfGGVIS(d)", 
            "title": "Performance"
        }, 
        {
            "location": "/performance/index.html#evaluating-learner-performance", 
            "text": "The quality of the predictions of a model in  mlr  can be assessed with respect to a\nnumber of different performance measures.\nIn order to calculate the performance measures, call  performance  on the object\nreturned by  predict  and specify the desired performance measures.", 
            "title": "Evaluating Learner Performance"
        }, 
        {
            "location": "/performance/index.html#available-performance-measures", 
            "text": "mlr  provides a large number of performance measures for all types of learning problems.\nTypical performance measures for  classification  are the mean misclassification error ( mmce ),\naccuracy ( acc ) or measures based on  ROC analysis .\nFor  regression  the mean of squared errors ( mse ) or mean of absolute errors ( mae )\nare usually considered.\nFor  clustering  tasks, measures such as the Dunn index ( dunn ) are provided,\nwhile for  survival  predictions, the Concordance Index ( cindex ) is\nsupported, and for  cost-sensitive  predictions the misclassification penalty\n( mcp ) and others. It is also possible to access the time to train the\nlearner ( timetrain ), the time to compute the prediction ( timepredict ) and their\nsum ( timeboth ) as performance measures.  To see which performance measures are implemented, have a look at the table of performance measures  and the  measures  documentation page.  If you want to implement an additional measure or include a measure with\nnon-standard misclassification costs, see the section on creating custom measures .", 
            "title": "Available performance measures"
        }, 
        {
            "location": "/performance/index.html#listing-measures", 
            "text": "The properties and requirements of the individual measures are shown in the  table of performance measures .  If you would like a list of available measures with certain properties or suitable for a\ncertain learning  Task  use the function  listMeasures .  ## Performance measures for classification with multiple classes\nlistMeasures( classif , properties =  classif.multi )\n#  [1]  featperc         mmce             timeboth         acc            \n#  [5]  multiclass.auc   ber              timepredict      timetrain \n## Performance measure suitable for the iris classification task\nlistMeasures(iris.task)\n#  [1]  featperc         mmce             timeboth         acc            \n#  [5]  multiclass.auc   ber              timepredict      timetrain", 
            "title": "Listing measures"
        }, 
        {
            "location": "/performance/index.html#calculate-performance-measures", 
            "text": "In the following example we fit a  gradient boosting machine  on a subset of the BostonHousing  data set and calculate the mean squared error\n( mse ) on the remaining observations.  n = getTaskSize(bh.task)\nlrn = makeLearner( regr.gbm , n.trees = 1000)\nmod = train(lrn, task = bh.task, subset = seq(1, n, 2))\npred = predict(mod, task = bh.task, subset = seq(2, n, 2))\n\n# mse is the default measure for regression, we do not have to specify\n# it here\nperformance(pred)\n#       mse \n#  42.68414  The following code computes the median of squared errors ( medse ) instead.  performance(pred, measures = medse)\n#     medse \n#  9.134965  Of course, we can also calculate multiple performance measures at once by simply passing a\nlist of measures which can also include  your own measure .  Calculate the mean squared error, median squared error and mean absolute error ( mae ).  performance(pred, measures = list(mse, medse, mae))\n#        mse     medse       mae \n#  42.684141  9.134965  4.536750  For the other types of learning problems and measures, calculating the performance basically\nworks in the same way.  Requirements of performance measures  Note that in order to calculate some performance measures it is required that you pass the Task  or the  fitted model  in addition to the  Prediction .  For example in order to assess the time needed for training ( timetrain ), the fitted\nmodel has to be passed.  performance(pred, measures = timetrain, model = mod)\n#  timetrain \n#      0.158  For many performance measures in cluster analysis the  Task  is required.  lrn = makeLearner( cluster.kmeans , centers = 3)\nmod = train(lrn, mtcars.task)\npred = predict(mod, task = mtcars.task)\n\n## Calculate the Dunn index\nperformance(pred, measures = dunn, task = mtcars.task)\n#       dunn \n#  0.1462919  Moreover, some measures require a certain type of prediction.\nFor example in binary classification in order to calculate the AUC ( auc ) -- the area\nunder the ROC (receiver operating characteristic) curve -- we have to make sure that posterior\nprobabilities are predicted.\nFor more information on ROC analysis, see the section on  ROC analysis .  lrn = makeLearner( classif.rpart , predict.type =  prob )\nmod = train(lrn, task = sonar.task)\npred = predict(mod, task = sonar.task)\n\nperformance(pred, measures = auc)\n#        auc \n#  0.9224018  Also bear in mind that many of the performance measures that are available for classification,\ne.g., the false positive rate ( fpr ), are only suitable for binary problems.", 
            "title": "Calculate performance measures"
        }, 
        {
            "location": "/performance/index.html#access-a-performance-measure", 
            "text": "Performance measures in  mlr  are objects of class  Measure .\nIf you are interested in the properties or requirements of a single measure you can access it directly.\nSee the help page of  Measure  for information on the individual slots.  ## Mean misclassification error\nstr(mmce)\n#  List of 10\n#   $ id        : chr  mmce \n#   $ minimize  : logi TRUE\n#   $ properties: chr [1:4]  classif   classif.multi   req.pred   req.truth \n#   $ fun       :function (task, model, pred, feats, extra.args)  \n#   $ extra.args: list()\n#   $ best      : num 0\n#   $ worst     : num 1\n#   $ name      : chr  Mean misclassification error \n#   $ note      : chr  \n#   $ aggr      :List of 2\n#    ..$ id : chr  test.mean \n#    ..$ fun:function (task, perf.test, perf.train, measure, group, pred)  \n#    ..- attr(*,  class )= chr  Aggregation \n#   - attr(*,  class )= chr  Measure", 
            "title": "Access a performance measure"
        }, 
        {
            "location": "/performance/index.html#binary-classification-plot-performance-versus-threshold", 
            "text": "As you may recall (see the previous section on  making predictions )\nin binary classification we can adjust the threshold used to map probabilities to class labels.\nHelpful in this regard is are the functions  generateThreshVsPerfData  and  plotThreshVsPerf , which generate and plot, respectively, the learner performance versus the threshold.  For more performance plots and automatic threshold tuning see  here .  In the following example we consider the  Sonar  data set and\nplot the false positive rate ( fpr ), the false negative rate ( fnr )\nas well as the misclassification rate ( mmce ) for all possible threshold values.  lrn = makeLearner( classif.lda , predict.type =  prob )\nn = getTaskSize(sonar.task)\nmod = train(lrn, task = sonar.task, subset = seq(1, n, by = 2))\npred = predict(mod, task = sonar.task, subset = seq(2, n, by = 2))\n\n## Performance for the default threshold 0.5\nperformance(pred, measures = list(fpr, fnr, mmce))\n#        fpr       fnr      mmce \n#  0.2500000 0.3035714 0.2788462\n## Plot false negative and positive rates as well as the error rate versus the threshold\nd = generateThreshVsPerfData(pred, measures = list(fpr, fnr, mmce))\nplotThreshVsPerf(d)     There is an experimental  ggvis  plotting function  plotThreshVsPerfGGVIS  which performs similarly\nto  plotThreshVsPerf  but instead of creating facetted subplots to visualize multiple learners and/or\nmultiple measures, one of them is mapped to an interactive sidebar which selects what to display.  plotThreshVsPerfGGVIS(d)", 
            "title": "Binary classification: Plot performance versus threshold"
        }, 
        {
            "location": "/resample/index.html", 
            "text": "Resampling\n\n\nIn order to assess the performance of a learning algorithm, resampling\nstrategies are usually used.\nThe entire data set is split into (multiple) training and test sets.\nYou train a learner on each training set, predict on the corresponding test set (sometimes\non the training set as well) and calculate some performance measure.\nThen the individual performance values are aggregated, typically by calculating the mean.\nThere exist various different resampling strategies, for example\ncross-validation and bootstrap, to mention just two popular approaches.\n\n\n\n\nIf you want to read up further details, the paper\n\nResampling Strategies for Model Assessment and Selection\n\nby Simon is proabably not a bad choice.\nBernd has also published a paper\n\nResampling methods for meta-model validation with recommendations for evolutionary computation\n\nwhich contains detailed descriptions and lots of statistical background information on resampling methods.\n\n\nIn \nmlr\n the resampling strategy can be chosen via the function \nmakeResampleDesc\n.\nThe supported resampling strategies are:\n\n\n\n\nCross-validation (\n\"CV\"\n),\n\n\nLeave-one-out cross-validation (\n\"LOO\"\"\n),\n\n\nRepeated cross-validation (\n\"RepCV\"\n),\n\n\nOut-of-bag bootstrap and other variants (\n\"Bootstrap\"\n),\n\n\nSubsampling, also called Monte-Carlo cross-validaton (\n\"Subsample\"\n),\n\n\nHoldout (training/test) (\n\"Holdout\"\n).\n\n\n\n\nThe \nresample\n function evaluates the performance of a \nLearner\n using\nthe specified resampling strategy for a given machine learning \nTask\n.\n\n\nIn the following example the performance of the\n\nCox proportional hazards model\n on the\n\nlung\n data set is calculated using \n3-fold cross-validation\n.\nGenerally, in \nK-fold cross-validation\n the data set \nD\n is partitioned into \nK\n subsets of\n(approximately) equal size.\nIn the \ni\n-th step of the \nK\n iterations, the \ni\n-th subset is\nused for testing, while the union of the remaining parts forms the training\nset.\nThe default performance measure in survival analysis is the concordance index (\ncindex\n).\n\n\n## Specify the resampling strategy (3-fold cross-validation)\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\n\n## Calculate the performance\nr = resample(\nsurv.coxph\n, lung.task, rdesc)\n#\n [Resample] cross-validation iter: 1\n#\n [Resample] cross-validation iter: 2\n#\n [Resample] cross-validation iter: 3\n#\n [Resample] Result: cindex.test.mean=0.627\nr\n#\n Resample Result\n#\n Task: lung-example\n#\n Learner: surv.coxph\n#\n cindex.aggr: 0.63\n#\n cindex.mean: 0.63\n#\n cindex.sd: 0.05\n#\n Runtime: 0.266612\n## peak a little bit into r\nnames(r)\n#\n  [1] \nlearner.id\n     \ntask.id\n        \nmeasures.train\n \nmeasures.test\n \n#\n  [5] \naggr\n           \npred\n           \nmodels\n         \nerr.msgs\n      \n#\n  [9] \nextract\n        \nruntime\n\nr$aggr\n#\n cindex.test.mean \n#\n        0.6271182\nr$measures.test\n#\n   iter    cindex\n#\n 1    1 0.5783027\n#\n 2    2 0.6324074\n#\n 3    3 0.6706444\nr$measures.train\n#\n   iter cindex\n#\n 1    1     NA\n#\n 2    2     NA\n#\n 3    3     NA\n\n\n\n\nr$measures.test\n gives the value of the performance measure on the 3 individual test\ndata sets.\n\nr$aggr\n shows the aggregated performance value.\nIts name, \n\"cindex.test.mean\"\n, indicates the performance measure, \ncindex\n,\nand the method used to aggregate the 3 individual performances.\n\ntest.mean\n is the default method and, as the name implies, takes the mean over the\nperformances on the 3 test data sets.\nNo predictions on the training data sets were made and thus \nr$measures.train\n contains missing values.\n\n\nIf predictions for the training set are required, too, set \npredict = \"train\"\nor \npredict = \"both\"\n\nin \nmakeResampleDesc\n. This is necessary for some bootstrap methods (\nb632\n and \nb632+\n) and\nwe will see some examples later on.\n\n\nr$pred\n is an object of class \nResamplePrediction\n.\nJust as a \nPrediction\n object (see the section on \nmaking predictions\n)\n\nr$pred\n has an element called \n\"data\"\n which is a \ndata.frame\n that contains the\npredictions and in case of a supervised learning problem the true values of the target\nvariable.\n\n\nhead(r$pred$data)\n#\n    id truth.time truth.event   response iter  set\n#\n 2   1        455        TRUE -0.4951788    1 test\n#\n 4   2        210        TRUE  0.9573824    1 test\n#\n 7   4        310        TRUE  0.8069059    1 test\n#\n 17 10        613        TRUE  0.1918188    1 test\n#\n 19 12         61        TRUE  0.6638736    1 test\n#\n 22 14         81        TRUE -0.1873917    1 test\n\n\n\n\nThe columns \niter\n and \nset\nindicate the resampling iteration and\nif an individual prediction was made on the test or the training data set.\n\n\nIn the above example the performance measure is the concordance index (\ncindex\n).\nOf course, it is  possible to compute multiple performance measures at once by\npassing a list of measures\n(see also the previous section on \nevaluating learner performance\n).\n\n\nIn the following we estimate the Dunn index (\ndunn\n), the Davies-Bouldin cluster\nseparation measure (\ndb\n), and the time for training the learner (\ntimetrain\n)\nby \nsubsampling\n with 5 iterations.\nIn each iteration the data set \nD\n is randomly partitioned into a\ntraining and a test set according to a given percentage, e.g., 2/3\ntraining and 1/3 test set. If there is just one iteration, the strategy\nis commonly called \nholdout\n or \ntest sample estimation\n.\n\n\n## cluster iris feature data\ntask = makeClusterTask(data = iris[,-5])\n## Subsampling with 5 iterations and default split 2/3\nrdesc = makeResampleDesc(\nSubsample\n, iters = 5)\n## Subsampling with 5 iterations and 4/5 training data\nrdesc = makeResampleDesc(\nSubsample\n, iters = 5, split = 4/5)\n\n## Calculate the three performance measures\nr = resample(\ncluster.kmeans\n, task, rdesc, measures = list(dunn, db, timetrain))\n#\n [Resample] subsampling iter: 1\n#\n [Resample] subsampling iter: 2\n#\n [Resample] subsampling iter: 3\n#\n [Resample] subsampling iter: 4\n#\n [Resample] subsampling iter: 5\n#\n [Resample] Result: dunn.test.mean=0.274,db.test.mean=0.51,timetrain.test.mean=0.002\nr$aggr\n#\n      dunn.test.mean        db.test.mean timetrain.test.mean \n#\n           0.2738893           0.5103655           0.0020000\n\n\n\n\nStratified resampling\n\n\nFor classification, it is usually desirable to have the same proportion of the classes in all of the partitions of the original data set. Stratified resampling ensures this.\nThis is particularly useful in case of imbalanced classes and small data sets. Otherwise it may happen, for example,\nthat observations of less frequent classes are missing in some of the training sets which can\ndecrease the performance of the learner, or lead to model crashes\nIn order to conduct stratified resampling, set \nstratify = TRUE\n when calling \nmakeResampleDesc\n.\n\n\n## 3-fold cross-validation\nrdesc = makeResampleDesc(\nCV\n, iters = 3, stratify = TRUE)\n\nr = resample(\nclassif.lda\n, iris.task, rdesc)\n#\n [Resample] cross-validation iter: 1\n#\n [Resample] cross-validation iter: 2\n#\n [Resample] cross-validation iter: 3\n#\n [Resample] Result: mmce.test.mean=0.02\n\n\n\n\nStratification is also available for survival tasks.\nHere the stratification balances the censoring rate.\n\n\nSometimes it is required to also stratify on the input data, e.g. to ensure that all subgroups are represented in all training and test sets.\nTo stratify on the input columns, specify factor columns of your task data via \nstratify.cols\n\n\nrdesc = makeResampleDesc(\nCV\n, iters = 3, stratify.cols = \nchas\n)\nr = resample(\nregr.rpart\n, bh.task, rdesc)\n#\n [Resample] cross-validation iter: 1\n#\n [Resample] cross-validation iter: 2\n#\n [Resample] cross-validation iter: 3\n#\n [Resample] Result: mse.test.mean=23.2\n\n\n\n\nAccessing individual learner models\n\n\nIn each resampling iteration a \nLearner\n is fitted on the respective training set.\nBy default, the resulting \nWrappedModel\ns are not returned by \nresample\n.\nIf you want to keep them, set \nmodels = TRUE\n when calling \nresample\n.\n\n\n## 3-fold cross-validation\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\n\nr = resample(\nclassif.lda\n, iris.task, rdesc, models = TRUE)\n#\n [Resample] cross-validation iter: 1\n#\n [Resample] cross-validation iter: 2\n#\n [Resample] cross-validation iter: 3\n#\n [Resample] Result: mmce.test.mean=0.02\nr$models\n#\n [[1]]\n#\n Model for learner.id=classif.lda; learner.class=classif.lda\n#\n Trained on: task.id = iris-example; obs = 100; features = 4\n#\n Hyperparameters: \n#\n \n#\n [[2]]\n#\n Model for learner.id=classif.lda; learner.class=classif.lda\n#\n Trained on: task.id = iris-example; obs = 100; features = 4\n#\n Hyperparameters: \n#\n \n#\n [[3]]\n#\n Model for learner.id=classif.lda; learner.class=classif.lda\n#\n Trained on: task.id = iris-example; obs = 100; features = 4\n#\n Hyperparameters:\n\n\n\n\nKeeping only certain information instead of entire \nmodels\n, for example the\nvariable importance in a regression tree, can be achieved using the \nextract\n argument.\nThe function passed to \nextract\n is applied to each \nmodel\n fitted on one of\nthe 3 training sets.\n\n\n## 3-fold cross-validation\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\n\n## Extract the variable importance in a regression tree\nr = resample(\nregr.rpart\n, bh.task, rdesc,\n    extract = function(x) x$learner.model$variable.importance)\n#\n [Resample] cross-validation iter: 1\n#\n [Resample] cross-validation iter: 2\n#\n [Resample] cross-validation iter: 3\n#\n [Resample] Result: mse.test.mean=30.3\nr$extract\n#\n [[1]]\n#\n         rm      lstat       crim      indus        age    ptratio \n#\n 15228.2872 10742.2277  3893.2744  3651.6232  2601.5262  2551.8492 \n#\n        dis        nox        rad        tax         zn \n#\n  2498.2748  2419.5269  1014.2609   743.3742   308.8209 \n#\n \n#\n [[2]]\n#\n       lstat         nox         age       indus        crim          rm \n#\n 15725.19021  9323.20270  8474.23077  8358.67000  8251.74446  7332.59637 \n#\n          zn         dis         tax         rad     ptratio           b \n#\n  6151.29577  2741.12074  2055.67537  1216.01398   634.78381    71.00088 \n#\n \n#\n [[3]]\n#\n         rm      lstat        age    ptratio        nox        dis \n#\n 15890.9279 13262.3672  4296.4175  3678.6651  3668.4944  3512.2753 \n#\n       crim        tax      indus         zn          b        rad \n#\n  3474.5883  2844.9918  1437.7900  1284.4714   578.6932   496.2382\n\n\n\n\nResample descriptions and resample instances\n\n\nAs shown above, the function \nmakeResampleDesc\n is used to specify the resampling strategy.\n\n\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\nstr(rdesc)\n#\n List of 4\n#\n  $ id      : chr \ncross-validation\n\n#\n  $ iters   : int 3\n#\n  $ predict : chr \ntest\n\n#\n  $ stratify: logi FALSE\n#\n  - attr(*, \nclass\n)= chr [1:2] \nCVDesc\n \nResampleDesc\n\n\n\n\n\nThe result \nrdesc\nis an object of class \nResampleDesc\n and contains,\nas the name implies, a description of the resampling strategy.\nIn principle, this is an instruction for drawing training and test sets including\nthe necessary parameters like the number of iterations, the sizes of the training and test\nsets etc.\n\n\nBased on this description, the data set is randomly partitioned into multiple training and\ntest sets.\nFor each iteration, we get a set of index vectors indicating the training and test examples.\nThese are stored in a \nResampleInstance\n.\n\n\nIf a \nResampleDesc\n is passed to \nresample\n, it is instantiated internally.\nNaturally, it is also possible to pass a \nResampleInstance\n directly.\n\n\nA \nResampleInstance\n can be created through the function\n\nmakeResampleInstance\n given a \nResampleDesc\n and either the size of\nthe data set at hand or the \nTask\n.\nIt basically performs the random drawing of indices to separate the data into training and\ntest sets according to the description.\n\n\n## Create a resample instance based an a task\nrin = makeResampleInstance(rdesc, task = iris.task)\nrin\n#\n Resample instance for 150 cases.\n#\n Resample description: cross-validation with 3 iterations.\n#\n Predict: test\n#\n Stratification: FALSE\n\n## Create a resample instance given the size of the data set\nrin = makeResampleInstance(rdesc, size = nrow(iris))\nstr(rin)\n#\n List of 5\n#\n  $ desc      :List of 4\n#\n   ..$ id      : chr \ncross-validation\n\n#\n   ..$ iters   : int 3\n#\n   ..$ predict : chr \ntest\n\n#\n   ..$ stratify: logi FALSE\n#\n   ..- attr(*, \nclass\n)= chr [1:2] \nCVDesc\n \nResampleDesc\n\n#\n  $ size      : int 150\n#\n  $ train.inds:List of 3\n#\n   ..$ : int [1:100] 36 81 6 82 120 110 118 132 105 61 ...\n#\n   ..$ : int [1:100] 6 119 120 110 121 118 99 100 29 127 ...\n#\n   ..$ : int [1:100] 36 81 82 119 121 99 132 105 61 115 ...\n#\n  $ test.inds :List of 3\n#\n   ..$ : int [1:50] 2 3 4 5 7 9 11 16 22 24 ...\n#\n   ..$ : int [1:50] 8 12 17 19 20 23 25 27 32 33 ...\n#\n   ..$ : int [1:50] 1 6 10 13 14 15 18 21 29 31 ...\n#\n  $ group     : Factor w/ 0 levels: \n#\n  - attr(*, \nclass\n)= chr \nResampleInstance\n\n\n## Access the indices of the training observations in iteration 3\nrin$train.inds[[3]]\n#\n   [1]  36  81  82 119 121  99 132 105  61 115  17  42   4  71   5  79  30\n#\n  [18] 113 138  19 150  77  58  92 114 133   8 109  33 145  22 111  97  24\n#\n  [35]   7  44   3  20 134  96  16  43 149   9  46  32 139  87   2  11  52\n#\n  [52]  86  40 141 142  72  54  48  83  64  90 112 148 129 137 116 143  69\n#\n  [69]  84  25  80  37  38  75 130 126 135 107 146  26  12  98  55 124  60\n#\n  [86]  63 117  23  67  73  28 106  76  50 144  59  47 102  56  27\n\n\n\n\nWhile having two separate objects, resample descriptions and instances as well as the \nresample\n\nfunction seems overly complicated, it has several advantages:\n\n\n\n\nResample instances allow for paired experiments, that is comparing the performance\n  of several learners on exactly the same training and test sets.\n  This is particularly useful if you want to add another method to a comparison experiment\n  you already did.\n\n\n\n\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\nrin = makeResampleInstance(rdesc, task = iris.task)\n\n## Calculate the performance of two learners based on the same resample instance\nr.lda = resample(\nclassif.lda\n, iris.task, rin, show.info = FALSE)\nr.rpart = resample(\nclassif.rpart\n, iris.task, rin, show.info = FALSE)\nr.lda$aggr\n#\n mmce.test.mean \n#\n     0.02666667\nr.rpart$aggr\n#\n mmce.test.mean \n#\n           0.06\n\n\n\n\n\n\nIt is easy to add other resampling methods later on. You can\n  simply derive from the \nResampleInstance\n\n  class, but you do not have to touch any methods that use the\n  resampling strategy.\n\n\n\n\nAs mentioned above, when calling \nmakeResampleInstance\n the index sets are drawn randomly.\nMainly for \nholdout\n (\ntest sample\n) \nestimation\n you might want full control about the training\nand tests set and specify them manually.\nThis can be done using the function \nmakeFixedHoldoutInstance\n.\n\n\nrin = makeFixedHoldoutInstance(train.inds = 1:100, test.inds = 101:150, size = 150)\nrin\n#\n Resample instance for 150 cases.\n#\n Resample description: holdout with 0.67 split rate.\n#\n Predict: test\n#\n Stratification: FALSE\n\n\n\n\nAggregating performance values\n\n\nIn resampling we get (for each measure we wish to calculate) one performance\nvalue (on the test set, training set, or both) for each iteration.\nSubsequently, these are aggregated.\nAs mentioned above, mainly the mean over the performance values on the test data sets\n(\ntest.mean\n) is calculated.\n\n\nFor example, a 10-fold cross validation computes 10 values for the chosen\nperformance measure.\nThe aggregated value is the mean of these 10 numbers.\n\nmlr\n knows how to handle it because each \nMeasure\n knows how it is aggregated:\n\n\n## Mean misclassification error\nmmce$aggr\n#\n Aggregation function: test.mean\n\n## Root mean square error\nrmse$aggr\n#\n Aggregation function: test.sqrt.of.mean\n\n\n\n\nThe aggregation method of a \nMeasure\n can be changed via the function \nsetAggregation\n.\nSee the documentation of \naggregations\n for available methods.\n\n\nExample: Different measures and aggregations\n\n\ntest.median\n computes the median of the performance values on the test sets.\n\n\n## We use the mean error rate and the median of the true positive rates\nm1 = mmce\nm2 = setAggregation(tpr, test.median)\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\nr = resample(\nclassif.rpart\n, sonar.task, rdesc, measures = list(m1, m2))\n#\n [Resample] cross-validation iter: 1\n#\n [Resample] cross-validation iter: 2\n#\n [Resample] cross-validation iter: 3\n#\n [Resample] Result: mmce.test.mean=0.293,tpr.test.median=0.735\nr$aggr\n#\n  mmce.test.mean tpr.test.median \n#\n       0.2930987       0.7352941\n\n\n\n\nExample: Calculating the training error\n\n\nHere we calculate the mean misclassification error (\nmmce\n) on the training and the test\ndata sets. Note that we have to set \npredict = \"both\"\nwhen calling \nmakeResampleDesc\n\nin order to get predictions on both data sets, training and test.\n\n\nmmce.train.mean = setAggregation(mmce, train.mean)\nrdesc = makeResampleDesc(\nCV\n, iters = 3, predict = \nboth\n)\nr = resample(\nclassif.rpart\n, iris.task, rdesc, measures = list(mmce, mmce.train.mean))\n#\n [Resample] cross-validation iter: 1\n#\n [Resample] cross-validation iter: 2\n#\n [Resample] cross-validation iter: 3\n#\n [Resample] Result: mmce.test.mean=0.0467,mmce.train.mean=0.0367\nr$measures.train\n#\n   iter mmce mmce\n#\n 1    1 0.04 0.04\n#\n 2    2 0.03 0.03\n#\n 3    3 0.04 0.04\nr$aggr\n#\n  mmce.test.mean mmce.train.mean \n#\n      0.04666667      0.03666667\n\n\n\n\nExample: Bootstrap\n\n\nIn \nout-of-bag bootstrap estimation\n \nB\n new data sets \nD_1\n to \nD_B\n are drawn from the\ndata set \nD\n with replacement, each of the same size as \nD\n.\nIn the \ni\n-th iteration, \nD_i\n forms the training set, while the remaining elements from\n\nD\n, i.e., elements not in the training set, form the test set.\n\n\n\n\n\nThe variants \nb632\n and \nb632+\n calculate a convex combination of the training performance and\nthe out-of-bag bootstrap performance and thus require predictions on the training sets and an\nappropriate aggregation strategy.\n\n\nrdesc = makeResampleDesc(\nBootstrap\n, predict = \nboth\n, iters = 10)\nb632.mmce = setAggregation(mmce, b632)\nb632plus.mmce = setAggregation(mmce, b632plus)\nb632.mmce\n#\n Name: Mean misclassification error\n#\n Performance measure: mmce\n#\n Properties: classif,classif.multi,req.pred,req.truth\n#\n Minimize: TRUE\n#\n Best: 0; Worst: 1\n#\n Aggregated by: b632\n#\n Note:\n\nr = resample(\nclassif.rpart\n, iris.task, rdesc,\n    measures = list(mmce, b632.mmce, b632plus.mmce), show.info = FALSE)\nhead(r$measures.train)\n#\n   iter        mmce        mmce        mmce\n#\n 1    1 0.026666667 0.026666667 0.026666667\n#\n 2    2 0.026666667 0.026666667 0.026666667\n#\n 3    3 0.006666667 0.006666667 0.006666667\n#\n 4    4 0.026666667 0.026666667 0.026666667\n#\n 5    5 0.033333333 0.033333333 0.033333333\n#\n 6    6 0.013333333 0.013333333 0.013333333\nr$aggr\n#\n mmce.test.mean      mmce.b632  mmce.b632plus \n#\n     0.07051905     0.05389071     0.05496489\n\n\n\n\nConvenience functions\n\n\nWhen quickly trying out some learners, it can get tedious to write the \nR\n\ncode for generating a resample instance, setting the aggregation strategy and so\non. For this reason \nmlr\n provides some convenience functions for the\nfrequently used resampling strategies, for example \nholdout\n,\n\ncrossval\n or \nbootstrapB632\n. But note that you do not\nhave as much control and flexibility as when using \nresample\n with a resample\ndescription or instance.\n\n\nholdout(\nregr.lm\n, bh.task, measures = list(mse, mae))\ncrossval(\nclassif.lda\n, iris.task, iters = 3, measures = list(mmce, ber))", 
            "title": "Resampling"
        }, 
        {
            "location": "/resample/index.html#resampling", 
            "text": "In order to assess the performance of a learning algorithm, resampling\nstrategies are usually used.\nThe entire data set is split into (multiple) training and test sets.\nYou train a learner on each training set, predict on the corresponding test set (sometimes\non the training set as well) and calculate some performance measure.\nThen the individual performance values are aggregated, typically by calculating the mean.\nThere exist various different resampling strategies, for example\ncross-validation and bootstrap, to mention just two popular approaches.   If you want to read up further details, the paper Resampling Strategies for Model Assessment and Selection \nby Simon is proabably not a bad choice.\nBernd has also published a paper Resampling methods for meta-model validation with recommendations for evolutionary computation \nwhich contains detailed descriptions and lots of statistical background information on resampling methods.  In  mlr  the resampling strategy can be chosen via the function  makeResampleDesc .\nThe supported resampling strategies are:   Cross-validation ( \"CV\" ),  Leave-one-out cross-validation ( \"LOO\"\" ),  Repeated cross-validation ( \"RepCV\" ),  Out-of-bag bootstrap and other variants ( \"Bootstrap\" ),  Subsampling, also called Monte-Carlo cross-validaton ( \"Subsample\" ),  Holdout (training/test) ( \"Holdout\" ).   The  resample  function evaluates the performance of a  Learner  using\nthe specified resampling strategy for a given machine learning  Task .  In the following example the performance of the Cox proportional hazards model  on the lung  data set is calculated using  3-fold cross-validation .\nGenerally, in  K-fold cross-validation  the data set  D  is partitioned into  K  subsets of\n(approximately) equal size.\nIn the  i -th step of the  K  iterations, the  i -th subset is\nused for testing, while the union of the remaining parts forms the training\nset.\nThe default performance measure in survival analysis is the concordance index ( cindex ).  ## Specify the resampling strategy (3-fold cross-validation)\nrdesc = makeResampleDesc( CV , iters = 3)\n\n## Calculate the performance\nr = resample( surv.coxph , lung.task, rdesc)\n#  [Resample] cross-validation iter: 1\n#  [Resample] cross-validation iter: 2\n#  [Resample] cross-validation iter: 3\n#  [Resample] Result: cindex.test.mean=0.627\nr\n#  Resample Result\n#  Task: lung-example\n#  Learner: surv.coxph\n#  cindex.aggr: 0.63\n#  cindex.mean: 0.63\n#  cindex.sd: 0.05\n#  Runtime: 0.266612\n## peak a little bit into r\nnames(r)\n#   [1]  learner.id       task.id          measures.train   measures.test  \n#   [5]  aggr             pred             models           err.msgs       \n#   [9]  extract          runtime \nr$aggr\n#  cindex.test.mean \n#         0.6271182\nr$measures.test\n#    iter    cindex\n#  1    1 0.5783027\n#  2    2 0.6324074\n#  3    3 0.6706444\nr$measures.train\n#    iter cindex\n#  1    1     NA\n#  2    2     NA\n#  3    3     NA  r$measures.test  gives the value of the performance measure on the 3 individual test\ndata sets. r$aggr  shows the aggregated performance value.\nIts name,  \"cindex.test.mean\" , indicates the performance measure,  cindex ,\nand the method used to aggregate the 3 individual performances. test.mean  is the default method and, as the name implies, takes the mean over the\nperformances on the 3 test data sets.\nNo predictions on the training data sets were made and thus  r$measures.train  contains missing values.  If predictions for the training set are required, too, set  predict = \"train\" or  predict = \"both\" \nin  makeResampleDesc . This is necessary for some bootstrap methods ( b632  and  b632+ ) and\nwe will see some examples later on.  r$pred  is an object of class  ResamplePrediction .\nJust as a  Prediction  object (see the section on  making predictions ) r$pred  has an element called  \"data\"  which is a  data.frame  that contains the\npredictions and in case of a supervised learning problem the true values of the target\nvariable.  head(r$pred$data)\n#     id truth.time truth.event   response iter  set\n#  2   1        455        TRUE -0.4951788    1 test\n#  4   2        210        TRUE  0.9573824    1 test\n#  7   4        310        TRUE  0.8069059    1 test\n#  17 10        613        TRUE  0.1918188    1 test\n#  19 12         61        TRUE  0.6638736    1 test\n#  22 14         81        TRUE -0.1873917    1 test  The columns  iter  and  set indicate the resampling iteration and\nif an individual prediction was made on the test or the training data set.  In the above example the performance measure is the concordance index ( cindex ).\nOf course, it is  possible to compute multiple performance measures at once by\npassing a list of measures\n(see also the previous section on  evaluating learner performance ).  In the following we estimate the Dunn index ( dunn ), the Davies-Bouldin cluster\nseparation measure ( db ), and the time for training the learner ( timetrain )\nby  subsampling  with 5 iterations.\nIn each iteration the data set  D  is randomly partitioned into a\ntraining and a test set according to a given percentage, e.g., 2/3\ntraining and 1/3 test set. If there is just one iteration, the strategy\nis commonly called  holdout  or  test sample estimation .  ## cluster iris feature data\ntask = makeClusterTask(data = iris[,-5])\n## Subsampling with 5 iterations and default split 2/3\nrdesc = makeResampleDesc( Subsample , iters = 5)\n## Subsampling with 5 iterations and 4/5 training data\nrdesc = makeResampleDesc( Subsample , iters = 5, split = 4/5)\n\n## Calculate the three performance measures\nr = resample( cluster.kmeans , task, rdesc, measures = list(dunn, db, timetrain))\n#  [Resample] subsampling iter: 1\n#  [Resample] subsampling iter: 2\n#  [Resample] subsampling iter: 3\n#  [Resample] subsampling iter: 4\n#  [Resample] subsampling iter: 5\n#  [Resample] Result: dunn.test.mean=0.274,db.test.mean=0.51,timetrain.test.mean=0.002\nr$aggr\n#       dunn.test.mean        db.test.mean timetrain.test.mean \n#            0.2738893           0.5103655           0.0020000", 
            "title": "Resampling"
        }, 
        {
            "location": "/resample/index.html#stratified-resampling", 
            "text": "For classification, it is usually desirable to have the same proportion of the classes in all of the partitions of the original data set. Stratified resampling ensures this.\nThis is particularly useful in case of imbalanced classes and small data sets. Otherwise it may happen, for example,\nthat observations of less frequent classes are missing in some of the training sets which can\ndecrease the performance of the learner, or lead to model crashes\nIn order to conduct stratified resampling, set  stratify = TRUE  when calling  makeResampleDesc .  ## 3-fold cross-validation\nrdesc = makeResampleDesc( CV , iters = 3, stratify = TRUE)\n\nr = resample( classif.lda , iris.task, rdesc)\n#  [Resample] cross-validation iter: 1\n#  [Resample] cross-validation iter: 2\n#  [Resample] cross-validation iter: 3\n#  [Resample] Result: mmce.test.mean=0.02  Stratification is also available for survival tasks.\nHere the stratification balances the censoring rate.  Sometimes it is required to also stratify on the input data, e.g. to ensure that all subgroups are represented in all training and test sets.\nTo stratify on the input columns, specify factor columns of your task data via  stratify.cols  rdesc = makeResampleDesc( CV , iters = 3, stratify.cols =  chas )\nr = resample( regr.rpart , bh.task, rdesc)\n#  [Resample] cross-validation iter: 1\n#  [Resample] cross-validation iter: 2\n#  [Resample] cross-validation iter: 3\n#  [Resample] Result: mse.test.mean=23.2", 
            "title": "Stratified resampling"
        }, 
        {
            "location": "/resample/index.html#accessing-individual-learner-models", 
            "text": "In each resampling iteration a  Learner  is fitted on the respective training set.\nBy default, the resulting  WrappedModel s are not returned by  resample .\nIf you want to keep them, set  models = TRUE  when calling  resample .  ## 3-fold cross-validation\nrdesc = makeResampleDesc( CV , iters = 3)\n\nr = resample( classif.lda , iris.task, rdesc, models = TRUE)\n#  [Resample] cross-validation iter: 1\n#  [Resample] cross-validation iter: 2\n#  [Resample] cross-validation iter: 3\n#  [Resample] Result: mmce.test.mean=0.02\nr$models\n#  [[1]]\n#  Model for learner.id=classif.lda; learner.class=classif.lda\n#  Trained on: task.id = iris-example; obs = 100; features = 4\n#  Hyperparameters: \n#  \n#  [[2]]\n#  Model for learner.id=classif.lda; learner.class=classif.lda\n#  Trained on: task.id = iris-example; obs = 100; features = 4\n#  Hyperparameters: \n#  \n#  [[3]]\n#  Model for learner.id=classif.lda; learner.class=classif.lda\n#  Trained on: task.id = iris-example; obs = 100; features = 4\n#  Hyperparameters:  Keeping only certain information instead of entire  models , for example the\nvariable importance in a regression tree, can be achieved using the  extract  argument.\nThe function passed to  extract  is applied to each  model  fitted on one of\nthe 3 training sets.  ## 3-fold cross-validation\nrdesc = makeResampleDesc( CV , iters = 3)\n\n## Extract the variable importance in a regression tree\nr = resample( regr.rpart , bh.task, rdesc,\n    extract = function(x) x$learner.model$variable.importance)\n#  [Resample] cross-validation iter: 1\n#  [Resample] cross-validation iter: 2\n#  [Resample] cross-validation iter: 3\n#  [Resample] Result: mse.test.mean=30.3\nr$extract\n#  [[1]]\n#          rm      lstat       crim      indus        age    ptratio \n#  15228.2872 10742.2277  3893.2744  3651.6232  2601.5262  2551.8492 \n#         dis        nox        rad        tax         zn \n#   2498.2748  2419.5269  1014.2609   743.3742   308.8209 \n#  \n#  [[2]]\n#        lstat         nox         age       indus        crim          rm \n#  15725.19021  9323.20270  8474.23077  8358.67000  8251.74446  7332.59637 \n#           zn         dis         tax         rad     ptratio           b \n#   6151.29577  2741.12074  2055.67537  1216.01398   634.78381    71.00088 \n#  \n#  [[3]]\n#          rm      lstat        age    ptratio        nox        dis \n#  15890.9279 13262.3672  4296.4175  3678.6651  3668.4944  3512.2753 \n#        crim        tax      indus         zn          b        rad \n#   3474.5883  2844.9918  1437.7900  1284.4714   578.6932   496.2382", 
            "title": "Accessing individual learner models"
        }, 
        {
            "location": "/resample/index.html#resample-descriptions-and-resample-instances", 
            "text": "As shown above, the function  makeResampleDesc  is used to specify the resampling strategy.  rdesc = makeResampleDesc( CV , iters = 3)\nstr(rdesc)\n#  List of 4\n#   $ id      : chr  cross-validation \n#   $ iters   : int 3\n#   $ predict : chr  test \n#   $ stratify: logi FALSE\n#   - attr(*,  class )= chr [1:2]  CVDesc   ResampleDesc   The result  rdesc is an object of class  ResampleDesc  and contains,\nas the name implies, a description of the resampling strategy.\nIn principle, this is an instruction for drawing training and test sets including\nthe necessary parameters like the number of iterations, the sizes of the training and test\nsets etc.  Based on this description, the data set is randomly partitioned into multiple training and\ntest sets.\nFor each iteration, we get a set of index vectors indicating the training and test examples.\nThese are stored in a  ResampleInstance .  If a  ResampleDesc  is passed to  resample , it is instantiated internally.\nNaturally, it is also possible to pass a  ResampleInstance  directly.  A  ResampleInstance  can be created through the function makeResampleInstance  given a  ResampleDesc  and either the size of\nthe data set at hand or the  Task .\nIt basically performs the random drawing of indices to separate the data into training and\ntest sets according to the description.  ## Create a resample instance based an a task\nrin = makeResampleInstance(rdesc, task = iris.task)\nrin\n#  Resample instance for 150 cases.\n#  Resample description: cross-validation with 3 iterations.\n#  Predict: test\n#  Stratification: FALSE\n\n## Create a resample instance given the size of the data set\nrin = makeResampleInstance(rdesc, size = nrow(iris))\nstr(rin)\n#  List of 5\n#   $ desc      :List of 4\n#    ..$ id      : chr  cross-validation \n#    ..$ iters   : int 3\n#    ..$ predict : chr  test \n#    ..$ stratify: logi FALSE\n#    ..- attr(*,  class )= chr [1:2]  CVDesc   ResampleDesc \n#   $ size      : int 150\n#   $ train.inds:List of 3\n#    ..$ : int [1:100] 36 81 6 82 120 110 118 132 105 61 ...\n#    ..$ : int [1:100] 6 119 120 110 121 118 99 100 29 127 ...\n#    ..$ : int [1:100] 36 81 82 119 121 99 132 105 61 115 ...\n#   $ test.inds :List of 3\n#    ..$ : int [1:50] 2 3 4 5 7 9 11 16 22 24 ...\n#    ..$ : int [1:50] 8 12 17 19 20 23 25 27 32 33 ...\n#    ..$ : int [1:50] 1 6 10 13 14 15 18 21 29 31 ...\n#   $ group     : Factor w/ 0 levels: \n#   - attr(*,  class )= chr  ResampleInstance \n\n## Access the indices of the training observations in iteration 3\nrin$train.inds[[3]]\n#    [1]  36  81  82 119 121  99 132 105  61 115  17  42   4  71   5  79  30\n#   [18] 113 138  19 150  77  58  92 114 133   8 109  33 145  22 111  97  24\n#   [35]   7  44   3  20 134  96  16  43 149   9  46  32 139  87   2  11  52\n#   [52]  86  40 141 142  72  54  48  83  64  90 112 148 129 137 116 143  69\n#   [69]  84  25  80  37  38  75 130 126 135 107 146  26  12  98  55 124  60\n#   [86]  63 117  23  67  73  28 106  76  50 144  59  47 102  56  27  While having two separate objects, resample descriptions and instances as well as the  resample \nfunction seems overly complicated, it has several advantages:   Resample instances allow for paired experiments, that is comparing the performance\n  of several learners on exactly the same training and test sets.\n  This is particularly useful if you want to add another method to a comparison experiment\n  you already did.   rdesc = makeResampleDesc( CV , iters = 3)\nrin = makeResampleInstance(rdesc, task = iris.task)\n\n## Calculate the performance of two learners based on the same resample instance\nr.lda = resample( classif.lda , iris.task, rin, show.info = FALSE)\nr.rpart = resample( classif.rpart , iris.task, rin, show.info = FALSE)\nr.lda$aggr\n#  mmce.test.mean \n#      0.02666667\nr.rpart$aggr\n#  mmce.test.mean \n#            0.06   It is easy to add other resampling methods later on. You can\n  simply derive from the  ResampleInstance \n  class, but you do not have to touch any methods that use the\n  resampling strategy.   As mentioned above, when calling  makeResampleInstance  the index sets are drawn randomly.\nMainly for  holdout  ( test sample )  estimation  you might want full control about the training\nand tests set and specify them manually.\nThis can be done using the function  makeFixedHoldoutInstance .  rin = makeFixedHoldoutInstance(train.inds = 1:100, test.inds = 101:150, size = 150)\nrin\n#  Resample instance for 150 cases.\n#  Resample description: holdout with 0.67 split rate.\n#  Predict: test\n#  Stratification: FALSE", 
            "title": "Resample descriptions and resample instances"
        }, 
        {
            "location": "/resample/index.html#aggregating-performance-values", 
            "text": "In resampling we get (for each measure we wish to calculate) one performance\nvalue (on the test set, training set, or both) for each iteration.\nSubsequently, these are aggregated.\nAs mentioned above, mainly the mean over the performance values on the test data sets\n( test.mean ) is calculated.  For example, a 10-fold cross validation computes 10 values for the chosen\nperformance measure.\nThe aggregated value is the mean of these 10 numbers. mlr  knows how to handle it because each  Measure  knows how it is aggregated:  ## Mean misclassification error\nmmce$aggr\n#  Aggregation function: test.mean\n\n## Root mean square error\nrmse$aggr\n#  Aggregation function: test.sqrt.of.mean  The aggregation method of a  Measure  can be changed via the function  setAggregation .\nSee the documentation of  aggregations  for available methods.  Example: Different measures and aggregations  test.median  computes the median of the performance values on the test sets.  ## We use the mean error rate and the median of the true positive rates\nm1 = mmce\nm2 = setAggregation(tpr, test.median)\nrdesc = makeResampleDesc( CV , iters = 3)\nr = resample( classif.rpart , sonar.task, rdesc, measures = list(m1, m2))\n#  [Resample] cross-validation iter: 1\n#  [Resample] cross-validation iter: 2\n#  [Resample] cross-validation iter: 3\n#  [Resample] Result: mmce.test.mean=0.293,tpr.test.median=0.735\nr$aggr\n#   mmce.test.mean tpr.test.median \n#        0.2930987       0.7352941  Example: Calculating the training error  Here we calculate the mean misclassification error ( mmce ) on the training and the test\ndata sets. Note that we have to set  predict = \"both\" when calling  makeResampleDesc \nin order to get predictions on both data sets, training and test.  mmce.train.mean = setAggregation(mmce, train.mean)\nrdesc = makeResampleDesc( CV , iters = 3, predict =  both )\nr = resample( classif.rpart , iris.task, rdesc, measures = list(mmce, mmce.train.mean))\n#  [Resample] cross-validation iter: 1\n#  [Resample] cross-validation iter: 2\n#  [Resample] cross-validation iter: 3\n#  [Resample] Result: mmce.test.mean=0.0467,mmce.train.mean=0.0367\nr$measures.train\n#    iter mmce mmce\n#  1    1 0.04 0.04\n#  2    2 0.03 0.03\n#  3    3 0.04 0.04\nr$aggr\n#   mmce.test.mean mmce.train.mean \n#       0.04666667      0.03666667  Example: Bootstrap  In  out-of-bag bootstrap estimation   B  new data sets  D_1  to  D_B  are drawn from the\ndata set  D  with replacement, each of the same size as  D .\nIn the  i -th iteration,  D_i  forms the training set, while the remaining elements from D , i.e., elements not in the training set, form the test set.   The variants  b632  and  b632+  calculate a convex combination of the training performance and\nthe out-of-bag bootstrap performance and thus require predictions on the training sets and an\nappropriate aggregation strategy.  rdesc = makeResampleDesc( Bootstrap , predict =  both , iters = 10)\nb632.mmce = setAggregation(mmce, b632)\nb632plus.mmce = setAggregation(mmce, b632plus)\nb632.mmce\n#  Name: Mean misclassification error\n#  Performance measure: mmce\n#  Properties: classif,classif.multi,req.pred,req.truth\n#  Minimize: TRUE\n#  Best: 0; Worst: 1\n#  Aggregated by: b632\n#  Note:\n\nr = resample( classif.rpart , iris.task, rdesc,\n    measures = list(mmce, b632.mmce, b632plus.mmce), show.info = FALSE)\nhead(r$measures.train)\n#    iter        mmce        mmce        mmce\n#  1    1 0.026666667 0.026666667 0.026666667\n#  2    2 0.026666667 0.026666667 0.026666667\n#  3    3 0.006666667 0.006666667 0.006666667\n#  4    4 0.026666667 0.026666667 0.026666667\n#  5    5 0.033333333 0.033333333 0.033333333\n#  6    6 0.013333333 0.013333333 0.013333333\nr$aggr\n#  mmce.test.mean      mmce.b632  mmce.b632plus \n#      0.07051905     0.05389071     0.05496489", 
            "title": "Aggregating performance values"
        }, 
        {
            "location": "/resample/index.html#convenience-functions", 
            "text": "When quickly trying out some learners, it can get tedious to write the  R \ncode for generating a resample instance, setting the aggregation strategy and so\non. For this reason  mlr  provides some convenience functions for the\nfrequently used resampling strategies, for example  holdout , crossval  or  bootstrapB632 . But note that you do not\nhave as much control and flexibility as when using  resample  with a resample\ndescription or instance.  holdout( regr.lm , bh.task, measures = list(mse, mae))\ncrossval( classif.lda , iris.task, iters = 3, measures = list(mmce, ber))", 
            "title": "Convenience functions"
        }, 
        {
            "location": "/benchmark_experiments/index.html", 
            "text": "Benchmark Experiments\n\n\nIn a benchmark experiment different learning methods are applied to one or several data sets\nwith the aim to compare and rank the algorithms with respect to one or more\nperformance measures.\n\n\nIn \nmlr\n a benchmark experiment can be conducted by calling function \nbenchmark\n on\na \nlist\n of \nLearner\ns and a \nlist\n of \nTask\ns.\n\nbenchmark\n basically executes \nresample\n for each combination of \nLearner\n\nand \nTask\n.\nYou can specify an individual resampling strategy for each \nTask\n and select one or\nmultiple performance measures to be calculated.\n\n\nExample: One task, two learners, prediction on a single test set\n\n\nWe start with a small example. Two learners, \nlinear discriminant analysis\n and\n\nclassification trees\n, are applied to one classification problem (\nsonar.task\n).\nAs resampling strategy we choose \n\"Holdout\"\n.\nThe performance is thus calculated on one single randomly sampled test data set.\n\n\nIn the example below we create a resample description (\nResampleDesc\n),\nwhich is automatically instantiated by \nbenchmark\n.\nThe instantiation is done only once, that is, the same resample instance\n(\nResampleInstance\n) is used for all learners applied to the \nTask\n.\nIt's also possible to directly pass a \nResampleInstance\n.\n\n\nIf you would like to use a \nfixed test data set\n instead of a randomly selected one, you can\ncreate a suitable \nResampleInstance\n through function\n\nmakeFixedHoldoutInstance\n.\n\n\n## Two learners to be compared\nlrns = list(makeLearner(\nclassif.lda\n), makeLearner(\nclassif.rpart\n))\n\n## Choose the resampling strategy\nrdesc = makeResampleDesc(\nHoldout\n)\n\n## Conduct the benchmark experiment\nres = benchmark(lrns, sonar.task, rdesc)\n#\n Task: Sonar-example, Learner: classif.lda\n#\n [Resample] holdout iter: 1\n#\n [Resample] Result: mmce.test.mean= 0.3\n#\n Task: Sonar-example, Learner: classif.rpart\n#\n [Resample] holdout iter: 1\n#\n [Resample] Result: mmce.test.mean=0.286\n\nres\n#\n         task.id    learner.id mmce.test.mean\n#\n 1 Sonar-example   classif.lda      0.3000000\n#\n 2 Sonar-example classif.rpart      0.2857143\n\n\n\n\nIn the printed table every row corresponds to one pair of \nTask\n and \nLearner\n.\nThe entries show the mean misclassification error (\nmmce\n), the default performance\nmeasure for classification, on the test data set.\n\n\nThe result \nres\n is an object of class \nBenchmarkResult\n. Basically, this is a \nlist\n\nof lists of \nResampleResult\n objects, first ordered by \nTask\n and then by \nLearner\n.\n\n\nmlr\n provides several accessor functions, named \ngetBMR\nwhat_to_extract\n, that permit\nto retrieve information for further analyses. This includes for example the performances\nor predictions of the learning algorithms under consideration.\n\n\nLet's have a look at the benchmark result above.\n\ngetBMRPerformances\n returns individual performances in resampling runs, while\n\ngetBMRAggrPerformances\n gives the aggregated values.\n\n\ngetBMRPerformances(res)\n#\n $`Sonar-example`\n#\n $`Sonar-example`$classif.lda\n#\n   iter mmce\n#\n 1    1  0.3\n#\n \n#\n $`Sonar-example`$classif.rpart\n#\n   iter      mmce\n#\n 1    1 0.2857143\n\ngetBMRAggrPerformances(res)\n#\n $`Sonar-example`\n#\n $`Sonar-example`$classif.lda\n#\n mmce.test.mean \n#\n            0.3 \n#\n \n#\n $`Sonar-example`$classif.rpart\n#\n mmce.test.mean \n#\n      0.2857143\n\n\n\n\nSince we used holdout as resampling strategy, individual and aggregated performances are the\nsame.\n\n\nOften it is more convenient to work with \ndata.frame\ns. You can easily\nconvert the result structure by setting \nas.df = TRUE\n.\n\n\ngetBMRPerformances(res, as.df = TRUE)\n#\n         task.id    learner.id iter      mmce\n#\n 1 Sonar-example   classif.lda    1 0.3000000\n#\n 2 Sonar-example classif.rpart    1 0.2857143\n\ngetBMRAggrPerformances(res, as.df = TRUE)\n#\n         task.id    learner.id mmce.test.mean\n#\n 1 Sonar-example   classif.lda      0.3000000\n#\n 2 Sonar-example classif.rpart      0.2857143\n\n\n\n\nFunction \ngetBMRPredictions\n returns the predictions.\nPer default, you get a \nlist\n of lists of \nResamplePrediction\n objects.\nIn most cases you might prefer the \ndata.frame\n version.\n\n\ngetBMRPredictions(res)\n#\n $`Sonar-example`\n#\n $`Sonar-example`$classif.lda\n#\n Resampled Prediction for:\n#\n Resample description: holdout with 0.67 split rate.\n#\n Predict: test\n#\n Stratification: FALSE\n#\n predict.type: response\n#\n threshold: \n#\n time (mean): 0.01\n#\n      id truth response iter  set\n#\n 180 180     M        M    1 test\n#\n 100 100     M        R    1 test\n#\n 53   53     R        M    1 test\n#\n 89   89     R        R    1 test\n#\n 92   92     R        M    1 test\n#\n 11   11     R        R    1 test\n#\n \n#\n $`Sonar-example`$classif.rpart\n#\n Resampled Prediction for:\n#\n Resample description: holdout with 0.67 split rate.\n#\n Predict: test\n#\n Stratification: FALSE\n#\n predict.type: response\n#\n threshold: \n#\n time (mean): 0.01\n#\n      id truth response iter  set\n#\n 180 180     M        M    1 test\n#\n 100 100     M        M    1 test\n#\n 53   53     R        R    1 test\n#\n 89   89     R        M    1 test\n#\n 92   92     R        M    1 test\n#\n 11   11     R        R    1 test\n\nhead(getBMRPredictions(res, as.df = TRUE))\n#\n         task.id  learner.id  id truth response iter  set\n#\n 1 Sonar-example classif.lda 180     M        M    1 test\n#\n 2 Sonar-example classif.lda 100     M        R    1 test\n#\n 3 Sonar-example classif.lda  53     R        M    1 test\n#\n 4 Sonar-example classif.lda  89     R        R    1 test\n#\n 5 Sonar-example classif.lda  92     R        M    1 test\n#\n 6 Sonar-example classif.lda  11     R        R    1 test\n\n\n\n\nIt is also easily possible to access results for certain learners or tasks via their\nIDs. Nearly all \"getter\" functions have a \nlearner.ids\n and a \ntask.ids\n argument.\n\n\nhead(getBMRPredictions(res, learner.ids = \nclassif.rpart\n, as.df = TRUE))\n#\n           task.id    learner.id  id truth response iter  set\n#\n 180 Sonar-example classif.rpart 180     M        M    1 test\n#\n 100 Sonar-example classif.rpart 100     M        M    1 test\n#\n 53  Sonar-example classif.rpart  53     R        R    1 test\n#\n 89  Sonar-example classif.rpart  89     R        M    1 test\n#\n 92  Sonar-example classif.rpart  92     R        M    1 test\n#\n 11  Sonar-example classif.rpart  11     R        R    1 test\n\n\n\n\nAs you might recall, you can use the \nid\n option in \nmakeLearner\n or function \nsetId\n\nto set the ID of a \nLearner\n and the \nid\n option of \nmake*Task\n for\n\nTask\n IDs.\n\n\nThe IDs of all \nLearner\ns and \nTask\ns in a benchmark experiment can be retrieved\nas follows:\n\n\ngetBMRTaskIds(res)\n#\n [1] \nSonar-example\n\n\ngetBMRLearnerIds(res)\n#\n [1] \nclassif.lda\n   \nclassif.rpart\n\n\n\n\n\nExample: Two tasks, three learners, bootstrapping\n\n\nLet's have a look at a larger benchmark experiment with two classification tasks (\npid.task\n\nand \nsonar.task\n) and three learning algorithms.\nSince the default learner IDs are a little long, we choose shorter names.\n\n\nFor both tasks bootstrapping with 20 iterations is chosen as resampling strategy.\nThis is achieved by passing a single resample description to \nbenchmark\n, which is then\ninstantiated automatically once for each \nTask\n.\nThus, the same instance is used for all learners applied to one task.\n\n\nIt's also possible to choose a different resampling strategy for each \nTask\n by passing a\n\nlist\n of the same length as the number of tasks that can contain both\n\nResampleDesc\ns and \nResampleInstance\ns.\n\n\nIn the example below the accuracy (\nacc\n) and the area under curve (\nauc\n)\nare calculated.\n\n\n## Three learners to be compared\nlrns = list(makeLearner(\nclassif.lda\n, predict.type = \nprob\n, id = \nlda\n),\n  makeLearner(\nclassif.rpart\n, predict.type = \nprob\n, id = \nrpart\n),\n  makeLearner(\nclassif.randomForest\n, predict.type = \nprob\n, id = \nrF\n))\n\n## Two classification tasks\ntasks = list(pid.task, sonar.task)\n\n## Use bootstrapping for both tasks\nrdesc = makeResampleDesc(\nBootstrap\n, iters = 20)\n\n## Conduct the benchmark experiment\nres = benchmark(lrns, tasks, rdesc, measures = list(acc, auc), show.info = FALSE)\n\nres\n#\n                       task.id learner.id acc.test.mean auc.test.mean\n#\n 1 PimaIndiansDiabetes-example        lda     0.7677489     0.8237077\n#\n 2 PimaIndiansDiabetes-example      rpart     0.7386807     0.7367115\n#\n 3 PimaIndiansDiabetes-example         rF     0.7600432     0.8166053\n#\n 4               Sonar-example        lda     0.7137996     0.7755366\n#\n 5               Sonar-example      rpart     0.6985412     0.7331386\n#\n 6               Sonar-example         rF     0.8156073     0.9184084\n\n\n\n\nThe entries in the printed table show the aggregated accuracies and AUC values.\nOn the Pima data lda and random forest show nearly identical performance. \nOn the sonar example random forest has highest accuracy and AUC.\n\n\nInstead of just comparing mean performance values it's generally preferable to have a look\nat the distribution of performance values obtained in individual resampling runs.\nThe individual performances on the 20 bootstrap iterations for every task and learner are\nretrieved below.\n\n\nperf = getBMRPerformances(res, as.df = TRUE)\nhead(perf)\n#\n                       task.id learner.id iter       acc       auc\n#\n 1 PimaIndiansDiabetes-example        lda    1 0.7065217 0.7644810\n#\n 2 PimaIndiansDiabetes-example        lda    2 0.7789116 0.8158948\n#\n 3 PimaIndiansDiabetes-example        lda    3 0.7977941 0.8517787\n#\n 4 PimaIndiansDiabetes-example        lda    4 0.7781818 0.8162754\n#\n 5 PimaIndiansDiabetes-example        lda    5 0.7403509 0.7966082\n#\n 6 PimaIndiansDiabetes-example        lda    6 0.7681661 0.8356029\n\n\n\n\nAs part of a first exploratory analysis you might want to create some plots, for example\ndotplots, boxplots, densityplots or histograms.\nCurrently, \nmlr\n does not provide any plotting functionality for benchmark experiments.\nBut based on the \ndata.frame\n returned by \ngetBMRPerformances\n some basic\nplots are easily done.\n\n\nShown below are boxplots for the accuracy, \nacc\n, and densityplots for the AUC,\n\nauc\n, generated by function \nqplot\n from package\n\nggplot2\n.\n\n\nqplot(y = acc, x = task.id, colour = learner.id, data = perf, geom = \nboxplot\n)\n\n\n\n\n \n\n\nqplot(auc, colour = learner.id, facets = . ~ task.id, data = perf, geom = \ndensity\n)\n\n\n\n\n \n\n\nIn order to plot both performance measures in parallel \nperf\n is reshaped to long format.\nBelow we generate grouped boxplots and densityplots for all tasks, learners and measures.\n\n\nperfm = reshape2::melt(perf, id.vars = c(\ntask.id\n, \nlearner.id\n, \niter\n), measure.vars = c(\nacc\n, \nauc\n))\nhead(perfm)\n#\n                       task.id learner.id iter variable     value\n#\n 1 PimaIndiansDiabetes-example        lda    1      acc 0.7065217\n#\n 2 PimaIndiansDiabetes-example        lda    2      acc 0.7789116\n#\n 3 PimaIndiansDiabetes-example        lda    3      acc 0.7977941\n#\n 4 PimaIndiansDiabetes-example        lda    4      acc 0.7781818\n#\n 5 PimaIndiansDiabetes-example        lda    5      acc 0.7403509\n#\n 6 PimaIndiansDiabetes-example        lda    6      acc 0.7681661\n\nqplot(variable, value, data = perfm, colour = learner.id, facets = . ~ task.id, geom = \nboxplot\n,\n  xlab = \nmeasure\n, ylab = \nperformance\n)\n\n\n\n\n \n\n\nqplot(value, data = perfm, colour = learner.id, facets = variable ~ task.id, geom = \ndensity\n,\n  xlab = \nperformance\n)\n\n\n\n\n \n\n\nIt might also be useful to assess if learner performances in single resampling iterations,\ni.e., on the same bootstrap sample, are related.\nThis might help to gain further insight, for example by having a closer look at bootstrap\nsamples where one learner performs exceptionally well while another one is fairly bad.\nMoreover, this might be useful for the construction of ensembles of learning algorithms.\nBelow, function \nggpairs\n from package \nGGally\n is used to generate a scatterplot\nmatrix of learner accuracies (\nacc\n) on the sonar data set.\n\n\nperf = getBMRPerformances(res, task.id = \nSonar-example\n, as.df = TRUE)\nperfr = reshape(perf, direction = \nwide\n, v.names = c(\nacc\n, \nauc\n), timevar = \nlearner.id\n,\n  idvar = c(\ntask.id\n, \niter\n))\nhead(perfr)\n#\n         task.id iter   acc.lda   auc.lda acc.rpart auc.rpart    acc.rF\n#\n 1 Sonar-example    1 0.7468354 0.7928571 0.6202532 0.5928571 0.7341772\n#\n 2 Sonar-example    2 0.7466667 0.7642450 0.7066667 0.6613248 0.7200000\n#\n 3 Sonar-example    3 0.7968750 0.8666667 0.7031250 0.7406863 0.9062500\n#\n 4 Sonar-example    4 0.6666667 0.7309900 0.6266667 0.7109039 0.8000000\n#\n 5 Sonar-example    5 0.6883117 0.7335203 0.6753247 0.7685835 0.8051948\n#\n 6 Sonar-example    6 0.7792208 0.8367072 0.7272727 0.7864372 0.7922078\n#\n      auc.rF\n#\n 1 0.9272727\n#\n 2 0.8828348\n#\n 3 0.9696078\n#\n 4 0.8884505\n#\n 5 0.8923562\n#\n 6 0.9460189\n\nGGally::ggpairs(perfr, c(3,5,7))\n\n\n\n\n \n\n\nFurther comments\n\n\n\n\nIn the examples shown in this section we applied \"raw\" learning algorithms, but often things\nare more complicated.\nAt the very least, many learners have hyperparameters that need to be tuned to get sensible results.\nReliable performance estimates can be obtained by nested resampling, i.e., by doing the tuning in an\ninner resampling loop while estimating the performance in an outer loop.\nMoreover, you might want to combine learners with pre-processing steps like imputation, scaling,\noutlier removal, dimensionality reduction or feature selection and so on.\nAll this can be easily done by using \nmlr\n's wrapper functionality.\nThe general principle is explained in the section about \nwrapped learners\n in the\nAdvanced part of this tutorial. There are also several sections devoted to common pre-processing\nsteps.\n\n\nBenchmark experiments can very quickly become computationally demanding. \nmlr\n offers\nsome possibilities for \nparallelization\n.", 
            "title": "Benchmark Experiments"
        }, 
        {
            "location": "/benchmark_experiments/index.html#benchmark-experiments", 
            "text": "In a benchmark experiment different learning methods are applied to one or several data sets\nwith the aim to compare and rank the algorithms with respect to one or more\nperformance measures.  In  mlr  a benchmark experiment can be conducted by calling function  benchmark  on\na  list  of  Learner s and a  list  of  Task s. benchmark  basically executes  resample  for each combination of  Learner \nand  Task .\nYou can specify an individual resampling strategy for each  Task  and select one or\nmultiple performance measures to be calculated.", 
            "title": "Benchmark Experiments"
        }, 
        {
            "location": "/benchmark_experiments/index.html#example-one-task-two-learners-prediction-on-a-single-test-set", 
            "text": "We start with a small example. Two learners,  linear discriminant analysis  and classification trees , are applied to one classification problem ( sonar.task ).\nAs resampling strategy we choose  \"Holdout\" .\nThe performance is thus calculated on one single randomly sampled test data set.  In the example below we create a resample description ( ResampleDesc ),\nwhich is automatically instantiated by  benchmark .\nThe instantiation is done only once, that is, the same resample instance\n( ResampleInstance ) is used for all learners applied to the  Task .\nIt's also possible to directly pass a  ResampleInstance .  If you would like to use a  fixed test data set  instead of a randomly selected one, you can\ncreate a suitable  ResampleInstance  through function makeFixedHoldoutInstance .  ## Two learners to be compared\nlrns = list(makeLearner( classif.lda ), makeLearner( classif.rpart ))\n\n## Choose the resampling strategy\nrdesc = makeResampleDesc( Holdout )\n\n## Conduct the benchmark experiment\nres = benchmark(lrns, sonar.task, rdesc)\n#  Task: Sonar-example, Learner: classif.lda\n#  [Resample] holdout iter: 1\n#  [Resample] Result: mmce.test.mean= 0.3\n#  Task: Sonar-example, Learner: classif.rpart\n#  [Resample] holdout iter: 1\n#  [Resample] Result: mmce.test.mean=0.286\n\nres\n#          task.id    learner.id mmce.test.mean\n#  1 Sonar-example   classif.lda      0.3000000\n#  2 Sonar-example classif.rpart      0.2857143  In the printed table every row corresponds to one pair of  Task  and  Learner .\nThe entries show the mean misclassification error ( mmce ), the default performance\nmeasure for classification, on the test data set.  The result  res  is an object of class  BenchmarkResult . Basically, this is a  list \nof lists of  ResampleResult  objects, first ordered by  Task  and then by  Learner .  mlr  provides several accessor functions, named  getBMR what_to_extract , that permit\nto retrieve information for further analyses. This includes for example the performances\nor predictions of the learning algorithms under consideration.  Let's have a look at the benchmark result above. getBMRPerformances  returns individual performances in resampling runs, while getBMRAggrPerformances  gives the aggregated values.  getBMRPerformances(res)\n#  $`Sonar-example`\n#  $`Sonar-example`$classif.lda\n#    iter mmce\n#  1    1  0.3\n#  \n#  $`Sonar-example`$classif.rpart\n#    iter      mmce\n#  1    1 0.2857143\n\ngetBMRAggrPerformances(res)\n#  $`Sonar-example`\n#  $`Sonar-example`$classif.lda\n#  mmce.test.mean \n#             0.3 \n#  \n#  $`Sonar-example`$classif.rpart\n#  mmce.test.mean \n#       0.2857143  Since we used holdout as resampling strategy, individual and aggregated performances are the\nsame.  Often it is more convenient to work with  data.frame s. You can easily\nconvert the result structure by setting  as.df = TRUE .  getBMRPerformances(res, as.df = TRUE)\n#          task.id    learner.id iter      mmce\n#  1 Sonar-example   classif.lda    1 0.3000000\n#  2 Sonar-example classif.rpart    1 0.2857143\n\ngetBMRAggrPerformances(res, as.df = TRUE)\n#          task.id    learner.id mmce.test.mean\n#  1 Sonar-example   classif.lda      0.3000000\n#  2 Sonar-example classif.rpart      0.2857143  Function  getBMRPredictions  returns the predictions.\nPer default, you get a  list  of lists of  ResamplePrediction  objects.\nIn most cases you might prefer the  data.frame  version.  getBMRPredictions(res)\n#  $`Sonar-example`\n#  $`Sonar-example`$classif.lda\n#  Resampled Prediction for:\n#  Resample description: holdout with 0.67 split rate.\n#  Predict: test\n#  Stratification: FALSE\n#  predict.type: response\n#  threshold: \n#  time (mean): 0.01\n#       id truth response iter  set\n#  180 180     M        M    1 test\n#  100 100     M        R    1 test\n#  53   53     R        M    1 test\n#  89   89     R        R    1 test\n#  92   92     R        M    1 test\n#  11   11     R        R    1 test\n#  \n#  $`Sonar-example`$classif.rpart\n#  Resampled Prediction for:\n#  Resample description: holdout with 0.67 split rate.\n#  Predict: test\n#  Stratification: FALSE\n#  predict.type: response\n#  threshold: \n#  time (mean): 0.01\n#       id truth response iter  set\n#  180 180     M        M    1 test\n#  100 100     M        M    1 test\n#  53   53     R        R    1 test\n#  89   89     R        M    1 test\n#  92   92     R        M    1 test\n#  11   11     R        R    1 test\n\nhead(getBMRPredictions(res, as.df = TRUE))\n#          task.id  learner.id  id truth response iter  set\n#  1 Sonar-example classif.lda 180     M        M    1 test\n#  2 Sonar-example classif.lda 100     M        R    1 test\n#  3 Sonar-example classif.lda  53     R        M    1 test\n#  4 Sonar-example classif.lda  89     R        R    1 test\n#  5 Sonar-example classif.lda  92     R        M    1 test\n#  6 Sonar-example classif.lda  11     R        R    1 test  It is also easily possible to access results for certain learners or tasks via their\nIDs. Nearly all \"getter\" functions have a  learner.ids  and a  task.ids  argument.  head(getBMRPredictions(res, learner.ids =  classif.rpart , as.df = TRUE))\n#            task.id    learner.id  id truth response iter  set\n#  180 Sonar-example classif.rpart 180     M        M    1 test\n#  100 Sonar-example classif.rpart 100     M        M    1 test\n#  53  Sonar-example classif.rpart  53     R        R    1 test\n#  89  Sonar-example classif.rpart  89     R        M    1 test\n#  92  Sonar-example classif.rpart  92     R        M    1 test\n#  11  Sonar-example classif.rpart  11     R        R    1 test  As you might recall, you can use the  id  option in  makeLearner  or function  setId \nto set the ID of a  Learner  and the  id  option of  make*Task  for Task  IDs.  The IDs of all  Learner s and  Task s in a benchmark experiment can be retrieved\nas follows:  getBMRTaskIds(res)\n#  [1]  Sonar-example \n\ngetBMRLearnerIds(res)\n#  [1]  classif.lda     classif.rpart", 
            "title": "Example: One task, two learners, prediction on a single test set"
        }, 
        {
            "location": "/benchmark_experiments/index.html#example-two-tasks-three-learners-bootstrapping", 
            "text": "Let's have a look at a larger benchmark experiment with two classification tasks ( pid.task \nand  sonar.task ) and three learning algorithms.\nSince the default learner IDs are a little long, we choose shorter names.  For both tasks bootstrapping with 20 iterations is chosen as resampling strategy.\nThis is achieved by passing a single resample description to  benchmark , which is then\ninstantiated automatically once for each  Task .\nThus, the same instance is used for all learners applied to one task.  It's also possible to choose a different resampling strategy for each  Task  by passing a list  of the same length as the number of tasks that can contain both ResampleDesc s and  ResampleInstance s.  In the example below the accuracy ( acc ) and the area under curve ( auc )\nare calculated.  ## Three learners to be compared\nlrns = list(makeLearner( classif.lda , predict.type =  prob , id =  lda ),\n  makeLearner( classif.rpart , predict.type =  prob , id =  rpart ),\n  makeLearner( classif.randomForest , predict.type =  prob , id =  rF ))\n\n## Two classification tasks\ntasks = list(pid.task, sonar.task)\n\n## Use bootstrapping for both tasks\nrdesc = makeResampleDesc( Bootstrap , iters = 20)\n\n## Conduct the benchmark experiment\nres = benchmark(lrns, tasks, rdesc, measures = list(acc, auc), show.info = FALSE)\n\nres\n#                        task.id learner.id acc.test.mean auc.test.mean\n#  1 PimaIndiansDiabetes-example        lda     0.7677489     0.8237077\n#  2 PimaIndiansDiabetes-example      rpart     0.7386807     0.7367115\n#  3 PimaIndiansDiabetes-example         rF     0.7600432     0.8166053\n#  4               Sonar-example        lda     0.7137996     0.7755366\n#  5               Sonar-example      rpart     0.6985412     0.7331386\n#  6               Sonar-example         rF     0.8156073     0.9184084  The entries in the printed table show the aggregated accuracies and AUC values.\nOn the Pima data lda and random forest show nearly identical performance. \nOn the sonar example random forest has highest accuracy and AUC.  Instead of just comparing mean performance values it's generally preferable to have a look\nat the distribution of performance values obtained in individual resampling runs.\nThe individual performances on the 20 bootstrap iterations for every task and learner are\nretrieved below.  perf = getBMRPerformances(res, as.df = TRUE)\nhead(perf)\n#                        task.id learner.id iter       acc       auc\n#  1 PimaIndiansDiabetes-example        lda    1 0.7065217 0.7644810\n#  2 PimaIndiansDiabetes-example        lda    2 0.7789116 0.8158948\n#  3 PimaIndiansDiabetes-example        lda    3 0.7977941 0.8517787\n#  4 PimaIndiansDiabetes-example        lda    4 0.7781818 0.8162754\n#  5 PimaIndiansDiabetes-example        lda    5 0.7403509 0.7966082\n#  6 PimaIndiansDiabetes-example        lda    6 0.7681661 0.8356029  As part of a first exploratory analysis you might want to create some plots, for example\ndotplots, boxplots, densityplots or histograms.\nCurrently,  mlr  does not provide any plotting functionality for benchmark experiments.\nBut based on the  data.frame  returned by  getBMRPerformances  some basic\nplots are easily done.  Shown below are boxplots for the accuracy,  acc , and densityplots for the AUC, auc , generated by function  qplot  from package ggplot2 .  qplot(y = acc, x = task.id, colour = learner.id, data = perf, geom =  boxplot )     qplot(auc, colour = learner.id, facets = . ~ task.id, data = perf, geom =  density )     In order to plot both performance measures in parallel  perf  is reshaped to long format.\nBelow we generate grouped boxplots and densityplots for all tasks, learners and measures.  perfm = reshape2::melt(perf, id.vars = c( task.id ,  learner.id ,  iter ), measure.vars = c( acc ,  auc ))\nhead(perfm)\n#                        task.id learner.id iter variable     value\n#  1 PimaIndiansDiabetes-example        lda    1      acc 0.7065217\n#  2 PimaIndiansDiabetes-example        lda    2      acc 0.7789116\n#  3 PimaIndiansDiabetes-example        lda    3      acc 0.7977941\n#  4 PimaIndiansDiabetes-example        lda    4      acc 0.7781818\n#  5 PimaIndiansDiabetes-example        lda    5      acc 0.7403509\n#  6 PimaIndiansDiabetes-example        lda    6      acc 0.7681661\n\nqplot(variable, value, data = perfm, colour = learner.id, facets = . ~ task.id, geom =  boxplot ,\n  xlab =  measure , ylab =  performance )     qplot(value, data = perfm, colour = learner.id, facets = variable ~ task.id, geom =  density ,\n  xlab =  performance )     It might also be useful to assess if learner performances in single resampling iterations,\ni.e., on the same bootstrap sample, are related.\nThis might help to gain further insight, for example by having a closer look at bootstrap\nsamples where one learner performs exceptionally well while another one is fairly bad.\nMoreover, this might be useful for the construction of ensembles of learning algorithms.\nBelow, function  ggpairs  from package  GGally  is used to generate a scatterplot\nmatrix of learner accuracies ( acc ) on the sonar data set.  perf = getBMRPerformances(res, task.id =  Sonar-example , as.df = TRUE)\nperfr = reshape(perf, direction =  wide , v.names = c( acc ,  auc ), timevar =  learner.id ,\n  idvar = c( task.id ,  iter ))\nhead(perfr)\n#          task.id iter   acc.lda   auc.lda acc.rpart auc.rpart    acc.rF\n#  1 Sonar-example    1 0.7468354 0.7928571 0.6202532 0.5928571 0.7341772\n#  2 Sonar-example    2 0.7466667 0.7642450 0.7066667 0.6613248 0.7200000\n#  3 Sonar-example    3 0.7968750 0.8666667 0.7031250 0.7406863 0.9062500\n#  4 Sonar-example    4 0.6666667 0.7309900 0.6266667 0.7109039 0.8000000\n#  5 Sonar-example    5 0.6883117 0.7335203 0.6753247 0.7685835 0.8051948\n#  6 Sonar-example    6 0.7792208 0.8367072 0.7272727 0.7864372 0.7922078\n#       auc.rF\n#  1 0.9272727\n#  2 0.8828348\n#  3 0.9696078\n#  4 0.8884505\n#  5 0.8923562\n#  6 0.9460189\n\nGGally::ggpairs(perfr, c(3,5,7))", 
            "title": "Example: Two tasks, three learners, bootstrapping"
        }, 
        {
            "location": "/benchmark_experiments/index.html#further-comments", 
            "text": "In the examples shown in this section we applied \"raw\" learning algorithms, but often things\nare more complicated.\nAt the very least, many learners have hyperparameters that need to be tuned to get sensible results.\nReliable performance estimates can be obtained by nested resampling, i.e., by doing the tuning in an\ninner resampling loop while estimating the performance in an outer loop.\nMoreover, you might want to combine learners with pre-processing steps like imputation, scaling,\noutlier removal, dimensionality reduction or feature selection and so on.\nAll this can be easily done by using  mlr 's wrapper functionality.\nThe general principle is explained in the section about  wrapped learners  in the\nAdvanced part of this tutorial. There are also several sections devoted to common pre-processing\nsteps.  Benchmark experiments can very quickly become computationally demanding.  mlr  offers\nsome possibilities for  parallelization .", 
            "title": "Further comments"
        }, 
        {
            "location": "/parallelization/index.html", 
            "text": "Parallelization\n\n\nR\n by default does not make use of parallelization.\nWith the integration of \nparallelMap\n into \nmlr\n, it becomes easy to activate the parallel computing capabilities already supported by \nmlr\n.\n\nparallelMap\n supports all major parallelization backends: local multicore execution using \nparallel\n, socket and MPI clusters using \nsnow\n, makeshift SSH-cluster using \nBatchJobs\n and high performance computing clusters (managed by a scheduler like SLURM, Torque/PBS, SGE or LSF) also using \nBatchJobs\n.\n\n\nAll you have to do is select a backend by calling one of the\n\nparallelStart*\n functions.\nThe first loop \nmlr\n encounters which is marked as parallel executable will be automatically parallelized.\nIt is good practice to call \nparallelStop\n at the end of your script.\n\n\nlibrary(\nparallelMap\n)\nparallelStartSocket(2)\n#\n Starting parallelization in mode=socket with cpus=2.\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\nr = resample(\nclassif.lda\n, iris.task, rdesc)\n#\n Exporting objects to slaves for mode socket: .mlr.slave.options\n#\n Mapping in parallel: mode = socket; cpus = 2; elements = 3.\n#\n [Resample] Result: mmce.test.mean=0.02\nparallelStop()\n#\n Stopped parallelization. All cleaned up.\n\n\n\n\nOn Linux or Mac OS X, you may want to use\n\nparallelStartMulticore\n instead.\n\n\nParallelization levels\n\n\nWe offer different parallelization levels for fine grained control over the parallelization.\nE.g., if you do not want to parallelize the \nbenchmark\n function because it has only very few iterations but want to parallelize the \nresampling\n of each learner instead,\nyou can specifically pass the \nlevel\n \n\"mlr.resample\"\n to the \nparallelStart*\n function.\nCurrently the following levels are supported:\n\n\nparallelGetRegisteredLevels()\n#\n mlr: mlr.benchmark, mlr.resample, mlr.selectFeatures, mlr.tuneParams\n\n\n\n\nHere is a brief explanation of what these levels do:\n\n\n\n\nmlr.resample\n: Each resampling iteration (a train / test step) is a parallel job.\n\n\nmlr.benchmark\n: Each experiment \"run this learner on this data set\" is a parallel job.\n\n\nmlr.tuneParams\n: Each evaluation in hyperparameter space \"resample with these parameter settings\" is a parallel job.\n  How many of these can be run independently in parallel, depends on the tuning algorithm.\n  For grid search or random search this is\n  no problem, but for other tuners it depends on how many points are produced in each iteration\n  of the optimization. If a tuner works in a purely sequential fashion, we cannot work magic and the\n  hyperparameter evaluation will also run sequentially. But note that you can still parallelize the\n  underlying resampling.\n\n\nmlr.selectFeatures\n: Each evaluation in feature space \"resample with this feature subset\" is a parallel job.\n  The same comments as for \"mlr.tuneParams\" apply here.\n\n\n\n\nCustom learners and parallelization\n\n\nIf you have implemented a custom learner yourself, locally, you currently need to export this to the slave.\nSo if you see an error after calling, e.g., a parallelized version of [resample] like this:\n\n\nno applicable method for 'trainLearner' applied to an object of class \nmy_new_learner\n\n\n\n\n\nsimply add the following line somewhere after calling \nparallelStart\n.\n\n\nparallelExport(\ntrainLearner.\nmy_new_learner\n, \npredictLearner.\nmy_new_learner\n)\n\n\n\n\nThe end\n\n\nFor further details, consult the\n\nparallelMap tutorial\n and help.", 
            "title": "Parallelization"
        }, 
        {
            "location": "/parallelization/index.html#parallelization", 
            "text": "R  by default does not make use of parallelization.\nWith the integration of  parallelMap  into  mlr , it becomes easy to activate the parallel computing capabilities already supported by  mlr . parallelMap  supports all major parallelization backends: local multicore execution using  parallel , socket and MPI clusters using  snow , makeshift SSH-cluster using  BatchJobs  and high performance computing clusters (managed by a scheduler like SLURM, Torque/PBS, SGE or LSF) also using  BatchJobs .  All you have to do is select a backend by calling one of the parallelStart*  functions.\nThe first loop  mlr  encounters which is marked as parallel executable will be automatically parallelized.\nIt is good practice to call  parallelStop  at the end of your script.  library( parallelMap )\nparallelStartSocket(2)\n#  Starting parallelization in mode=socket with cpus=2.\nrdesc = makeResampleDesc( CV , iters = 3)\nr = resample( classif.lda , iris.task, rdesc)\n#  Exporting objects to slaves for mode socket: .mlr.slave.options\n#  Mapping in parallel: mode = socket; cpus = 2; elements = 3.\n#  [Resample] Result: mmce.test.mean=0.02\nparallelStop()\n#  Stopped parallelization. All cleaned up.  On Linux or Mac OS X, you may want to use parallelStartMulticore  instead.", 
            "title": "Parallelization"
        }, 
        {
            "location": "/parallelization/index.html#parallelization-levels", 
            "text": "We offer different parallelization levels for fine grained control over the parallelization.\nE.g., if you do not want to parallelize the  benchmark  function because it has only very few iterations but want to parallelize the  resampling  of each learner instead,\nyou can specifically pass the  level   \"mlr.resample\"  to the  parallelStart*  function.\nCurrently the following levels are supported:  parallelGetRegisteredLevels()\n#  mlr: mlr.benchmark, mlr.resample, mlr.selectFeatures, mlr.tuneParams  Here is a brief explanation of what these levels do:   mlr.resample : Each resampling iteration (a train / test step) is a parallel job.  mlr.benchmark : Each experiment \"run this learner on this data set\" is a parallel job.  mlr.tuneParams : Each evaluation in hyperparameter space \"resample with these parameter settings\" is a parallel job.\n  How many of these can be run independently in parallel, depends on the tuning algorithm.\n  For grid search or random search this is\n  no problem, but for other tuners it depends on how many points are produced in each iteration\n  of the optimization. If a tuner works in a purely sequential fashion, we cannot work magic and the\n  hyperparameter evaluation will also run sequentially. But note that you can still parallelize the\n  underlying resampling.  mlr.selectFeatures : Each evaluation in feature space \"resample with this feature subset\" is a parallel job.\n  The same comments as for \"mlr.tuneParams\" apply here.", 
            "title": "Parallelization levels"
        }, 
        {
            "location": "/parallelization/index.html#custom-learners-and-parallelization", 
            "text": "If you have implemented a custom learner yourself, locally, you currently need to export this to the slave.\nSo if you see an error after calling, e.g., a parallelized version of [resample] like this:  no applicable method for 'trainLearner' applied to an object of class  my_new_learner   simply add the following line somewhere after calling  parallelStart .  parallelExport( trainLearner. my_new_learner ,  predictLearner. my_new_learner )", 
            "title": "Custom learners and parallelization"
        }, 
        {
            "location": "/parallelization/index.html#the-end", 
            "text": "For further details, consult the parallelMap tutorial  and help.", 
            "title": "The end"
        }, 
        {
            "location": "/configureMlr/index.html", 
            "text": "Configure mlr\n\n\nIf you really know what you are doing you may think \nmlr\n is limiting you in certain ways.\n\nmlr\n is designed to make usage errors due to typos or invalid parameter values\nas unlikely as possible.\nBut sometimes you want to break those barriers and get full access.\nFor all parameters, simply refer to the documentation of \nconfigureMlr\n.\n\n\nExample: Reduce the output on the console\n\n\nYou are bothered by all the output on the console like in this example?\n\n\n## Perform a 3-fold cross-validation\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\nr = resample(\nclassif.ksvm\n, iris.task, rdesc)\n#\n [Resample] cross-validation iter: 1\n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel\n#\n [Resample] cross-validation iter: 2\n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel\n#\n [Resample] cross-validation iter: 3\n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel\n#\n [Resample] Result: mmce.test.mean=0.06\n\n\n\n\nJust try the following:\n\n\nconfigureMlr(show.learner.output = FALSE)\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\nr = resample(\nclassif.ksvm\n, iris.task, rdesc)\n#\n [Resample] cross-validation iter: 1\n#\n [Resample] cross-validation iter: 2\n#\n [Resample] cross-validation iter: 3\n#\n [Resample] Result: mmce.test.mean=0.0533\n\n\n\n\nAccess the current configuration\n\n\nFunction \ngetMlrOptions\n returns a \nlist\n that shows the current configuration:\n\n\ngetMlrOptions()\n#\n $on.learner.error\n#\n [1] \nstop\n\n#\n \n#\n $on.learner.warning\n#\n [1] \nwarn\n\n#\n \n#\n $on.par.out.of.bounds\n#\n [1] \nstop\n\n#\n \n#\n $on.par.without.desc\n#\n [1] \nstop\n\n#\n \n#\n $show.info\n#\n [1] TRUE\n#\n \n#\n $show.learner.output\n#\n [1] FALSE\n\n\n\n\nExample: Turn off parameter checking\n\n\nOr maybe you want to access a new parameter of a \nLearner\n where the learner\nis already available in \nmlr\n, but the parameter is not \"registered\" in the learner's parameter set yet.\nIf this is the case you might want to \ncontact us\n\nor \nopen an issue\n as well!\nBut until then you can turn off \nmlr\n's parameter checking like this:\n\n\nlrn = makeLearner(\nclassif.ksvm\n, newPar = 3)\n#\n Error in setHyperPars2.Learner(learner, insert(par.vals, args)): classif.ksvm: Setting parameter newPar without available description object!\n#\n You can switch off this check by using configureMlr!\nlrn = makeLearner(\nclassif.ksvm\n, epsilon = -3)\n#\n Error in setHyperPars2.Learner(learner, insert(par.vals, args)): classif.ksvm: Setting parameter epsilon without available description object!\n#\n You can switch off this check by using configureMlr!\n\nconfigureMlr(on.par.without.desc = \nquiet\n)\nlrn = makeLearner(\nclassif.ksvm\n, newPar = 3)\nlrn = makeLearner(\nclassif.ksvm\n, epsilon = -3)\n\n\n\n\nThe parameter setting will then be passed to the underlying function without further ado.\n\n\nExample: Handle errors in an underlying learning method\n\n\nAnother common situation is that a particular learning method throws an error.\nThe default behavior of \nmlr\n is to generate an exception as well.\nHowever, in some situations, for example if you conduct a \nbenchmark study\n\nwith multiple data sets and learners, you normally do not want to terminate the whole\nbenchmark experiment due to one error.\nThe following example shows how to prevent this:\n\n\n## This call gives an error caused by the low number of observations in class `virginica`\ntrain(\nclassif.qda\n, task = iris.task, subset = 1:104)\n#\n Error in qda.default(x, grouping, ...): some group is too small for 'qda'\n#\n Timing stopped at: 0.005 0 0.005\n\nconfigureMlr(on.learner.error = \nwarn\n)\nmod = train(\nclassif.qda\n, task = iris.task, subset = 1:104)\n#\n Warning in train(\nclassif.qda\n, task = iris.task, subset = 1:104): Could not train learner classif.qda: Error in qda.default(x, grouping, ...) : \n#\n   some group is too small for 'qda'\nmod\n#\n Model for learner.id=classif.qda; learner.class=classif.qda\n#\n Trained on: task.id = iris-example; obs = 104; features = 4\n#\n Hyperparameters: \n#\n Training failed: Error in qda.default(x, grouping, ...) : \n#\n   some group is too small for 'qda'\n#\n \n#\n Training failed: Error in qda.default(x, grouping, ...) : \n#\n   some group is too small for 'qda'\n\n## mod is an object of class FailureModel\nisFailureModel(mod)\n#\n [1] TRUE\n\n## Get the error message\ngetFailureModelMsg(mod)\n#\n [1] \nError in qda.default(x, grouping, ...) : \\n  some group is too small for 'qda'\\n\n\n\n## NAs are predicted\npredict(mod, iris.task)\n#\n Prediction: 150 observations\n#\n predict.type: response\n#\n threshold: \n#\n time: NA\n#\n   id  truth response\n#\n 1  1 setosa     \nNA\n\n#\n 2  2 setosa     \nNA\n\n#\n 3  3 setosa     \nNA\n\n#\n 4  4 setosa     \nNA\n\n#\n 5  5 setosa     \nNA\n\n#\n 6  6 setosa     \nNA\n\n\n\n\n\nInstead of an exception, a warning is issued and a \nFailureModel\n is created that predicts\n\nNA\ns for all new observations. Function \ngetFailureModelMsg\n extracts the error\nmessage.", 
            "title": "Configuration"
        }, 
        {
            "location": "/configureMlr/index.html#configure-mlr", 
            "text": "If you really know what you are doing you may think  mlr  is limiting you in certain ways. mlr  is designed to make usage errors due to typos or invalid parameter values\nas unlikely as possible.\nBut sometimes you want to break those barriers and get full access.\nFor all parameters, simply refer to the documentation of  configureMlr .", 
            "title": "Configure mlr"
        }, 
        {
            "location": "/configureMlr/index.html#example-reduce-the-output-on-the-console", 
            "text": "You are bothered by all the output on the console like in this example?  ## Perform a 3-fold cross-validation\nrdesc = makeResampleDesc( CV , iters = 3)\nr = resample( classif.ksvm , iris.task, rdesc)\n#  [Resample] cross-validation iter: 1\n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel\n#  [Resample] cross-validation iter: 2\n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel\n#  [Resample] cross-validation iter: 3\n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel\n#  [Resample] Result: mmce.test.mean=0.06  Just try the following:  configureMlr(show.learner.output = FALSE)\nrdesc = makeResampleDesc( CV , iters = 3)\nr = resample( classif.ksvm , iris.task, rdesc)\n#  [Resample] cross-validation iter: 1\n#  [Resample] cross-validation iter: 2\n#  [Resample] cross-validation iter: 3\n#  [Resample] Result: mmce.test.mean=0.0533", 
            "title": "Example: Reduce the output on the console"
        }, 
        {
            "location": "/configureMlr/index.html#access-the-current-configuration", 
            "text": "Function  getMlrOptions  returns a  list  that shows the current configuration:  getMlrOptions()\n#  $on.learner.error\n#  [1]  stop \n#  \n#  $on.learner.warning\n#  [1]  warn \n#  \n#  $on.par.out.of.bounds\n#  [1]  stop \n#  \n#  $on.par.without.desc\n#  [1]  stop \n#  \n#  $show.info\n#  [1] TRUE\n#  \n#  $show.learner.output\n#  [1] FALSE", 
            "title": "Access the current configuration"
        }, 
        {
            "location": "/configureMlr/index.html#example-turn-off-parameter-checking", 
            "text": "Or maybe you want to access a new parameter of a  Learner  where the learner\nis already available in  mlr , but the parameter is not \"registered\" in the learner's parameter set yet.\nIf this is the case you might want to  contact us \nor  open an issue  as well!\nBut until then you can turn off  mlr 's parameter checking like this:  lrn = makeLearner( classif.ksvm , newPar = 3)\n#  Error in setHyperPars2.Learner(learner, insert(par.vals, args)): classif.ksvm: Setting parameter newPar without available description object!\n#  You can switch off this check by using configureMlr!\nlrn = makeLearner( classif.ksvm , epsilon = -3)\n#  Error in setHyperPars2.Learner(learner, insert(par.vals, args)): classif.ksvm: Setting parameter epsilon without available description object!\n#  You can switch off this check by using configureMlr!\n\nconfigureMlr(on.par.without.desc =  quiet )\nlrn = makeLearner( classif.ksvm , newPar = 3)\nlrn = makeLearner( classif.ksvm , epsilon = -3)  The parameter setting will then be passed to the underlying function without further ado.", 
            "title": "Example: Turn off parameter checking"
        }, 
        {
            "location": "/configureMlr/index.html#example-handle-errors-in-an-underlying-learning-method", 
            "text": "Another common situation is that a particular learning method throws an error.\nThe default behavior of  mlr  is to generate an exception as well.\nHowever, in some situations, for example if you conduct a  benchmark study \nwith multiple data sets and learners, you normally do not want to terminate the whole\nbenchmark experiment due to one error.\nThe following example shows how to prevent this:  ## This call gives an error caused by the low number of observations in class `virginica`\ntrain( classif.qda , task = iris.task, subset = 1:104)\n#  Error in qda.default(x, grouping, ...): some group is too small for 'qda'\n#  Timing stopped at: 0.005 0 0.005\n\nconfigureMlr(on.learner.error =  warn )\nmod = train( classif.qda , task = iris.task, subset = 1:104)\n#  Warning in train( classif.qda , task = iris.task, subset = 1:104): Could not train learner classif.qda: Error in qda.default(x, grouping, ...) : \n#    some group is too small for 'qda'\nmod\n#  Model for learner.id=classif.qda; learner.class=classif.qda\n#  Trained on: task.id = iris-example; obs = 104; features = 4\n#  Hyperparameters: \n#  Training failed: Error in qda.default(x, grouping, ...) : \n#    some group is too small for 'qda'\n#  \n#  Training failed: Error in qda.default(x, grouping, ...) : \n#    some group is too small for 'qda'\n\n## mod is an object of class FailureModel\nisFailureModel(mod)\n#  [1] TRUE\n\n## Get the error message\ngetFailureModelMsg(mod)\n#  [1]  Error in qda.default(x, grouping, ...) : \\n  some group is too small for 'qda'\\n \n\n## NAs are predicted\npredict(mod, iris.task)\n#  Prediction: 150 observations\n#  predict.type: response\n#  threshold: \n#  time: NA\n#    id  truth response\n#  1  1 setosa      NA \n#  2  2 setosa      NA \n#  3  3 setosa      NA \n#  4  4 setosa      NA \n#  5  5 setosa      NA \n#  6  6 setosa      NA   Instead of an exception, a warning is issued and a  FailureModel  is created that predicts NA s for all new observations. Function  getFailureModelMsg  extracts the error\nmessage.", 
            "title": "Example: Handle errors in an underlying learning method"
        }, 
        {
            "location": "/wrapper/index.html", 
            "text": "Wrapper\n\n\nWrappers can be employed to extend integrated \nlearners\n with new functionality.\nThe broad scope of operations and methods which are implemented as wrappers underline the flexibility of the wrapping approach:\n\n\n\n\nData preprocessing\n\n\nImputation\n\n\nBagging\n\n\nTuning\n\n\nFeature selection\n\n\nCost-sensitive classification\n\n\nOver- and undersampling\n for imbalanced classification problems\n\n\nMulticlass extension\n for binary-class learners\n\n\n\n\nAll these operations and methods have a few things in common:\nFirst, they all wrap around \nmlr\n \nlearners\n and they return a new learner.\nTherefore learners can be wrapped multiple times.\nSecond, they are implemented using a \ntrain\n (pre-model hook) and \npredict\n (post-model hook) method.\n\n\nExample: Bagging wrapper\n\n\nIn this section we exemplary describe the bagging wrapper to create a random forest which supports weights.\nTo achieve that we combine several decision trees from the \nrpart\n package to create our own custom random forest.\n\n\nFirst, we create a weighted toy task.\n\n\ndata(iris)\ntask = makeClassifTask(data = iris, target = \nSpecies\n, weights = as.integer(iris$Species))\n\n\n\n\nNext, we use \nmakeBaggingWrapper\n to create the base learners and the bagged learner.\nWe choose to set equivalents of \nntree\n (100 base learners) and \nmtry\n (proportion of randomly selected features).\n\n\nbase.lrn = makeLearner(\nclassif.rpart\n)\nwrapped.lrn = makeBaggingWrapper(base.lrn, bw.iters = 100, bw.feats = 0.5)\nprint(wrapped.lrn)\n#\n Learner classif.rpart.bagged from package rpart\n#\n Type: classif\n#\n Name: ; Short name: \n#\n Class: BaggingWrapper\n#\n Properties: numerics,factors,ordered,missings,weights,prob,twoclass,multiclass\n#\n Predict-Type: response\n#\n Hyperparameters: xval=0,bw.iters=100,bw.feats=0.5\n\n\n\n\nAs we can see in the output, the wrapped learner inherited all properties from the base learner, especially the \"weights\" attribute is still present.\nWe can use this newly constructed learner like all base learners, i.e. we can use it in \ntrain\n, \nbenchmark\n, \nresample\n, etc.\n\n\nbenchmark(tasks = task, learners = list(base.lrn, wrapped.lrn))\n#\n Task: iris, Learner: classif.rpart\n#\n [Resample] cross-validation iter: 1\n#\n [Resample] cross-validation iter: 2\n#\n [Resample] cross-validation iter: 3\n#\n [Resample] cross-validation iter: 4\n#\n [Resample] cross-validation iter: 5\n#\n [Resample] cross-validation iter: 6\n#\n [Resample] cross-validation iter: 7\n#\n [Resample] cross-validation iter: 8\n#\n [Resample] cross-validation iter: 9\n#\n [Resample] cross-validation iter: 10\n#\n [Resample] Result: mmce.test.mean=0.0667\n#\n Task: iris, Learner: classif.rpart.bagged\n#\n [Resample] cross-validation iter: 1\n#\n [Resample] cross-validation iter: 2\n#\n [Resample] cross-validation iter: 3\n#\n [Resample] cross-validation iter: 4\n#\n [Resample] cross-validation iter: 5\n#\n [Resample] cross-validation iter: 6\n#\n [Resample] cross-validation iter: 7\n#\n [Resample] cross-validation iter: 8\n#\n [Resample] cross-validation iter: 9\n#\n [Resample] cross-validation iter: 10\n#\n [Resample] Result: mmce.test.mean=0.06\n#\n   task.id           learner.id mmce.test.mean\n#\n 1    iris        classif.rpart     0.06666667\n#\n 2    iris classif.rpart.bagged     0.06000000\n\n\n\n\nThat far we are quite happy with our new learner.\nBut we hope for a better performance by tuning some hyperparameters of both the decision trees and bagging wrapper.\nLet's have a look at the available hyperparameters of the fused learner:\n\n\ngetParamSet(wrapped.lrn)\n#\n                    Type len   Def   Constr Req Tunable Trafo\n#\n bw.iters        integer   -    10 1 to Inf   -    TRUE     -\n#\n bw.replace      logical   -  TRUE        -   -    TRUE     -\n#\n bw.size         numeric   -     -   0 to 1   -    TRUE     -\n#\n bw.feats        numeric   - 0.667   0 to 1   -    TRUE     -\n#\n minsplit        integer   -    20 1 to Inf   -    TRUE     -\n#\n minbucket       integer   -     - 1 to Inf   -    TRUE     -\n#\n cp              numeric   -  0.01   0 to 1   -    TRUE     -\n#\n maxcompete      integer   -     4 0 to Inf   -    TRUE     -\n#\n maxsurrogate    integer   -     5 0 to Inf   -    TRUE     -\n#\n usesurrogate   discrete   -     2    0,1,2   -    TRUE     -\n#\n surrogatestyle discrete   -     0      0,1   -    TRUE     -\n#\n maxdepth        integer   -    30  1 to 30   -    TRUE     -\n#\n xval            integer   -    10 0 to Inf   -    TRUE     -\n#\n parms           untyped   -     -        -   -   FALSE     -\n\n\n\n\nWe choose to tune the parameters \nminsplit\n and \nbw.feats\n for the \nmmce\n using a \nrandom search\n in a 3-fold CV:\n\n\nctrl = makeTuneControlRandom(maxit = 10)\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\npar.set = makeParamSet(\n  makeIntegerParam(\nminsplit\n, lower = 1, upper = 10),\n  makeNumericParam(\nbw.feats\n, lower = 0.25, upper = 1)\n)\ntuned.lrn = makeTuneWrapper(wrapped.lrn, rdesc, mmce, par.set, ctrl)\nprint(tuned.lrn)\n#\n Learner classif.rpart.bagged.tuned from package rpart\n#\n Type: classif\n#\n Name: ; Short name: \n#\n Class: TuneWrapper\n#\n Properties: numerics,factors,ordered,missings,weights,prob,twoclass,multiclass\n#\n Predict-Type: response\n#\n Hyperparameters: xval=0,bw.iters=100,bw.feats=0.5\n\n\n\n\nCalling the train method of the newly constructed learner performs the following steps:\n\n\n\n\nThe tuning wrapper sets parameters for the underlying model in slot \n$next.learner\n and calls its train method.\n\n\nNext learner is the bagging wrapper. The passed down argument \nbw.feats\n is used in the bagging wrapper training function, the argument \nminsplit\n gets passed down to \n$next.learner\n.\n   The base wrapper function calls the base learner \nbw.iters\n times and stores the resulting models.\n\n\nThe bagged models are evaluated using the mean \nmmce\n (default aggregation for this performance measure) and new parameters are selected using the tuning method.\n\n\nThis is repeated until the tuner terminates. Output is a tuned bagged learner.\n\n\n\n\nlrn = train(tuned.lrn, task = task)\n#\n [Tune] Started tuning learner classif.rpart.bagged for parameter set:\n#\n             Type len Def    Constr Req Tunable Trafo\n#\n minsplit integer   -   -   1 to 10   -    TRUE     -\n#\n bw.feats numeric   -   - 0.25 to 1   -    TRUE     -\n#\n With control class: TuneControlRandom\n#\n Imputation value: 1\n#\n [Tune-x] 1: minsplit=5; bw.feats=0.935\n#\n [Tune-y] 1: mmce.test.mean=0.0467; time: 0.1 min; memory: 142Mb use, 251Mb max\n#\n [Tune-x] 2: minsplit=9; bw.feats=0.675\n#\n [Tune-y] 2: mmce.test.mean=0.0467; time: 0.1 min; memory: 142Mb use, 251Mb max\n#\n [Tune-x] 3: minsplit=2; bw.feats=0.847\n#\n [Tune-y] 3: mmce.test.mean=0.0467; time: 0.1 min; memory: 142Mb use, 251Mb max\n#\n [Tune-x] 4: minsplit=4; bw.feats=0.761\n#\n [Tune-y] 4: mmce.test.mean=0.0467; time: 0.1 min; memory: 142Mb use, 251Mb max\n#\n [Tune-x] 5: minsplit=6; bw.feats=0.338\n#\n [Tune-y] 5: mmce.test.mean=0.0867; time: 0.1 min; memory: 142Mb use, 251Mb max\n#\n [Tune-x] 6: minsplit=1; bw.feats=0.637\n#\n [Tune-y] 6: mmce.test.mean=0.0467; time: 0.1 min; memory: 142Mb use, 251Mb max\n#\n [Tune-x] 7: minsplit=1; bw.feats=0.998\n#\n [Tune-y] 7: mmce.test.mean=0.0467; time: 0.1 min; memory: 142Mb use, 251Mb max\n#\n [Tune-x] 8: minsplit=4; bw.feats=0.698\n#\n [Tune-y] 8: mmce.test.mean=0.0467; time: 0.1 min; memory: 142Mb use, 251Mb max\n#\n [Tune-x] 9: minsplit=3; bw.feats=0.836\n#\n [Tune-y] 9: mmce.test.mean=0.0467; time: 0.1 min; memory: 142Mb use, 251Mb max\n#\n [Tune-x] 10: minsplit=10; bw.feats=0.529\n#\n [Tune-y] 10: mmce.test.mean=0.0533; time: 0.1 min; memory: 142Mb use, 251Mb max\n#\n [Tune] Result: minsplit=1; bw.feats=0.998 : mmce.test.mean=0.0467\nprint(lrn)\n#\n Model for learner.id=classif.rpart.bagged.tuned; learner.class=TuneWrapper\n#\n Trained on: task.id = iris; obs = 150; features = 4\n#\n Hyperparameters: xval=0,bw.iters=100,bw.feats=0.5", 
            "title": "Wrapped Learners"
        }, 
        {
            "location": "/wrapper/index.html#wrapper", 
            "text": "Wrappers can be employed to extend integrated  learners  with new functionality.\nThe broad scope of operations and methods which are implemented as wrappers underline the flexibility of the wrapping approach:   Data preprocessing  Imputation  Bagging  Tuning  Feature selection  Cost-sensitive classification  Over- and undersampling  for imbalanced classification problems  Multiclass extension  for binary-class learners   All these operations and methods have a few things in common:\nFirst, they all wrap around  mlr   learners  and they return a new learner.\nTherefore learners can be wrapped multiple times.\nSecond, they are implemented using a  train  (pre-model hook) and  predict  (post-model hook) method.", 
            "title": "Wrapper"
        }, 
        {
            "location": "/wrapper/index.html#example-bagging-wrapper", 
            "text": "In this section we exemplary describe the bagging wrapper to create a random forest which supports weights.\nTo achieve that we combine several decision trees from the  rpart  package to create our own custom random forest.  First, we create a weighted toy task.  data(iris)\ntask = makeClassifTask(data = iris, target =  Species , weights = as.integer(iris$Species))  Next, we use  makeBaggingWrapper  to create the base learners and the bagged learner.\nWe choose to set equivalents of  ntree  (100 base learners) and  mtry  (proportion of randomly selected features).  base.lrn = makeLearner( classif.rpart )\nwrapped.lrn = makeBaggingWrapper(base.lrn, bw.iters = 100, bw.feats = 0.5)\nprint(wrapped.lrn)\n#  Learner classif.rpart.bagged from package rpart\n#  Type: classif\n#  Name: ; Short name: \n#  Class: BaggingWrapper\n#  Properties: numerics,factors,ordered,missings,weights,prob,twoclass,multiclass\n#  Predict-Type: response\n#  Hyperparameters: xval=0,bw.iters=100,bw.feats=0.5  As we can see in the output, the wrapped learner inherited all properties from the base learner, especially the \"weights\" attribute is still present.\nWe can use this newly constructed learner like all base learners, i.e. we can use it in  train ,  benchmark ,  resample , etc.  benchmark(tasks = task, learners = list(base.lrn, wrapped.lrn))\n#  Task: iris, Learner: classif.rpart\n#  [Resample] cross-validation iter: 1\n#  [Resample] cross-validation iter: 2\n#  [Resample] cross-validation iter: 3\n#  [Resample] cross-validation iter: 4\n#  [Resample] cross-validation iter: 5\n#  [Resample] cross-validation iter: 6\n#  [Resample] cross-validation iter: 7\n#  [Resample] cross-validation iter: 8\n#  [Resample] cross-validation iter: 9\n#  [Resample] cross-validation iter: 10\n#  [Resample] Result: mmce.test.mean=0.0667\n#  Task: iris, Learner: classif.rpart.bagged\n#  [Resample] cross-validation iter: 1\n#  [Resample] cross-validation iter: 2\n#  [Resample] cross-validation iter: 3\n#  [Resample] cross-validation iter: 4\n#  [Resample] cross-validation iter: 5\n#  [Resample] cross-validation iter: 6\n#  [Resample] cross-validation iter: 7\n#  [Resample] cross-validation iter: 8\n#  [Resample] cross-validation iter: 9\n#  [Resample] cross-validation iter: 10\n#  [Resample] Result: mmce.test.mean=0.06\n#    task.id           learner.id mmce.test.mean\n#  1    iris        classif.rpart     0.06666667\n#  2    iris classif.rpart.bagged     0.06000000  That far we are quite happy with our new learner.\nBut we hope for a better performance by tuning some hyperparameters of both the decision trees and bagging wrapper.\nLet's have a look at the available hyperparameters of the fused learner:  getParamSet(wrapped.lrn)\n#                     Type len   Def   Constr Req Tunable Trafo\n#  bw.iters        integer   -    10 1 to Inf   -    TRUE     -\n#  bw.replace      logical   -  TRUE        -   -    TRUE     -\n#  bw.size         numeric   -     -   0 to 1   -    TRUE     -\n#  bw.feats        numeric   - 0.667   0 to 1   -    TRUE     -\n#  minsplit        integer   -    20 1 to Inf   -    TRUE     -\n#  minbucket       integer   -     - 1 to Inf   -    TRUE     -\n#  cp              numeric   -  0.01   0 to 1   -    TRUE     -\n#  maxcompete      integer   -     4 0 to Inf   -    TRUE     -\n#  maxsurrogate    integer   -     5 0 to Inf   -    TRUE     -\n#  usesurrogate   discrete   -     2    0,1,2   -    TRUE     -\n#  surrogatestyle discrete   -     0      0,1   -    TRUE     -\n#  maxdepth        integer   -    30  1 to 30   -    TRUE     -\n#  xval            integer   -    10 0 to Inf   -    TRUE     -\n#  parms           untyped   -     -        -   -   FALSE     -  We choose to tune the parameters  minsplit  and  bw.feats  for the  mmce  using a  random search  in a 3-fold CV:  ctrl = makeTuneControlRandom(maxit = 10)\nrdesc = makeResampleDesc( CV , iters = 3)\npar.set = makeParamSet(\n  makeIntegerParam( minsplit , lower = 1, upper = 10),\n  makeNumericParam( bw.feats , lower = 0.25, upper = 1)\n)\ntuned.lrn = makeTuneWrapper(wrapped.lrn, rdesc, mmce, par.set, ctrl)\nprint(tuned.lrn)\n#  Learner classif.rpart.bagged.tuned from package rpart\n#  Type: classif\n#  Name: ; Short name: \n#  Class: TuneWrapper\n#  Properties: numerics,factors,ordered,missings,weights,prob,twoclass,multiclass\n#  Predict-Type: response\n#  Hyperparameters: xval=0,bw.iters=100,bw.feats=0.5  Calling the train method of the newly constructed learner performs the following steps:   The tuning wrapper sets parameters for the underlying model in slot  $next.learner  and calls its train method.  Next learner is the bagging wrapper. The passed down argument  bw.feats  is used in the bagging wrapper training function, the argument  minsplit  gets passed down to  $next.learner .\n   The base wrapper function calls the base learner  bw.iters  times and stores the resulting models.  The bagged models are evaluated using the mean  mmce  (default aggregation for this performance measure) and new parameters are selected using the tuning method.  This is repeated until the tuner terminates. Output is a tuned bagged learner.   lrn = train(tuned.lrn, task = task)\n#  [Tune] Started tuning learner classif.rpart.bagged for parameter set:\n#              Type len Def    Constr Req Tunable Trafo\n#  minsplit integer   -   -   1 to 10   -    TRUE     -\n#  bw.feats numeric   -   - 0.25 to 1   -    TRUE     -\n#  With control class: TuneControlRandom\n#  Imputation value: 1\n#  [Tune-x] 1: minsplit=5; bw.feats=0.935\n#  [Tune-y] 1: mmce.test.mean=0.0467; time: 0.1 min; memory: 142Mb use, 251Mb max\n#  [Tune-x] 2: minsplit=9; bw.feats=0.675\n#  [Tune-y] 2: mmce.test.mean=0.0467; time: 0.1 min; memory: 142Mb use, 251Mb max\n#  [Tune-x] 3: minsplit=2; bw.feats=0.847\n#  [Tune-y] 3: mmce.test.mean=0.0467; time: 0.1 min; memory: 142Mb use, 251Mb max\n#  [Tune-x] 4: minsplit=4; bw.feats=0.761\n#  [Tune-y] 4: mmce.test.mean=0.0467; time: 0.1 min; memory: 142Mb use, 251Mb max\n#  [Tune-x] 5: minsplit=6; bw.feats=0.338\n#  [Tune-y] 5: mmce.test.mean=0.0867; time: 0.1 min; memory: 142Mb use, 251Mb max\n#  [Tune-x] 6: minsplit=1; bw.feats=0.637\n#  [Tune-y] 6: mmce.test.mean=0.0467; time: 0.1 min; memory: 142Mb use, 251Mb max\n#  [Tune-x] 7: minsplit=1; bw.feats=0.998\n#  [Tune-y] 7: mmce.test.mean=0.0467; time: 0.1 min; memory: 142Mb use, 251Mb max\n#  [Tune-x] 8: minsplit=4; bw.feats=0.698\n#  [Tune-y] 8: mmce.test.mean=0.0467; time: 0.1 min; memory: 142Mb use, 251Mb max\n#  [Tune-x] 9: minsplit=3; bw.feats=0.836\n#  [Tune-y] 9: mmce.test.mean=0.0467; time: 0.1 min; memory: 142Mb use, 251Mb max\n#  [Tune-x] 10: minsplit=10; bw.feats=0.529\n#  [Tune-y] 10: mmce.test.mean=0.0533; time: 0.1 min; memory: 142Mb use, 251Mb max\n#  [Tune] Result: minsplit=1; bw.feats=0.998 : mmce.test.mean=0.0467\nprint(lrn)\n#  Model for learner.id=classif.rpart.bagged.tuned; learner.class=TuneWrapper\n#  Trained on: task.id = iris; obs = 150; features = 4\n#  Hyperparameters: xval=0,bw.iters=100,bw.feats=0.5", 
            "title": "Example: Bagging wrapper"
        }, 
        {
            "location": "/preproc/index.html", 
            "text": "Data Preprocessing\n\n\nmlr\n offers several options for data preprocessing.\n\n\nSome of the following simple methods were already mentioned in section\n\nLearning tasks\n:\n\n\n\n\ncapLargeValues\n: Convert large/infinite numeric values in a \ndata.frame\n\n  or \nTask\n.\n\n\ncreateDummyFeatures\n: Generate dummy variables for factor features in a\n  \ndata.frame\n or \nTask\n.\n\n\ndropFeatures\n: Remove some features from a \nTask\n.\n\n\njoinClassLevels\n: Only for classification: Merge existing classes in a \nTask\n to\n  new, larger classes.\n\n\nmergeSmallFactorLevels\n: Merge infrequent levels of factor features in a \nTask\n.\n\n\nnormalizeFeatures\n: Normalize features in a \nTask\n by different methods, e.g.,\n  standardization or scaling to a certain range.\n\n\nremoveConstantFeatures\n: Remove constant features from a \nTask\n.\n\n\nsubsetTask\n: Remove observations and/or features from a \nTask\n.\n\n\n\n\nMoreover, distinct sections in this tutorial are devoted to\n\n\n\n\nFeature selection\n and\n\n\nImputation of missing values\n.\n\n\n\n\nAdditionally, \nmlr\n permits to fuse a \nLearner\n with any preprocessing method\nof your choice like any kind of data transformation, normalization, dimensionality reduction\nor outlier removal.\n\n\nFusing a learner with data preprocessing\n\n\nA \nLearner\n can be coupled with a preprocessing method by function \n\nmakePreprocWrapper\n.\n\n\nAs described in the section about \nwrapped learners\n wrappers are implemented\nusing a \ntrain\n and a \npredict\n method.\nIn case of preprocessing wrappers these methods specify how to transform the data before\ntraining and prediction and are \ncompletely defined by the user\n.\n\n\nThe specified preprocessing steps then \"belong\" to the wrapped \nLearner\n.\nIn contrast to the preprocessing options listed above like \nnormalizeFeatures\n\n\n\n\nthe \nTask\n itself remains unchanged,\n\n\nthe preprocessing is not done globally, i.e., for the whole data set, but for every pair of\n  training/test data sets in e.g. resampling,\n\n\nany parameters controlling the preprocessing as, e.g., the percentage of outliers to be removed\n  can be \ntuned\n together with the base learner parameters.\n\n\n\n\nLet's see how to create a preprocessing wrapper using the following simple example:\nSome learning methods as e.g. k nearest neighbors, support vector machines or neural networks\nusually require scaled features.\nMany, but not all, have a built-in scaling option where the training data set is scaled before\nmodel fitting and the test data set is scaled accordingly, that is by using the scaling\nparameters from the training stage, before making predictions.\nIn the following we show how to add a scaling option to a \nLearner\n by coupling\nit with function \nscale\n.\n\n\nSpecifying the train method\n\n\nThe \ntrain\n function has to be a function with the following arguments:\n\n\n\n\ndata\n is a \ndata.frame\n with values of all features and\n  the target variable.\n\n\ntarget\n is a string and denotes the name of the target variable in \ndata\n.\n\n\nargs\n is a \nlist\n of further arguments and parameters that influence the\n  preprocessing.\n\n\n\n\nIt must return a \nlist\n with elements \n$data\n and \n$control\n,\nwhere \n$data\n is the preprocessed data set and \n$control\n stores all information required\nto preprocess the data before prediction.\n\n\nThe \ntrain\n function for the scaling example is given below. It calls \nscale\n on the\nnumerical features and returns the scaled training data and the corresponding scaling parameters.\n\nargs\n contains the \ncenter\n and \nscale\n arguments of function \nscale\n\nand slot \n$control\n stores the scaling parameters.\n\n\ntr.fun = function(data, target, args = list(center, scale)) {\n  cns = colnames(data)\n  nums = setdiff(cns[sapply(data, is.numeric)], target)\n  x = as.matrix(data[, nums, drop = FALSE])\n  x = scale(x, center = args$center, scale = args$scale)\n  ctrl = args\n  if (is.logical(ctrl$center) \n ctrl$center)\n    ctrl$center = attr(x, \nscaled:center\n)\n  if (is.logical(ctrl$scale) \n ctrl$scale)\n    ctrl$scale = attr(x, \nscaled:scale\n)\n  data = data[, setdiff(cns, nums), drop = FALSE]\n  data = cbind(data, as.data.frame(x))\n  return(list(data = data, control = ctrl))\n}\n\n\n\n\nSpecifying the predict method\n\n\nThe \npredict\n function has the following arguments:\n\n\n\n\ndata\n is a \ndata.frame\n with feature values.\n  (Naturally, it does not contain values of the target variable.)\n\n\ntarget\n is a string indicating the name of the target variable.\n\n\nargs\n are the \nargs\n that were passed to the \ntrain\n function.\n\n\ncontrol\n is the object returned by the \ntrain\n function.\n\n\n\n\nIt returns the preprocessed data.\n\n\nIn our running example the \npredict\n function scales the numerical features using the\nparameters from the training stage stored in \ncontrol\n.\n\n\npr.fun = function(data, target, args, control) {\n  cns = colnames(data)\n  nums = cns[sapply(data, is.numeric)]\n  x = as.matrix(data[, nums, drop = FALSE])\n  x = scale(x, center = control$center, scale = control$scale)\n  data = data[, setdiff(cns, nums), drop = FALSE]  \n  data = cbind(data, as.data.frame(x))\n  return(data)\n}\n\n\n\n\nCreating the preprocessing wrapper\n\n\nBelow we create a preprocessing wrapper with a \nregression neural network\n (which\nitself does not have a scaling option) as base learner.\n\n\nThe \ntrain\n and \npredict\n functions defined above are passed to \nmakePreprocWrapper\n via\nthe \ntrain\n and \npredict\n arguments.\n\npar.vals\n is a \nlist\n of parameter values that is relayed to the \nargs\n\nargument of the \ntrain\n function.\n\n\nlrn = makeLearner(\nregr.nnet\n, trace = FALSE, decay = 1e-02)\nlrn = makePreprocWrapper(lrn, train = tr.fun, predict = pr.fun,\n  par.vals = list(center = TRUE, scale = TRUE))\nlrn\n#\n Learner regr.nnet.preproc from package nnet\n#\n Type: regr\n#\n Name: ; Short name: \n#\n Class: PreprocWrapper\n#\n Properties: numerics,factors,weights\n#\n Predict-Type: response\n#\n Hyperparameters: size=3,trace=FALSE,decay=0.01\n\n\n\n\nLet's compare the cross-validated mean squared error (\nmse\n) on the\n\nBoston Housing data set\n with and without scaling.\n\n\nrdesc = makeResampleDesc(\nCV\n, iters = 10)\n\nr = resample(lrn, bh.task, resampling = rdesc, show.info = FALSE)\nr$aggr\n#\n mse.test.mean \n#\n      20.98447\n\nlrn = makeLearner(\nregr.nnet\n, trace = FALSE, decay = 1e-02)\nr = resample(lrn, bh.task, resampling = rdesc, show.info = FALSE)\nr$aggr\n#\n mse.test.mean \n#\n      54.37792", 
            "title": "Preprocessing"
        }, 
        {
            "location": "/preproc/index.html#data-preprocessing", 
            "text": "mlr  offers several options for data preprocessing.  Some of the following simple methods were already mentioned in section Learning tasks :   capLargeValues : Convert large/infinite numeric values in a  data.frame \n  or  Task .  createDummyFeatures : Generate dummy variables for factor features in a\n   data.frame  or  Task .  dropFeatures : Remove some features from a  Task .  joinClassLevels : Only for classification: Merge existing classes in a  Task  to\n  new, larger classes.  mergeSmallFactorLevels : Merge infrequent levels of factor features in a  Task .  normalizeFeatures : Normalize features in a  Task  by different methods, e.g.,\n  standardization or scaling to a certain range.  removeConstantFeatures : Remove constant features from a  Task .  subsetTask : Remove observations and/or features from a  Task .   Moreover, distinct sections in this tutorial are devoted to   Feature selection  and  Imputation of missing values .   Additionally,  mlr  permits to fuse a  Learner  with any preprocessing method\nof your choice like any kind of data transformation, normalization, dimensionality reduction\nor outlier removal.", 
            "title": "Data Preprocessing"
        }, 
        {
            "location": "/preproc/index.html#fusing-a-learner-with-data-preprocessing", 
            "text": "A  Learner  can be coupled with a preprocessing method by function  makePreprocWrapper .  As described in the section about  wrapped learners  wrappers are implemented\nusing a  train  and a  predict  method.\nIn case of preprocessing wrappers these methods specify how to transform the data before\ntraining and prediction and are  completely defined by the user .  The specified preprocessing steps then \"belong\" to the wrapped  Learner .\nIn contrast to the preprocessing options listed above like  normalizeFeatures   the  Task  itself remains unchanged,  the preprocessing is not done globally, i.e., for the whole data set, but for every pair of\n  training/test data sets in e.g. resampling,  any parameters controlling the preprocessing as, e.g., the percentage of outliers to be removed\n  can be  tuned  together with the base learner parameters.   Let's see how to create a preprocessing wrapper using the following simple example:\nSome learning methods as e.g. k nearest neighbors, support vector machines or neural networks\nusually require scaled features.\nMany, but not all, have a built-in scaling option where the training data set is scaled before\nmodel fitting and the test data set is scaled accordingly, that is by using the scaling\nparameters from the training stage, before making predictions.\nIn the following we show how to add a scaling option to a  Learner  by coupling\nit with function  scale .  Specifying the train method  The  train  function has to be a function with the following arguments:   data  is a  data.frame  with values of all features and\n  the target variable.  target  is a string and denotes the name of the target variable in  data .  args  is a  list  of further arguments and parameters that influence the\n  preprocessing.   It must return a  list  with elements  $data  and  $control ,\nwhere  $data  is the preprocessed data set and  $control  stores all information required\nto preprocess the data before prediction.  The  train  function for the scaling example is given below. It calls  scale  on the\nnumerical features and returns the scaled training data and the corresponding scaling parameters. args  contains the  center  and  scale  arguments of function  scale \nand slot  $control  stores the scaling parameters.  tr.fun = function(data, target, args = list(center, scale)) {\n  cns = colnames(data)\n  nums = setdiff(cns[sapply(data, is.numeric)], target)\n  x = as.matrix(data[, nums, drop = FALSE])\n  x = scale(x, center = args$center, scale = args$scale)\n  ctrl = args\n  if (is.logical(ctrl$center)   ctrl$center)\n    ctrl$center = attr(x,  scaled:center )\n  if (is.logical(ctrl$scale)   ctrl$scale)\n    ctrl$scale = attr(x,  scaled:scale )\n  data = data[, setdiff(cns, nums), drop = FALSE]\n  data = cbind(data, as.data.frame(x))\n  return(list(data = data, control = ctrl))\n}  Specifying the predict method  The  predict  function has the following arguments:   data  is a  data.frame  with feature values.\n  (Naturally, it does not contain values of the target variable.)  target  is a string indicating the name of the target variable.  args  are the  args  that were passed to the  train  function.  control  is the object returned by the  train  function.   It returns the preprocessed data.  In our running example the  predict  function scales the numerical features using the\nparameters from the training stage stored in  control .  pr.fun = function(data, target, args, control) {\n  cns = colnames(data)\n  nums = cns[sapply(data, is.numeric)]\n  x = as.matrix(data[, nums, drop = FALSE])\n  x = scale(x, center = control$center, scale = control$scale)\n  data = data[, setdiff(cns, nums), drop = FALSE]  \n  data = cbind(data, as.data.frame(x))\n  return(data)\n}  Creating the preprocessing wrapper  Below we create a preprocessing wrapper with a  regression neural network  (which\nitself does not have a scaling option) as base learner.  The  train  and  predict  functions defined above are passed to  makePreprocWrapper  via\nthe  train  and  predict  arguments. par.vals  is a  list  of parameter values that is relayed to the  args \nargument of the  train  function.  lrn = makeLearner( regr.nnet , trace = FALSE, decay = 1e-02)\nlrn = makePreprocWrapper(lrn, train = tr.fun, predict = pr.fun,\n  par.vals = list(center = TRUE, scale = TRUE))\nlrn\n#  Learner regr.nnet.preproc from package nnet\n#  Type: regr\n#  Name: ; Short name: \n#  Class: PreprocWrapper\n#  Properties: numerics,factors,weights\n#  Predict-Type: response\n#  Hyperparameters: size=3,trace=FALSE,decay=0.01  Let's compare the cross-validated mean squared error ( mse ) on the Boston Housing data set  with and without scaling.  rdesc = makeResampleDesc( CV , iters = 10)\n\nr = resample(lrn, bh.task, resampling = rdesc, show.info = FALSE)\nr$aggr\n#  mse.test.mean \n#       20.98447\n\nlrn = makeLearner( regr.nnet , trace = FALSE, decay = 1e-02)\nr = resample(lrn, bh.task, resampling = rdesc, show.info = FALSE)\nr$aggr\n#  mse.test.mean \n#       54.37792", 
            "title": "Fusing a learner with data preprocessing"
        }, 
        {
            "location": "/impute/index.html", 
            "text": "Imputation of Missing Values\n\n\nmlr\n provides several imputation methods which are listed on the help page \nimputations\n.\nThese include standard techniques as imputation by a constant value (like a fixed constant, the mean, median or mode)\nand random numbers (either from the empirical distribution of the feature under consideration or a certain\ndistribution family).\nMoreover, missing values in one feature can be replaced based on the other features by predictions from\nany supervised \nLearner\n integrated into \nmlr\n.\n\n\nFurthermore, \nmlr\n permits to \ncreate your own imputation method\n.\n\n\nNote that some of the learning algorithms included in \nmlr\n can deal with missing values in a sensible way, i.e.,\nother than deleting observations with \nNA's\n.\nThose \nLearner\ns have the property \n\"missings\"\n and are indicated in the\n\ntable of integrated learners\n.\n\n\nImputation and reimputation\n\n\nImputation can be done by function \nimpute\n.\nYou can specify an imputation method for each feature individually or for classes of features like numerics or factors.\nMoreover, you can generate dummy variables that indicate which values are missing, also either for classes of features\nor for individual features.\nThese allow to identify the patterns and reasons for missing data and permit to treat imputed and observed values\ndifferently in a subsequent analysis.\n\n\nLet's have a look at the \nairquality\n data set.\n\n\ndata(airquality)\nsummary(airquality)\n#\n      Ozone           Solar.R           Wind             Temp      \n#\n  Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n#\n  1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n#\n  Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n#\n  Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n#\n  3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n#\n  Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n#\n  NA's   :37       NA's   :7                                       \n#\n      Month            Day      \n#\n  Min.   :5.000   Min.   : 1.0  \n#\n  1st Qu.:6.000   1st Qu.: 8.0  \n#\n  Median :7.000   Median :16.0  \n#\n  Mean   :6.993   Mean   :15.8  \n#\n  3rd Qu.:8.000   3rd Qu.:23.0  \n#\n  Max.   :9.000   Max.   :31.0  \n#\n \n\n\n\n\nThere are 37 \nNA's\n in variable \nOzone\n (ozone pollution) and 7 \nNA's\n in variable \nSolar.R\n (solar radiation).\nFor demonstration purposes we insert artificial \nNA's\n in column \nWind\n (wind speed) and coerce it into a\n\nfactor\n.\n\n\nairq = airquality\nind = sample(nrow(airq), 10)\nairq$Wind[ind] = NA\nairq$Wind = cut(airq$Wind, c(0,8,16,24))\nsummary(airq)\n#\n      Ozone           Solar.R           Wind         Temp      \n#\n  Min.   :  1.00   Min.   :  7.0   (0,8]  :51   Min.   :56.00  \n#\n  1st Qu.: 18.00   1st Qu.:115.8   (8,16] :86   1st Qu.:72.00  \n#\n  Median : 31.50   Median :205.0   (16,24]: 6   Median :79.00  \n#\n  Mean   : 42.13   Mean   :185.9   NA's   :10   Mean   :77.88  \n#\n  3rd Qu.: 63.25   3rd Qu.:258.8                3rd Qu.:85.00  \n#\n  Max.   :168.00   Max.   :334.0                Max.   :97.00  \n#\n  NA's   :37       NA's   :7                                   \n#\n      Month            Day      \n#\n  Min.   :5.000   Min.   : 1.0  \n#\n  1st Qu.:6.000   1st Qu.: 8.0  \n#\n  Median :7.000   Median :16.0  \n#\n  Mean   :6.993   Mean   :15.8  \n#\n  3rd Qu.:8.000   3rd Qu.:23.0  \n#\n  Max.   :9.000   Max.   :31.0  \n#\n \n\n\n\n\nIf you want to impute \nNA's\n in all integer features (these include \nOzone\n and \nSolar.R\n) by the mean,\nin all factor features (\nWind\n) by the mode and additionally generate dummy variables for all integer features,\nyou can do this as follows:\n\n\nimp = impute(airq, classes = list(integer = imputeMean(), factor = imputeMode()),\n  dummy.classes = \ninteger\n)\n\n\n\n\nimpute\n returns a \nlist\n where slot \n$data\n contains the imputed data set.\nPer default, the dummy variables are factors with levels \n\"TRUE\"\n and \n\"FALSE\"\n.\nIt is also possible to create numeric zero-one indicator variables.\n\n\nhead(imp$data, 10)\n#\n       Ozone  Solar.R    Wind Temp Month Day Ozone.dummy Solar.R.dummy\n#\n 1  41.00000 190.0000   (0,8]   67     5   1       FALSE         FALSE\n#\n 2  36.00000 118.0000   (0,8]   72     5   2       FALSE         FALSE\n#\n 3  12.00000 149.0000  (8,16]   74     5   3       FALSE         FALSE\n#\n 4  18.00000 313.0000  (8,16]   62     5   4       FALSE         FALSE\n#\n 5  42.12931 185.9315  (8,16]   56     5   5        TRUE          TRUE\n#\n 6  28.00000 185.9315  (8,16]   66     5   6       FALSE          TRUE\n#\n 7  23.00000 299.0000  (8,16]   65     5   7       FALSE         FALSE\n#\n 8  19.00000  99.0000  (8,16]   59     5   8       FALSE         FALSE\n#\n 9   8.00000  19.0000 (16,24]   61     5   9       FALSE         FALSE\n#\n 10 42.12931 194.0000  (8,16]   69     5  10        TRUE         FALSE\n\n\n\n\nSlot \n$desc\n is an \nImputationDesc\n object that stores all relevant information about the\nimputation.\nFor the current example this includes the means and the mode computed on the non-missing data.\n\n\nimp$desc\n#\n Imputation description\n#\n Target: \n#\n Features: 6; Imputed: 6\n#\n impute.new.levels: TRUE\n#\n recode.factor.levels: TRUE\n#\n dummy.type: factor\n\n\n\n\nThe imputation description shows the name of the target variable (not present), the number of features\nand the number of imputed features.\nNote that the latter number refers to the features for which an imputation method was specified\n(five integers plus one factor) and not to the features actually containing \nNA's\n.\n\ndummy.type\n indicates that the dummy variables are factors.\nFor \nimpute.new.levels\n and \nrecode.factor.levels\n see the help page for function \nimpute\n.\n\n\nLet's have a look at another example involving a target variable.\nA possible learning task associated with the \nairquality\n data is to predict the ozone\npollution based on the meteorological features.\nSince we do not want to use columns \nDay\n and \nMonth\n we remove them.\n\n\nairq = subset(airq, select = 1:4)\n\n\n\n\nThe first 100 observations are used as training data set.\n\n\nairq.train = airq[1:100,]\nairq.test = airq[-c(1:100),]\n\n\n\n\nIn case of a supervised learning problem you need to pass the name of the target variable to \nimpute\n.\nThis prevents imputation and creation of a dummy variable for the target variable itself and makes sure\nthat the target variable is not used to impute the features.\n\n\nIn contrast to the example above we specify imputation methods for individual features instead of classes of features.\n\n\nMissing values in \nSolar.R\n are imputed by random numbers drawn from the empirical distribution of\nthe non-missing observations.\n\n\nFunction \nimputeLearner\n allows to use all supervised learning algorithms integrated into \nmlr\n\nfor imputation.\nThe type of the \nLearner\n (\nregr\n, \nclassif\n) must correspond to the class of the feature to\nbe imputed.\nThe missing values in \nWind\n are replaced by the predictions of a classification tree (\nrpart\n).\nPer default, all available columns in \nairq.train\n except the target variable (\nOzone\n) and the variable to\nbe imputed (\nWind\n) are used as features in the classification tree, here \nSolar.R\n and \nTemp\n.\nYou can also select manually which columns to use.\nNote that \nrpart\n can deal with missing feature values, therefore the \nNA's\n in column \nSolar.R\n\ndo not pose a problem.\n\n\nimp = impute(airq.train, target = \nOzone\n, cols = list(Solar.R = imputeHist(),\n  Wind = imputeLearner(makeLearner(\nclassif.rpart\n))), dummy.cols = c(\nSolar.R\n, \nWind\n))\nsummary(imp$data)\n#\n      Ozone           Solar.R            Wind         Temp      \n#\n  Min.   :  1.00   Min.   :  7.00   (0,8]  :34   Min.   :56.00  \n#\n  1st Qu.: 16.00   1st Qu.: 98.75   (8,16] :61   1st Qu.:69.00  \n#\n  Median : 34.00   Median :221.50   (16,24]: 5   Median :79.50  \n#\n  Mean   : 41.59   Mean   :191.54                Mean   :76.87  \n#\n  3rd Qu.: 63.00   3rd Qu.:274.25                3rd Qu.:84.00  \n#\n  Max.   :135.00   Max.   :334.00                Max.   :93.00  \n#\n  NA's   :31                                                    \n#\n  Solar.R.dummy Wind.dummy\n#\n  FALSE:93      FALSE:92  \n#\n  TRUE : 7      TRUE : 8  \n#\n                          \n#\n                          \n#\n                          \n#\n                          \n#\n \nimp$desc\n#\n Imputation description\n#\n Target: Ozone\n#\n Features: 3; Imputed: 2\n#\n impute.new.levels: TRUE\n#\n recode.factor.levels: TRUE\n#\n dummy.type: factor\n\n\n\n\nThe \nImputationDesc\n object can be used by function \nreimpute\n to impute the test data set the same way\nas the training data.\n\n\nairq.test.imp = reimpute(airq.test, imp$desc)\nhead(airq.test.imp)\n#\n   Ozone Solar.R   Wind Temp Solar.R.dummy Wind.dummy\n#\n 1   110     207  (0,8]   90         FALSE      FALSE\n#\n 2    NA     222 (8,16]   92         FALSE      FALSE\n#\n 3    NA     137 (8,16]   86         FALSE      FALSE\n#\n 4    44     192 (8,16]   86         FALSE      FALSE\n#\n 5    28     273 (8,16]   82         FALSE      FALSE\n#\n 6    65     157 (8,16]   80         FALSE      FALSE\n\n\n\n\nEspecially when evaluating a machine learning method by some resampling technique you might want that\n\nimpute\n/\nreimpute\n are called automatically each time before training/prediction.\nThis can be achieved by creating an imputation wrapper.\n\n\nFusing a learner with imputation\n\n\nYou can couple a \nLearner\n with imputation by function \nmakeImputeWrapper\n which basically\nhas the same formal arguments as \nimpute\n.\nLike in the example above we impute \nSolar.R\n by random numbers from its empirical distribution,\n\nWind\n by the predictions of a classification tree and generate dummy variables for both features.\n\n\nlrn = makeImputeWrapper(\nregr.lm\n, cols = list(Solar.R = imputeHist(),\n  Wind = imputeLearner(makeLearner(\nclassif.rpart\n))), dummy.cols = c(\nSolar.R\n, \nWind\n))\nlrn\n#\n Learner regr.lm.preproc from package stats\n#\n Type: regr\n#\n Name: ; Short name: \n#\n Class: PreprocWrapper\n#\n Properties: numerics,factors,weights,se,missings\n#\n Predict-Type: response\n#\n Hyperparameters:\n\n\n\n\nBefore training the resulting \nLearner\n, \nimpute\n is applied to the training set.\nBefore prediction \nreimpute\n is called on the test set and the \nImputationDesc\n object\nfrom the training stage.\n\n\nWe again aim to predict the ozone pollution from the meteorological variables.\nIn order to create the \nTask\n we need to delete observations with missing values in the target variable.\n\n\nairq = subset(airq, subset = !is.na(airq$Ozone))\ntask = makeRegrTask(data = airq, target = \nOzone\n)\n\n\n\n\nIn the following the 3-fold cross-validated \nmean squared error\n is calculated.\n\n\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\nr = resample(lrn, task, resampling = rdesc, show.info = FALSE, models = TRUE)\nr$aggr\n#\n mse.test.mean \n#\n      524.3392\n\n\n\n\nlapply(r$models, getLearnerModel)\n#\n [[1]]\n#\n \n#\n Call:\n#\n stats::lm(formula = f, data = d)\n#\n \n#\n Coefficients:\n#\n       (Intercept)            Solar.R         Wind(8,16]  \n#\n         -117.0954             0.0853           -27.6763  \n#\n       Wind(16,24]               Temp  Solar.R.dummyTRUE  \n#\n           -9.0988             2.0505           -27.4152  \n#\n    Wind.dummyTRUE  \n#\n            2.2535  \n#\n \n#\n \n#\n [[2]]\n#\n \n#\n Call:\n#\n stats::lm(formula = f, data = d)\n#\n \n#\n Coefficients:\n#\n       (Intercept)            Solar.R         Wind(8,16]  \n#\n         -94.84542            0.03936          -16.26255  \n#\n       Wind(16,24]               Temp  Solar.R.dummyTRUE  \n#\n          -7.00707            1.79513          -11.08578  \n#\n    Wind.dummyTRUE  \n#\n          -0.68340  \n#\n \n#\n \n#\n [[3]]\n#\n \n#\n Call:\n#\n stats::lm(formula = f, data = d)\n#\n \n#\n Coefficients:\n#\n       (Intercept)            Solar.R         Wind(8,16]  \n#\n         -57.30438            0.07426          -30.70737  \n#\n       Wind(16,24]               Temp  Solar.R.dummyTRUE  \n#\n         -18.25055            1.35898           -2.16654  \n#\n    Wind.dummyTRUE  \n#\n          -5.56400", 
            "title": "Imputations"
        }, 
        {
            "location": "/impute/index.html#imputation-of-missing-values", 
            "text": "mlr  provides several imputation methods which are listed on the help page  imputations .\nThese include standard techniques as imputation by a constant value (like a fixed constant, the mean, median or mode)\nand random numbers (either from the empirical distribution of the feature under consideration or a certain\ndistribution family).\nMoreover, missing values in one feature can be replaced based on the other features by predictions from\nany supervised  Learner  integrated into  mlr .  Furthermore,  mlr  permits to  create your own imputation method .  Note that some of the learning algorithms included in  mlr  can deal with missing values in a sensible way, i.e.,\nother than deleting observations with  NA's .\nThose  Learner s have the property  \"missings\"  and are indicated in the table of integrated learners .", 
            "title": "Imputation of Missing Values"
        }, 
        {
            "location": "/impute/index.html#imputation-and-reimputation", 
            "text": "Imputation can be done by function  impute .\nYou can specify an imputation method for each feature individually or for classes of features like numerics or factors.\nMoreover, you can generate dummy variables that indicate which values are missing, also either for classes of features\nor for individual features.\nThese allow to identify the patterns and reasons for missing data and permit to treat imputed and observed values\ndifferently in a subsequent analysis.  Let's have a look at the  airquality  data set.  data(airquality)\nsummary(airquality)\n#       Ozone           Solar.R           Wind             Temp      \n#   Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n#   1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n#   Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n#   Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n#   3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n#   Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n#   NA's   :37       NA's   :7                                       \n#       Month            Day      \n#   Min.   :5.000   Min.   : 1.0  \n#   1st Qu.:6.000   1st Qu.: 8.0  \n#   Median :7.000   Median :16.0  \n#   Mean   :6.993   Mean   :15.8  \n#   3rd Qu.:8.000   3rd Qu.:23.0  \n#   Max.   :9.000   Max.   :31.0  \n#    There are 37  NA's  in variable  Ozone  (ozone pollution) and 7  NA's  in variable  Solar.R  (solar radiation).\nFor demonstration purposes we insert artificial  NA's  in column  Wind  (wind speed) and coerce it into a factor .  airq = airquality\nind = sample(nrow(airq), 10)\nairq$Wind[ind] = NA\nairq$Wind = cut(airq$Wind, c(0,8,16,24))\nsummary(airq)\n#       Ozone           Solar.R           Wind         Temp      \n#   Min.   :  1.00   Min.   :  7.0   (0,8]  :51   Min.   :56.00  \n#   1st Qu.: 18.00   1st Qu.:115.8   (8,16] :86   1st Qu.:72.00  \n#   Median : 31.50   Median :205.0   (16,24]: 6   Median :79.00  \n#   Mean   : 42.13   Mean   :185.9   NA's   :10   Mean   :77.88  \n#   3rd Qu.: 63.25   3rd Qu.:258.8                3rd Qu.:85.00  \n#   Max.   :168.00   Max.   :334.0                Max.   :97.00  \n#   NA's   :37       NA's   :7                                   \n#       Month            Day      \n#   Min.   :5.000   Min.   : 1.0  \n#   1st Qu.:6.000   1st Qu.: 8.0  \n#   Median :7.000   Median :16.0  \n#   Mean   :6.993   Mean   :15.8  \n#   3rd Qu.:8.000   3rd Qu.:23.0  \n#   Max.   :9.000   Max.   :31.0  \n#    If you want to impute  NA's  in all integer features (these include  Ozone  and  Solar.R ) by the mean,\nin all factor features ( Wind ) by the mode and additionally generate dummy variables for all integer features,\nyou can do this as follows:  imp = impute(airq, classes = list(integer = imputeMean(), factor = imputeMode()),\n  dummy.classes =  integer )  impute  returns a  list  where slot  $data  contains the imputed data set.\nPer default, the dummy variables are factors with levels  \"TRUE\"  and  \"FALSE\" .\nIt is also possible to create numeric zero-one indicator variables.  head(imp$data, 10)\n#        Ozone  Solar.R    Wind Temp Month Day Ozone.dummy Solar.R.dummy\n#  1  41.00000 190.0000   (0,8]   67     5   1       FALSE         FALSE\n#  2  36.00000 118.0000   (0,8]   72     5   2       FALSE         FALSE\n#  3  12.00000 149.0000  (8,16]   74     5   3       FALSE         FALSE\n#  4  18.00000 313.0000  (8,16]   62     5   4       FALSE         FALSE\n#  5  42.12931 185.9315  (8,16]   56     5   5        TRUE          TRUE\n#  6  28.00000 185.9315  (8,16]   66     5   6       FALSE          TRUE\n#  7  23.00000 299.0000  (8,16]   65     5   7       FALSE         FALSE\n#  8  19.00000  99.0000  (8,16]   59     5   8       FALSE         FALSE\n#  9   8.00000  19.0000 (16,24]   61     5   9       FALSE         FALSE\n#  10 42.12931 194.0000  (8,16]   69     5  10        TRUE         FALSE  Slot  $desc  is an  ImputationDesc  object that stores all relevant information about the\nimputation.\nFor the current example this includes the means and the mode computed on the non-missing data.  imp$desc\n#  Imputation description\n#  Target: \n#  Features: 6; Imputed: 6\n#  impute.new.levels: TRUE\n#  recode.factor.levels: TRUE\n#  dummy.type: factor  The imputation description shows the name of the target variable (not present), the number of features\nand the number of imputed features.\nNote that the latter number refers to the features for which an imputation method was specified\n(five integers plus one factor) and not to the features actually containing  NA's . dummy.type  indicates that the dummy variables are factors.\nFor  impute.new.levels  and  recode.factor.levels  see the help page for function  impute .  Let's have a look at another example involving a target variable.\nA possible learning task associated with the  airquality  data is to predict the ozone\npollution based on the meteorological features.\nSince we do not want to use columns  Day  and  Month  we remove them.  airq = subset(airq, select = 1:4)  The first 100 observations are used as training data set.  airq.train = airq[1:100,]\nairq.test = airq[-c(1:100),]  In case of a supervised learning problem you need to pass the name of the target variable to  impute .\nThis prevents imputation and creation of a dummy variable for the target variable itself and makes sure\nthat the target variable is not used to impute the features.  In contrast to the example above we specify imputation methods for individual features instead of classes of features.  Missing values in  Solar.R  are imputed by random numbers drawn from the empirical distribution of\nthe non-missing observations.  Function  imputeLearner  allows to use all supervised learning algorithms integrated into  mlr \nfor imputation.\nThe type of the  Learner  ( regr ,  classif ) must correspond to the class of the feature to\nbe imputed.\nThe missing values in  Wind  are replaced by the predictions of a classification tree ( rpart ).\nPer default, all available columns in  airq.train  except the target variable ( Ozone ) and the variable to\nbe imputed ( Wind ) are used as features in the classification tree, here  Solar.R  and  Temp .\nYou can also select manually which columns to use.\nNote that  rpart  can deal with missing feature values, therefore the  NA's  in column  Solar.R \ndo not pose a problem.  imp = impute(airq.train, target =  Ozone , cols = list(Solar.R = imputeHist(),\n  Wind = imputeLearner(makeLearner( classif.rpart ))), dummy.cols = c( Solar.R ,  Wind ))\nsummary(imp$data)\n#       Ozone           Solar.R            Wind         Temp      \n#   Min.   :  1.00   Min.   :  7.00   (0,8]  :34   Min.   :56.00  \n#   1st Qu.: 16.00   1st Qu.: 98.75   (8,16] :61   1st Qu.:69.00  \n#   Median : 34.00   Median :221.50   (16,24]: 5   Median :79.50  \n#   Mean   : 41.59   Mean   :191.54                Mean   :76.87  \n#   3rd Qu.: 63.00   3rd Qu.:274.25                3rd Qu.:84.00  \n#   Max.   :135.00   Max.   :334.00                Max.   :93.00  \n#   NA's   :31                                                    \n#   Solar.R.dummy Wind.dummy\n#   FALSE:93      FALSE:92  \n#   TRUE : 7      TRUE : 8  \n#                           \n#                           \n#                           \n#                           \n#  \nimp$desc\n#  Imputation description\n#  Target: Ozone\n#  Features: 3; Imputed: 2\n#  impute.new.levels: TRUE\n#  recode.factor.levels: TRUE\n#  dummy.type: factor  The  ImputationDesc  object can be used by function  reimpute  to impute the test data set the same way\nas the training data.  airq.test.imp = reimpute(airq.test, imp$desc)\nhead(airq.test.imp)\n#    Ozone Solar.R   Wind Temp Solar.R.dummy Wind.dummy\n#  1   110     207  (0,8]   90         FALSE      FALSE\n#  2    NA     222 (8,16]   92         FALSE      FALSE\n#  3    NA     137 (8,16]   86         FALSE      FALSE\n#  4    44     192 (8,16]   86         FALSE      FALSE\n#  5    28     273 (8,16]   82         FALSE      FALSE\n#  6    65     157 (8,16]   80         FALSE      FALSE  Especially when evaluating a machine learning method by some resampling technique you might want that impute / reimpute  are called automatically each time before training/prediction.\nThis can be achieved by creating an imputation wrapper.", 
            "title": "Imputation and reimputation"
        }, 
        {
            "location": "/impute/index.html#fusing-a-learner-with-imputation", 
            "text": "You can couple a  Learner  with imputation by function  makeImputeWrapper  which basically\nhas the same formal arguments as  impute .\nLike in the example above we impute  Solar.R  by random numbers from its empirical distribution, Wind  by the predictions of a classification tree and generate dummy variables for both features.  lrn = makeImputeWrapper( regr.lm , cols = list(Solar.R = imputeHist(),\n  Wind = imputeLearner(makeLearner( classif.rpart ))), dummy.cols = c( Solar.R ,  Wind ))\nlrn\n#  Learner regr.lm.preproc from package stats\n#  Type: regr\n#  Name: ; Short name: \n#  Class: PreprocWrapper\n#  Properties: numerics,factors,weights,se,missings\n#  Predict-Type: response\n#  Hyperparameters:  Before training the resulting  Learner ,  impute  is applied to the training set.\nBefore prediction  reimpute  is called on the test set and the  ImputationDesc  object\nfrom the training stage.  We again aim to predict the ozone pollution from the meteorological variables.\nIn order to create the  Task  we need to delete observations with missing values in the target variable.  airq = subset(airq, subset = !is.na(airq$Ozone))\ntask = makeRegrTask(data = airq, target =  Ozone )  In the following the 3-fold cross-validated  mean squared error  is calculated.  rdesc = makeResampleDesc( CV , iters = 3)\nr = resample(lrn, task, resampling = rdesc, show.info = FALSE, models = TRUE)\nr$aggr\n#  mse.test.mean \n#       524.3392  lapply(r$models, getLearnerModel)\n#  [[1]]\n#  \n#  Call:\n#  stats::lm(formula = f, data = d)\n#  \n#  Coefficients:\n#        (Intercept)            Solar.R         Wind(8,16]  \n#          -117.0954             0.0853           -27.6763  \n#        Wind(16,24]               Temp  Solar.R.dummyTRUE  \n#            -9.0988             2.0505           -27.4152  \n#     Wind.dummyTRUE  \n#             2.2535  \n#  \n#  \n#  [[2]]\n#  \n#  Call:\n#  stats::lm(formula = f, data = d)\n#  \n#  Coefficients:\n#        (Intercept)            Solar.R         Wind(8,16]  \n#          -94.84542            0.03936          -16.26255  \n#        Wind(16,24]               Temp  Solar.R.dummyTRUE  \n#           -7.00707            1.79513          -11.08578  \n#     Wind.dummyTRUE  \n#           -0.68340  \n#  \n#  \n#  [[3]]\n#  \n#  Call:\n#  stats::lm(formula = f, data = d)\n#  \n#  Coefficients:\n#        (Intercept)            Solar.R         Wind(8,16]  \n#          -57.30438            0.07426          -30.70737  \n#        Wind(16,24]               Temp  Solar.R.dummyTRUE  \n#          -18.25055            1.35898           -2.16654  \n#     Wind.dummyTRUE  \n#           -5.56400", 
            "title": "Fusing a learner with imputation"
        }, 
        {
            "location": "/bagging/index.html", 
            "text": "Generic Bagging\n\n\nOne reason why random forests perform so well is that they are using bagging as\na technique to gain more stability. But why do you want to limit yourself to the\nclassifiers already implemented in well known random forests when it is\nreally easy to build your own with \nmlr\n?\n\n\nJust bag an \nmlr\n learner already \nmakeBaggingWrapper\n.\n\n\nAs in a random forest, we need a \nLearner\n which is trained on a subset of the\ndata during each iteration of the bagging process.\nThe subsets are chosen according to the parameters given to \nmakeBaggingWrapper\n:\n\n\n\n\nbw.iters\n On how many subsets (samples) do we want to train our \nLearner\n?\n\n\nbw.replace\n Sample with replacement (also known as \nbootstrapping\n)?\n\n\nbw.size\n Percentage size of the samples. If \nbw.replace = TRUE\n, \nbw.size = 1\n is the default. This does not mean that one sample will contain all the observations as observations will occur multiple times in each sample.\n\n\nbw.feats\n Percentage size of randomly selected features for each iteration.\n\n\n\n\nOf course we also need a \nLearner\n which we have to pass to\n\nmakeBaggingWrapper\n.\n\n\nlrn = makeLearner(\nclassif.PART\n)\nbag.lrn = makeBaggingWrapper(lrn, bw.iters = 50, bw.replace = TRUE, bw.size = 0.8, bw.feats = 3/4)\n\n\n\n\nNow we can compare the performance with and without bagging.\nFirst let's try it without bagging:\n\n\nrdesc = makeResampleDesc(\nCV\n, iters = 10)\nr = resample(learner = lrn, task = sonar.task, resampling = rdesc, show.info = FALSE)\nr$aggr\n#\n mmce.test.mean \n#\n      0.2542857\n\n\n\n\nAnd now with bagging:\n\n\nrdesc = makeResampleDesc(\nCV\n, iters = 10)\nresult = resample(learner = bag.lrn, task = sonar.task, resampling = rdesc, show.info = FALSE)\nresult$aggr\n#\n mmce.test.mean \n#\n      0.1930952\n\n\n\n\nTraining more learners takes more time, but can outperform pure learners\non noisy data with many features.\n\n\nChanging the type of prediction\n\n\nIn case of a \nclassification\n problem the predicted class labels are determined by majority\nvoting over the predictions of the individual models.\nAdditionally, posterior probabilities can be estimated as the relative proportions\nof the predicted class labels.\nFor this purpose you have to change the predict type of the \nbagging learner\n as follows.\n\n\nbag.lrn = setPredictType(bag.lrn, predict.type = \nprob\n)\n\n\n\n\nNote that it is not relevant if the \nbase learner\n itself can predict probabilities and that\nfor this reason the predict type of the \nbase learner\n always has to be \n\"response\"\n.\n\n\nFor \nregression\n the mean value across predictions is computed.\nMoreover, the standard deviation across predictions is estimated if the predict type\nof the bagging learner is changed to \n\"se\"\n.\nBelow, we give a small example for regression.\n\n\nn = getTaskSize(bh.task)\ntrain.inds = seq(1, n, 3)\ntest.inds  = setdiff(1:n, train.inds)\nlrn = makeLearner(\nregr.rpart\n)\nbag.lrn = makeBaggingWrapper(lrn)\nbag.lrn = setPredictType(bag.lrn, predict.type = \nse\n)\nmod = train(learner = bag.lrn, task = bh.task, subset = train.inds)\n\n\n\n\nWith the function \ngetHomogeneousEnsembleModels\n, you can access the models fitted in the\nindividual iterations.\n\n\nhead(getHomogeneousEnsembleModels(mod), 2)\n#\n [[1]]\n#\n Model for learner.id=regr.rpart; learner.class=regr.rpart\n#\n Trained on: task.id = BostonHousing-example; obs = 169; features = 13\n#\n Hyperparameters: xval=0\n#\n \n#\n [[2]]\n#\n Model for learner.id=regr.rpart; learner.class=regr.rpart\n#\n Trained on: task.id = BostonHousing-example; obs = 169; features = 13\n#\n Hyperparameters: xval=0\n\n\n\n\nPredict the response and calculate the standard deviation:\n\n\npred = predict(mod, task = bh.task, subset = test.inds)\nhead(pred$data)\n#\n   id truth response       se\n#\n 2  2  21.6 21.46507 1.040829\n#\n 3  3  34.7 34.44916 4.090512\n#\n 5  5  36.2 33.39258 1.172627\n#\n 6  6  28.7 23.76128 1.106508\n#\n 8  8  27.1 15.76715 3.305897\n#\n 9  9  16.5 15.02399 2.865487\n\n\n\n\nIn the column labelled \nse\n the standard deviation for each prediction is given.\n\n\nLet's visualise this a bit using \nggplot2\n.\nHere we plot the percentage of lower status of the population (\nlstat\n) against the prediction.\n\n\nlibrary(\nggplot2\n)\nlibrary(\nreshape2\n)\ndata = cbind(pred$data, getTaskData(bh.task, subset = test.inds))\ng = ggplot(data, aes(x = lstat, y = response, ymin = response-se, ymax = response+se, col = age))\ng + geom_point() + geom_linerange(alpha=0.5)", 
            "title": "Bagging"
        }, 
        {
            "location": "/bagging/index.html#generic-bagging", 
            "text": "One reason why random forests perform so well is that they are using bagging as\na technique to gain more stability. But why do you want to limit yourself to the\nclassifiers already implemented in well known random forests when it is\nreally easy to build your own with  mlr ?  Just bag an  mlr  learner already  makeBaggingWrapper .  As in a random forest, we need a  Learner  which is trained on a subset of the\ndata during each iteration of the bagging process.\nThe subsets are chosen according to the parameters given to  makeBaggingWrapper :   bw.iters  On how many subsets (samples) do we want to train our  Learner ?  bw.replace  Sample with replacement (also known as  bootstrapping )?  bw.size  Percentage size of the samples. If  bw.replace = TRUE ,  bw.size = 1  is the default. This does not mean that one sample will contain all the observations as observations will occur multiple times in each sample.  bw.feats  Percentage size of randomly selected features for each iteration.   Of course we also need a  Learner  which we have to pass to makeBaggingWrapper .  lrn = makeLearner( classif.PART )\nbag.lrn = makeBaggingWrapper(lrn, bw.iters = 50, bw.replace = TRUE, bw.size = 0.8, bw.feats = 3/4)  Now we can compare the performance with and without bagging.\nFirst let's try it without bagging:  rdesc = makeResampleDesc( CV , iters = 10)\nr = resample(learner = lrn, task = sonar.task, resampling = rdesc, show.info = FALSE)\nr$aggr\n#  mmce.test.mean \n#       0.2542857  And now with bagging:  rdesc = makeResampleDesc( CV , iters = 10)\nresult = resample(learner = bag.lrn, task = sonar.task, resampling = rdesc, show.info = FALSE)\nresult$aggr\n#  mmce.test.mean \n#       0.1930952  Training more learners takes more time, but can outperform pure learners\non noisy data with many features.", 
            "title": "Generic Bagging"
        }, 
        {
            "location": "/bagging/index.html#changing-the-type-of-prediction", 
            "text": "In case of a  classification  problem the predicted class labels are determined by majority\nvoting over the predictions of the individual models.\nAdditionally, posterior probabilities can be estimated as the relative proportions\nof the predicted class labels.\nFor this purpose you have to change the predict type of the  bagging learner  as follows.  bag.lrn = setPredictType(bag.lrn, predict.type =  prob )  Note that it is not relevant if the  base learner  itself can predict probabilities and that\nfor this reason the predict type of the  base learner  always has to be  \"response\" .  For  regression  the mean value across predictions is computed.\nMoreover, the standard deviation across predictions is estimated if the predict type\nof the bagging learner is changed to  \"se\" .\nBelow, we give a small example for regression.  n = getTaskSize(bh.task)\ntrain.inds = seq(1, n, 3)\ntest.inds  = setdiff(1:n, train.inds)\nlrn = makeLearner( regr.rpart )\nbag.lrn = makeBaggingWrapper(lrn)\nbag.lrn = setPredictType(bag.lrn, predict.type =  se )\nmod = train(learner = bag.lrn, task = bh.task, subset = train.inds)  With the function  getHomogeneousEnsembleModels , you can access the models fitted in the\nindividual iterations.  head(getHomogeneousEnsembleModels(mod), 2)\n#  [[1]]\n#  Model for learner.id=regr.rpart; learner.class=regr.rpart\n#  Trained on: task.id = BostonHousing-example; obs = 169; features = 13\n#  Hyperparameters: xval=0\n#  \n#  [[2]]\n#  Model for learner.id=regr.rpart; learner.class=regr.rpart\n#  Trained on: task.id = BostonHousing-example; obs = 169; features = 13\n#  Hyperparameters: xval=0  Predict the response and calculate the standard deviation:  pred = predict(mod, task = bh.task, subset = test.inds)\nhead(pred$data)\n#    id truth response       se\n#  2  2  21.6 21.46507 1.040829\n#  3  3  34.7 34.44916 4.090512\n#  5  5  36.2 33.39258 1.172627\n#  6  6  28.7 23.76128 1.106508\n#  8  8  27.1 15.76715 3.305897\n#  9  9  16.5 15.02399 2.865487  In the column labelled  se  the standard deviation for each prediction is given.  Let's visualise this a bit using  ggplot2 .\nHere we plot the percentage of lower status of the population ( lstat ) against the prediction.  library( ggplot2 )\nlibrary( reshape2 )\ndata = cbind(pred$data, getTaskData(bh.task, subset = test.inds))\ng = ggplot(data, aes(x = lstat, y = response, ymin = response-se, ymax = response+se, col = age))\ng + geom_point() + geom_linerange(alpha=0.5)", 
            "title": "Changing the type of prediction"
        }, 
        {
            "location": "/tune/index.html", 
            "text": "Tuning Hyperparameters\n\n\nMany machine learning algorithms have hyperparameters that need to be set.\nIf selected by the user they can be specified as explained in the section about\n\nLearners\n -- simply pass them to \nmakeLearner\n.\nOften suitable parameter values are not obvious and it is preferable to tune the hyperparameters,\nthat is automatically identify values that lead to the best performance.\n\n\nBasics\n\n\nFor tuning you have to specify\n\n\n\n\nthe search space,\n\n\nthe optimization algorithm,\n\n\nan evaluation method, i.e., a resampling strategy and a performance measure.\n\n\n\n\nThe last point is already covered in this tutorial in the parts about the\n\nevaluation of learning methods\n and \nresampling\n.\n\n\nBelow we show how to specify the search space and optimization algorithm, how to do the\ntuning and how to access the tuning result, using the example of a grid search.\n\n\nThroughout this section we consider classification examples. For the other types of learning\nproblems tuning works analogously.\n\n\nGrid search with manual discretization\n\n\nA grid search is one of the standard -- albeit slow -- ways to choose an\nappropriate set of parameters from a given range of values.\n\n\nWe use the \niris classification task\n for illustration and tune the\nhyperparameters of an SVM (function \nksvm\n from the \nkernlab\n package)\nwith a radial basis kernel.\n\n\nFirst, we create a \nParamSet\n object, which describes the\nparameter space we wish to search.\nThis is done via function \nmakeParamSet\n.\nWe wish to tune the cost parameter \nC\n and the RBF kernel parameter \nsigma\n of the\n\nksvm\n function.\nSince we will use a grid search strategy, we add discrete parameters to the parameter set.\nThe specified \nvalues\n have to be vectors of feasible settings and the complete grid simply is\ntheir cross-product.\nEvery entry in the parameter set has to be named according to the corresponding parameter\nof the underlying \nR\n function.\n\n\nPlease note that whenever parameters in the underlying \nR\n functions should be\npassed in a \nlist\n structure, \nmlr\n tries to give you direct access to\neach parameter and get rid of the list structure.\nThis is the case with the \nkpar\n argument of \nksvm\n which is a list of\nkernel parameters like \nsigma\n.\n\n\nps = makeParamSet(\n  makeDiscreteParam(\nC\n, values = 2^(-2:2)),\n  makeDiscreteParam(\nsigma\n, values = 2^(-2:2))\n)\n\n\n\n\nAdditional to the parameter set, we need an instance of a \nTuneControl\n object.\nThese describe the optimization strategy to be used and its settings.\nHere we choose a grid search:\n\n\nctrl = makeTuneControlGrid()\n\n\n\n\nWe will use 3-fold cross-validation to assess the quality of a specific parameter setting.\nFor this we need to create a resampling description just like in the \nresampling\n\npart of the tutorial.\n\n\nrdesc = makeResampleDesc(\nCV\n, iters = 3L)\n\n\n\n\nFinally, by combining all the previous pieces, we can tune the SVM parameters by calling\n\ntuneParams\n.\n\n\nres = tuneParams(\nclassif.ksvm\n, task = iris.task, resampling = rdesc, par.set = ps,\n  control = ctrl)\n#\n [Tune] Started tuning learner classif.ksvm for parameter set:\n#\n           Type len Def         Constr Req Tunable Trafo\n#\n C     discrete   -   - 0.25,0.5,1,2,4   -    TRUE     -\n#\n sigma discrete   -   - 0.25,0.5,1,2,4   -    TRUE     -\n#\n With control class: TuneControlGrid\n#\n Imputation value: 1\n#\n [Tune-x] 1: C=0.25; sigma=0.25\n#\n [Tune-y] 1: mmce.test.mean=0.0467; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 2: C=0.5; sigma=0.25\n#\n [Tune-y] 2: mmce.test.mean=0.0467; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 3: C=1; sigma=0.25\n#\n [Tune-y] 3: mmce.test.mean=0.04; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 4: C=2; sigma=0.25\n#\n [Tune-y] 4: mmce.test.mean=0.0467; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 5: C=4; sigma=0.25\n#\n [Tune-y] 5: mmce.test.mean=0.0467; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 6: C=0.25; sigma=0.5\n#\n [Tune-y] 6: mmce.test.mean=0.06; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 7: C=0.5; sigma=0.5\n#\n [Tune-y] 7: mmce.test.mean=0.04; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 8: C=1; sigma=0.5\n#\n [Tune-y] 8: mmce.test.mean=0.04; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 9: C=2; sigma=0.5\n#\n [Tune-y] 9: mmce.test.mean=0.0467; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 10: C=4; sigma=0.5\n#\n [Tune-y] 10: mmce.test.mean=0.0467; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 11: C=0.25; sigma=1\n#\n [Tune-y] 11: mmce.test.mean=0.0533; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 12: C=0.5; sigma=1\n#\n [Tune-y] 12: mmce.test.mean=0.04; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 13: C=1; sigma=1\n#\n [Tune-y] 13: mmce.test.mean=0.0467; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 14: C=2; sigma=1\n#\n [Tune-y] 14: mmce.test.mean=0.0467; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 15: C=4; sigma=1\n#\n [Tune-y] 15: mmce.test.mean=0.0533; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 16: C=0.25; sigma=2\n#\n [Tune-y] 16: mmce.test.mean=0.0533; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 17: C=0.5; sigma=2\n#\n [Tune-y] 17: mmce.test.mean=0.04; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 18: C=1; sigma=2\n#\n [Tune-y] 18: mmce.test.mean=0.0333; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 19: C=2; sigma=2\n#\n [Tune-y] 19: mmce.test.mean=0.04; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 20: C=4; sigma=2\n#\n [Tune-y] 20: mmce.test.mean=0.0467; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 21: C=0.25; sigma=4\n#\n [Tune-y] 21: mmce.test.mean=0.113; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 22: C=0.5; sigma=4\n#\n [Tune-y] 22: mmce.test.mean=0.0667; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 23: C=1; sigma=4\n#\n [Tune-y] 23: mmce.test.mean=0.0533; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 24: C=2; sigma=4\n#\n [Tune-y] 24: mmce.test.mean=0.06; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune-x] 25: C=4; sigma=4\n#\n [Tune-y] 25: mmce.test.mean=0.0667; time: 0.0 min; memory: 139Mb use, 215Mb max\n#\n [Tune] Result: C=1; sigma=2 : mmce.test.mean=0.0333\nres\n#\n Tune result:\n#\n Op. pars: C=1; sigma=2\n#\n mmce.test.mean=0.0333\n\n\n\n\ntuneParams\n simply performs the cross-validation for every element of the\ncross-product and selects the parameter setting with the best mean performance.\nAs no performance measure was specified, by default the error rate (\nmmce\n) is\nused.\n\n\nNote that each \nmeasure\n \"knows\" if it is minimized or maximized during tuning.\n\n\n## error rate\nmmce$minimize\n#\n [1] TRUE\n\n## accuracy\nacc$minimize\n#\n [1] FALSE\n\n\n\n\nOf course, you can pass other measures and also a list of measures to \ntuneParams\n.\nIn the latter case the first measure is optimized during tuning, the others are simply evaluated.\nIf you are interested in optimizing several measures simultaneously have a look at the\nparagraph about multi-criteria tuning below.\n\n\nIn the example below we calculate the accuracy (\nacc\n) instead of the error\nrate.\nWe use function \nsetAggregation\n, as described in the section on \nresampling\n,\nto additionally obtain the standard deviation of the accuracy.\n\n\nres = tuneParams(\nclassif.ksvm\n, task = iris.task, resampling = rdesc, par.set = ps,\n  control = ctrl, measures = list(acc, setAggregation(acc, test.sd)), show.info = FALSE)\nres\n#\n Tune result:\n#\n Op. pars: C=0.25; sigma=0.25\n#\n acc.test.mean=0.953,acc.test.sd=0.0306\n\n\n\n\nAccessing the tuning result\n\n\nThe result object \nTuneResult\n allows you to access the best found settings \n$x\n and their\nestimated performance \n$y\n.\n\n\nres$x\n#\n $C\n#\n [1] 0.25\n#\n \n#\n $sigma\n#\n [1] 0.25\nres$y\n#\n acc.test.mean   acc.test.sd \n#\n     0.9533333     0.0305505\n\n\n\n\nMoreover, we can inspect all points evaluated during the search by accessing the\n\n$opt.path\n (see also the documentation of \nOptPath\n).\n\n\nres$opt.path\n#\n Optimization path\n#\n   Dimensions: x = 2/2, y = 2\n#\n   Length: 25\n#\n   Add x values transformed: FALSE\n#\n   Error messages: TRUE. Errors: 0 / 25.\n#\n   Exec times: TRUE. Range: 0.086 - 0.17. 0 NAs.\nopt.grid = as.data.frame(res$opt.path)\nhead(opt.grid)\n#\n      C sigma acc.test.mean acc.test.sd dob eol error.message exec.time\n#\n 1 0.25  0.25     0.9533333  0.03055050   1  NA          \nNA\n     0.095\n#\n 2  0.5  0.25     0.9466667  0.02309401   2  NA          \nNA\n     0.086\n#\n 3    1  0.25     0.9533333  0.01154701   3  NA          \nNA\n     0.102\n#\n 4    2  0.25     0.9533333  0.01154701   4  NA          \nNA\n     0.088\n#\n 5    4  0.25     0.9533333  0.01154701   5  NA          \nNA\n     0.170\n#\n 6 0.25   0.5     0.9333333  0.01154701   6  NA          \nNA\n     0.089\n\n\n\n\nA quick visualization of the performance values on the search grid can be accomplished as follows:\n\n\nlibrary(ggplot2)\ng = ggplot(opt.grid, aes(x = C, y = sigma, fill = acc.test.mean, label = round(acc.test.sd, 3)))\ng + geom_tile() + geom_text(color = \nwhite\n)\n\n\n\n\n \n\n\nThe colors of the tiles display the achieved accuracy, the tile labels show the standard deviation.\n\n\nUsing the optimal parameter values\n\n\nAfter tuning you can generate a \nLearner\n with optimal hyperparameter settings\nas follows:\n\n\nlrn = setHyperPars(makeLearner(\nclassif.ksvm\n), par.vals = res$x)\nlrn\n#\n Learner classif.ksvm from package kernlab\n#\n Type: classif\n#\n Name: Support Vector Machines; Short name: ksvm\n#\n Class: classif.ksvm\n#\n Properties: twoclass,multiclass,numerics,factors,prob\n#\n Predict-Type: response\n#\n Hyperparameters: fit=FALSE,C=0.25,sigma=0.25\n\n\n\n\nThen you can proceed as usual.\nHere we refit and predict the learner on the complete \niris\n data\nset.\n\n\nm = train(lrn, iris.task)\npredict(m, task = iris.task)\n#\n Prediction: 150 observations\n#\n predict.type: response\n#\n threshold: \n#\n time: 0.01\n#\n   id  truth response\n#\n 1  1 setosa   setosa\n#\n 2  2 setosa   setosa\n#\n 3  3 setosa   setosa\n#\n 4  4 setosa   setosa\n#\n 5  5 setosa   setosa\n#\n 6  6 setosa   setosa\n\n\n\n\nGrid search without manual discretization\n\n\nWe can also specify the true numeric parameter types of \nC\n and \nsigma\n when creating the\nparameter set and use the \nresolution\n option of \nmakeTuneControlGrid\n to\nautomatically discretize them.\n\n\nNote how we also make use of the \ntrafo\n option when creating the parameter set to easily\noptimize on a log-scale.\n\n\nTrafos work like this: All optimizers basically see the parameters on their\noriginal scale (from -12 to 12) in this case and produce values on this scale during the search.\nRight before they are passed to the learning algorithm, the transformation function is applied.\n\n\nps = makeParamSet(\n  makeNumericParam(\nC\n, lower = -12, upper = 12, trafo = function(x) 2^x),\n  makeNumericParam(\nsigma\n, lower = -12, upper = 12, trafo = function(x) 2^x)\n)\nctrl = makeTuneControlGrid(resolution = 3L)\nrdesc = makeResampleDesc(\nCV\n, iters = 2L)\nres = tuneParams(\nclassif.ksvm\n, iris.task, rdesc, par.set = ps, control = ctrl)\n#\n [Tune] Started tuning learner classif.ksvm for parameter set:\n#\n          Type len Def    Constr Req Tunable Trafo\n#\n C     numeric   -   - -12 to 12   -    TRUE     Y\n#\n sigma numeric   -   - -12 to 12   -    TRUE     Y\n#\n With control class: TuneControlGrid\n#\n Imputation value: 1\n#\n [Tune-x] 1: C=0.000244; sigma=0.000244\n#\n [Tune-y] 1: mmce.test.mean=0.527; time: 0.0 min; memory: 139Mb use, 251Mb max\n#\n [Tune-x] 2: C=1; sigma=0.000244\n#\n [Tune-y] 2: mmce.test.mean=0.527; time: 0.0 min; memory: 139Mb use, 251Mb max\n#\n [Tune-x] 3: C=4.1e+03; sigma=0.000244\n#\n [Tune-y] 3: mmce.test.mean=0.04; time: 0.0 min; memory: 139Mb use, 251Mb max\n#\n [Tune-x] 4: C=0.000244; sigma=1\n#\n [Tune-y] 4: mmce.test.mean=0.527; time: 0.0 min; memory: 139Mb use, 251Mb max\n#\n [Tune-x] 5: C=1; sigma=1\n#\n [Tune-y] 5: mmce.test.mean=0.04; time: 0.0 min; memory: 139Mb use, 251Mb max\n#\n [Tune-x] 6: C=4.1e+03; sigma=1\n#\n [Tune-y] 6: mmce.test.mean=0.0667; time: 0.0 min; memory: 139Mb use, 251Mb max\n#\n [Tune-x] 7: C=0.000244; sigma=4.1e+03\n#\n [Tune-y] 7: mmce.test.mean=0.567; time: 0.0 min; memory: 139Mb use, 251Mb max\n#\n [Tune-x] 8: C=1; sigma=4.1e+03\n#\n [Tune-y] 8: mmce.test.mean=0.687; time: 0.0 min; memory: 139Mb use, 251Mb max\n#\n [Tune-x] 9: C=4.1e+03; sigma=4.1e+03\n#\n [Tune-y] 9: mmce.test.mean=0.687; time: 0.0 min; memory: 139Mb use, 251Mb max\n#\n [Tune] Result: C=1; sigma=1 : mmce.test.mean=0.04\nres\n#\n Tune result:\n#\n Op. pars: C=1; sigma=1\n#\n mmce.test.mean=0.04\n\n\n\n\nNote that \nres$opt.path\n contains the parameter values \non the original scale\n.\n\n\nas.data.frame(res$opt.path)\n#\n     C sigma mmce.test.mean dob eol error.message exec.time\n#\n 1 -12   -12     0.52666667   1  NA          \nNA\n     0.094\n#\n 2   0   -12     0.52666667   2  NA          \nNA\n     0.068\n#\n 3  12   -12     0.04000000   3  NA          \nNA\n     0.063\n#\n 4 -12     0     0.52666667   4  NA          \nNA\n     0.064\n#\n 5   0     0     0.04000000   5  NA          \nNA\n     0.067\n#\n 6  12     0     0.06666667   6  NA          \nNA\n     0.067\n#\n 7 -12    12     0.56666667   7  NA          \nNA\n     0.066\n#\n 8   0    12     0.68666667   8  NA          \nNA\n     0.067\n#\n 9  12    12     0.68666667   9  NA          \nNA\n     0.064\n\n\n\n\nIn order to get the \ntransformed\n parameter values instead, use function\n\ntrafoOptPath\n.\n\n\nas.data.frame(trafoOptPath(res$opt.path))\n#\n              C        sigma mmce.test.mean dob eol\n#\n 1 2.441406e-04 2.441406e-04     0.52666667   1  NA\n#\n 2 1.000000e+00 2.441406e-04     0.52666667   2  NA\n#\n 3 4.096000e+03 2.441406e-04     0.04000000   3  NA\n#\n 4 2.441406e-04 1.000000e+00     0.52666667   4  NA\n#\n 5 1.000000e+00 1.000000e+00     0.04000000   5  NA\n#\n 6 4.096000e+03 1.000000e+00     0.06666667   6  NA\n#\n 7 2.441406e-04 4.096000e+03     0.56666667   7  NA\n#\n 8 1.000000e+00 4.096000e+03     0.68666667   8  NA\n#\n 9 4.096000e+03 4.096000e+03     0.68666667   9  NA\n\n\n\n\nIterated F-Racing for mixed spaces and dependencies\n\n\nThe package supports a larger number of tuning algorithms, which can all be looked up and\nselected via \nTuneControl\n. One of the cooler algorithms is iterated F-racing from the \n\nirace\n package. This not only works for arbitrary parameter types (numeric, integer,\ndiscrete, logical), but also for so-called dependent / hierarchical parameters:\n\n\nps = makeParamSet(\n  makeNumericParam(\nC\n, lower = -12, upper = 12, trafo = function(x) 2^x),\n  makeDiscreteParam(\nkernel\n, values = c(\nvanilladot\n, \npolydot\n, \nrbfdot\n)),\n  makeNumericParam(\nsigma\n, lower = -12, upper = 12, trafo = function(x) 2^x,\n    requires = quote(kernel == \nrbfdot\n)),\n  makeIntegerParam(\ndegree\n, lower = 2L, upper = 5L,\n    requires = quote(kernel == \npolydot\n))\n)\nctrl = makeTuneControlIrace(maxExperiments = 200L)\nrdesc = makeResampleDesc(\nHoldout\n)\nres = tuneParams(\nclassif.ksvm\n, iris.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)\nprint(head(as.data.frame(res$opt.path)))\n#\n           C     kernel      sigma degree mmce.test.mean dob eol\n#\n 1  4.226737 vanilladot         NA     NA           0.06   1  NA\n#\n 2  1.699061    polydot         NA      4           0.06   2  NA\n#\n 3 -2.001464     rbfdot -0.5400968     NA           0.04   3  NA\n#\n 4 10.508751     rbfdot -6.6748193     NA           0.06   4  NA\n#\n 5  7.772697 vanilladot         NA     NA           0.04   5  NA\n#\n 6 -6.093697 vanilladot         NA     NA           0.10   6  NA\n#\n   error.message exec.time\n#\n 1          \nNA\n     0.041\n#\n 2          \nNA\n     0.037\n#\n 3          \nNA\n     0.034\n#\n 4          \nNA\n     0.034\n#\n 5          \nNA\n     0.037\n#\n 6          \nNA\n     0.034\n\n\n\n\nSee how we made the kernel parameters like \nsigma\n and \ndegree\n dependent on the \nkernel\n\nselection parameters? This approach allows you to tune parameters of multiple kernels at once, \nefficiently concentrating on the ones which work best for your given data set.\n\n\nTuning across whole model spaces with ModelMultiplexer\n\n\nWe can now take the following example even one step further. If we use the\n\nModelMultiplexer\n we can tune over different model classes at once,\njust as we did with the SVM kernels above.\n\n\nbase.learners = list(\n  makeLearner(\nclassif.ksvm\n),\n  makeLearner(\nclassif.randomForest\n)\n)\nlrn = makeModelMultiplexer(base.learners)\n\n\n\n\nFunction \nmakeModelMultiplexerParamSet\n offers a simple way to contruct parameter set for tuning:\nThe parameter names are prefixed automatically and the \nrequires\n element is set, too,\nto make all paramaters subordinate to \nselected.learner\n.\n\n\nps = makeModelMultiplexerParamSet(lrn,\n  makeNumericParam(\nsigma\n, lower = -12, upper = 12, trafo = function(x) 2^x),\n  makeIntegerParam(\nntree\n, lower = 1L, upper = 500L)\n)\nprint(ps)\n#\n                                Type len Def\n#\n selected.learner           discrete   -   -\n#\n classif.ksvm.sigma          numeric   -   -\n#\n classif.randomForest.ntree  integer   -   -\n#\n                                                       Constr Req Tunable\n#\n selected.learner           classif.ksvm,classif.randomForest   -    TRUE\n#\n classif.ksvm.sigma                                 -12 to 12   Y    TRUE\n#\n classif.randomForest.ntree                          1 to 500   Y    TRUE\n#\n                            Trafo\n#\n selected.learner               -\n#\n classif.ksvm.sigma             Y\n#\n classif.randomForest.ntree     -\nrdesc = makeResampleDesc(\nCV\n, iters = 2L)\nctrl = makeTuneControlIrace(maxExperiments = 200L)\nres = tuneParams(lrn, iris.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)\nprint(head(as.data.frame(res$opt.path)))\n#\n       selected.learner classif.ksvm.sigma classif.randomForest.ntree\n#\n 1 classif.randomForest                 NA                         81\n#\n 2         classif.ksvm          -2.758811                         NA\n#\n 3         classif.ksvm          -2.485181                         NA\n#\n 4         classif.ksvm          -6.056252                         NA\n#\n 5         classif.ksvm         -11.450293                         NA\n#\n 6         classif.ksvm           9.867756                         NA\n#\n   mmce.test.mean dob eol error.message exec.time\n#\n 1     0.03333333   1  NA          \nNA\n     0.074\n#\n 2     0.06666667   2  NA          \nNA\n     0.065\n#\n 3     0.06666667   3  NA          \nNA\n     0.068\n#\n 4     0.08666667   4  NA          \nNA\n     0.064\n#\n 5     0.52666667   5  NA          \nNA\n     0.068\n#\n 6     0.67333333   6  NA          \nNA\n     0.068\n\n\n\n\nNested resampling\n\n\nAs we continually optimize over the same data during tuning, the estimated\nperformance value \nres$y\n might be optimistically biased.\nA clean approach to ensure unbiased performance estimation is nested resampling,\nwhere we embed the whole model selection process into an outer resampling loop.\n\n\nActually, we can get this for free without programming any looping by simply using the\n\nwrapper functionality\n of \nmlr\n.\nSee also function \nmakeTuneWrapper\n.\n\n\nLet's use cross-validation with 5 folds in the outer loop and use simple\nholdout test set estimation during tuning in the inner loop.\n\n\nps = makeParamSet(\n  makeDiscreteParam(\nC\n, values = 2^(-2:2)),\n  makeDiscreteParam(\nsigma\n, values = 2^(-2:2))\n)\nctrl = makeTuneControlGrid()\ninner = makeResampleDesc(\nHoldout\n)\nouter = makeResampleDesc(\nCV\n, iters = 5)\nlrn = makeTuneWrapper(\nclassif.ksvm\n, inner, par.set = ps, control = ctrl, show.info = FALSE)\nr = resample(lrn, iris.task, resampling = outer, extract = getTuneResult, show.info = FALSE)\n\n\n\n\nIf we want to find out how good those configurations are on the entire data\nset, we can look at the measures that we already know from \nresampling\n.\nWe receive 5 misclassification errors (one for each optimal parameter\nconfiguration per outer fold) and one aggregated version, i.e., the mean,\nof those 5 values.\n\n\nr$measures.test\n#\n   iter       mmce\n#\n 1    1 0.10000000\n#\n 2    2 0.03333333\n#\n 3    3 0.03333333\n#\n 4    4 0.03333333\n#\n 5    5 0.03333333\nr$aggr\n#\n mmce.test.mean \n#\n     0.04666667\n\n\n\n\nAccessing the tuning result\n\n\nWe have kept the results of the tuning for further evaluations.\nFor example one might want to find out, if the best obtained configurations vary for the\ndifferent outer splits.\nAs storing entire models may be expensive we used the \nextract\n option of \nresample\n.\nFunction \ngetTuneResult\n returns the optimal parameter values and the optimization path\nfor each iteration of the outer resampling loop.\n\n\nr$extract\n#\n [[1]]\n#\n Tune result:\n#\n Op. pars: C=2; sigma=0.5\n#\n mmce.test.mean=   0\n#\n \n#\n [[2]]\n#\n Tune result:\n#\n Op. pars: C=4; sigma=1\n#\n mmce.test.mean=0.025\n#\n \n#\n [[3]]\n#\n Tune result:\n#\n Op. pars: C=1; sigma=0.5\n#\n mmce.test.mean=0.05\n#\n \n#\n [[4]]\n#\n Tune result:\n#\n Op. pars: C=2; sigma=0.25\n#\n mmce.test.mean=0.025\n#\n \n#\n [[5]]\n#\n Tune result:\n#\n Op. pars: C=1; sigma=0.25\n#\n mmce.test.mean=0.025\n\n\n\n\nWe can compare the optimal parameter settings obtained in the 5 resampling iterations.\nAs you can see, the optimal configuration usually depends on the data. You may\nbe able to identify a \nrange\n of parameter settings that achieve good\nperformance though, e.g., the values for \nC\n should be at least 1 and the values\nfor \nsigma\n should be between 0 and 1.\n\n\nIn the following we extract the \nopt.path\n's for each of the 5 cross-validation iterations.\n\n\nopt.paths = lapply(r$extract, function(x) as.data.frame(x$opt.path))\nhead(opt.paths[[1]])\n#\n      C sigma mmce.test.mean dob eol error.message exec.time\n#\n 1 0.25  0.25          0.050   1  NA          \nNA\n     0.036\n#\n 2  0.5  0.25          0.025   2  NA          \nNA\n     0.039\n#\n 3    1  0.25          0.025   3  NA          \nNA\n     0.042\n#\n 4    2  0.25          0.025   4  NA          \nNA\n     0.044\n#\n 5    4  0.25          0.025   5  NA          \nNA\n     0.036\n#\n 6 0.25   0.5          0.050   6  NA          \nNA\n     0.037\n\n\n\n\nWith the following code you can visualize the \nopt.path\n of the first resampling iteration.\n\n\nopt.mmce = lapply(opt.paths, function(x) x$mmce.test.mean)\nopt.grid = opt.paths[[1]][, 1:2]\nopt.grid$mmce.test.mean = apply(simplify2array(opt.mmce), 1, mean)\ng = ggplot(opt.grid, aes(x = C, y = sigma, fill = mmce.test.mean))\ng + geom_tile()\n\n\n\n\n \n\n\nMulti-criteria evaluation and optimization\n\n\nDuring tuning you might want to optimize multiple, potentially conflicting, performance measures\nsimultaneously.\n\n\nIn the following example we aim to minimize both, the false positive and the false negative rates\n(\nfpr\n and \nfnr\n).\nWe again tune the hyperparameters of an SVM (function \nksvm\n) with a radial\nbasis kernel and use the \nsonar classification task\n for illustration.\nAs search strategy we choose a random search.\n\n\nFor all available multi-criteria tuning algorithms see \nTuneMultiCritControl\n.\n\n\nps = makeParamSet(\n  makeNumericParam(\nC\n, lower = -12, upper = 12, trafo = function(x) 2^x),\n  makeNumericParam(\nsigma\n, lower = -12, upper = 12, trafo = function(x) 2^x)\n)\nctrl = makeTuneMultiCritControlRandom(maxit = 30L)\nrdesc = makeResampleDesc(\nHoldout\n)\nres = tuneParamsMultiCrit(\nclassif.ksvm\n, task = sonar.task, resampling = rdesc, par.set = ps,\n  measures = list(fpr, fnr), control = ctrl, show.info = FALSE)\nres\n#\n Tune multicrit result:\n#\n Points on front: 6\nhead(as.data.frame(trafoOptPath(res$opt.path)))\n#\n              C        sigma fpr.test.mean fnr.test.mean dob eol\n#\n 1 3.580062e+00 1.116196e+00     1.0000000    0.00000000   1  NA\n#\n 2 2.380537e+01 1.704055e+00     1.0000000    0.00000000   2  NA\n#\n 3 3.394881e+01 6.680306e-04     0.4102564    0.06451613   3  NA\n#\n 4 2.617769e+02 3.708341e-01     0.9230769    0.00000000   4  NA\n#\n 5 2.459032e+01 5.099661e+01     1.0000000    0.00000000   5  NA\n#\n 6 1.031688e-03 1.843696e-02     1.0000000    0.00000000   6  NA\n\n\n\n\nThe results can be visualized with function \nplotTuneMultiCritResult\n.\nThe plot shows the false positive and false negative rates for all parameter settings evaluated\nduring tuning. Points on the Pareto front are slightly increased.\n\n\nplotTuneMultiCritResult(res)\n\n\n\n\n \n\n\nFurther comments\n\n\n\n\n\n\nTuning works for all other tasks like regression, survival analysis and so on in a completely\n  similar fashion.\n\n\n\n\n\n\nIn longer running tuning experiments it is very annoying if the computation stops due to\n  numerical or other errors. Have a look at \non.learner.error\n in \nconfigureMlr\n as well as\n  the examples given in section \nConfigure mlr\n of this tutorial.\n  You might also want to inform yourself about \nimpute.val\n in \nTuneControl\n.", 
            "title": "Tuning"
        }, 
        {
            "location": "/tune/index.html#tuning-hyperparameters", 
            "text": "Many machine learning algorithms have hyperparameters that need to be set.\nIf selected by the user they can be specified as explained in the section about Learners  -- simply pass them to  makeLearner .\nOften suitable parameter values are not obvious and it is preferable to tune the hyperparameters,\nthat is automatically identify values that lead to the best performance.", 
            "title": "Tuning Hyperparameters"
        }, 
        {
            "location": "/tune/index.html#basics", 
            "text": "For tuning you have to specify   the search space,  the optimization algorithm,  an evaluation method, i.e., a resampling strategy and a performance measure.   The last point is already covered in this tutorial in the parts about the evaluation of learning methods  and  resampling .  Below we show how to specify the search space and optimization algorithm, how to do the\ntuning and how to access the tuning result, using the example of a grid search.  Throughout this section we consider classification examples. For the other types of learning\nproblems tuning works analogously.  Grid search with manual discretization  A grid search is one of the standard -- albeit slow -- ways to choose an\nappropriate set of parameters from a given range of values.  We use the  iris classification task  for illustration and tune the\nhyperparameters of an SVM (function  ksvm  from the  kernlab  package)\nwith a radial basis kernel.  First, we create a  ParamSet  object, which describes the\nparameter space we wish to search.\nThis is done via function  makeParamSet .\nWe wish to tune the cost parameter  C  and the RBF kernel parameter  sigma  of the ksvm  function.\nSince we will use a grid search strategy, we add discrete parameters to the parameter set.\nThe specified  values  have to be vectors of feasible settings and the complete grid simply is\ntheir cross-product.\nEvery entry in the parameter set has to be named according to the corresponding parameter\nof the underlying  R  function.  Please note that whenever parameters in the underlying  R  functions should be\npassed in a  list  structure,  mlr  tries to give you direct access to\neach parameter and get rid of the list structure.\nThis is the case with the  kpar  argument of  ksvm  which is a list of\nkernel parameters like  sigma .  ps = makeParamSet(\n  makeDiscreteParam( C , values = 2^(-2:2)),\n  makeDiscreteParam( sigma , values = 2^(-2:2))\n)  Additional to the parameter set, we need an instance of a  TuneControl  object.\nThese describe the optimization strategy to be used and its settings.\nHere we choose a grid search:  ctrl = makeTuneControlGrid()  We will use 3-fold cross-validation to assess the quality of a specific parameter setting.\nFor this we need to create a resampling description just like in the  resampling \npart of the tutorial.  rdesc = makeResampleDesc( CV , iters = 3L)  Finally, by combining all the previous pieces, we can tune the SVM parameters by calling tuneParams .  res = tuneParams( classif.ksvm , task = iris.task, resampling = rdesc, par.set = ps,\n  control = ctrl)\n#  [Tune] Started tuning learner classif.ksvm for parameter set:\n#            Type len Def         Constr Req Tunable Trafo\n#  C     discrete   -   - 0.25,0.5,1,2,4   -    TRUE     -\n#  sigma discrete   -   - 0.25,0.5,1,2,4   -    TRUE     -\n#  With control class: TuneControlGrid\n#  Imputation value: 1\n#  [Tune-x] 1: C=0.25; sigma=0.25\n#  [Tune-y] 1: mmce.test.mean=0.0467; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 2: C=0.5; sigma=0.25\n#  [Tune-y] 2: mmce.test.mean=0.0467; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 3: C=1; sigma=0.25\n#  [Tune-y] 3: mmce.test.mean=0.04; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 4: C=2; sigma=0.25\n#  [Tune-y] 4: mmce.test.mean=0.0467; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 5: C=4; sigma=0.25\n#  [Tune-y] 5: mmce.test.mean=0.0467; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 6: C=0.25; sigma=0.5\n#  [Tune-y] 6: mmce.test.mean=0.06; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 7: C=0.5; sigma=0.5\n#  [Tune-y] 7: mmce.test.mean=0.04; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 8: C=1; sigma=0.5\n#  [Tune-y] 8: mmce.test.mean=0.04; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 9: C=2; sigma=0.5\n#  [Tune-y] 9: mmce.test.mean=0.0467; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 10: C=4; sigma=0.5\n#  [Tune-y] 10: mmce.test.mean=0.0467; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 11: C=0.25; sigma=1\n#  [Tune-y] 11: mmce.test.mean=0.0533; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 12: C=0.5; sigma=1\n#  [Tune-y] 12: mmce.test.mean=0.04; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 13: C=1; sigma=1\n#  [Tune-y] 13: mmce.test.mean=0.0467; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 14: C=2; sigma=1\n#  [Tune-y] 14: mmce.test.mean=0.0467; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 15: C=4; sigma=1\n#  [Tune-y] 15: mmce.test.mean=0.0533; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 16: C=0.25; sigma=2\n#  [Tune-y] 16: mmce.test.mean=0.0533; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 17: C=0.5; sigma=2\n#  [Tune-y] 17: mmce.test.mean=0.04; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 18: C=1; sigma=2\n#  [Tune-y] 18: mmce.test.mean=0.0333; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 19: C=2; sigma=2\n#  [Tune-y] 19: mmce.test.mean=0.04; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 20: C=4; sigma=2\n#  [Tune-y] 20: mmce.test.mean=0.0467; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 21: C=0.25; sigma=4\n#  [Tune-y] 21: mmce.test.mean=0.113; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 22: C=0.5; sigma=4\n#  [Tune-y] 22: mmce.test.mean=0.0667; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 23: C=1; sigma=4\n#  [Tune-y] 23: mmce.test.mean=0.0533; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 24: C=2; sigma=4\n#  [Tune-y] 24: mmce.test.mean=0.06; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune-x] 25: C=4; sigma=4\n#  [Tune-y] 25: mmce.test.mean=0.0667; time: 0.0 min; memory: 139Mb use, 215Mb max\n#  [Tune] Result: C=1; sigma=2 : mmce.test.mean=0.0333\nres\n#  Tune result:\n#  Op. pars: C=1; sigma=2\n#  mmce.test.mean=0.0333  tuneParams  simply performs the cross-validation for every element of the\ncross-product and selects the parameter setting with the best mean performance.\nAs no performance measure was specified, by default the error rate ( mmce ) is\nused.  Note that each  measure  \"knows\" if it is minimized or maximized during tuning.  ## error rate\nmmce$minimize\n#  [1] TRUE\n\n## accuracy\nacc$minimize\n#  [1] FALSE  Of course, you can pass other measures and also a list of measures to  tuneParams .\nIn the latter case the first measure is optimized during tuning, the others are simply evaluated.\nIf you are interested in optimizing several measures simultaneously have a look at the\nparagraph about multi-criteria tuning below.  In the example below we calculate the accuracy ( acc ) instead of the error\nrate.\nWe use function  setAggregation , as described in the section on  resampling ,\nto additionally obtain the standard deviation of the accuracy.  res = tuneParams( classif.ksvm , task = iris.task, resampling = rdesc, par.set = ps,\n  control = ctrl, measures = list(acc, setAggregation(acc, test.sd)), show.info = FALSE)\nres\n#  Tune result:\n#  Op. pars: C=0.25; sigma=0.25\n#  acc.test.mean=0.953,acc.test.sd=0.0306  Accessing the tuning result  The result object  TuneResult  allows you to access the best found settings  $x  and their\nestimated performance  $y .  res$x\n#  $C\n#  [1] 0.25\n#  \n#  $sigma\n#  [1] 0.25\nres$y\n#  acc.test.mean   acc.test.sd \n#      0.9533333     0.0305505  Moreover, we can inspect all points evaluated during the search by accessing the $opt.path  (see also the documentation of  OptPath ).  res$opt.path\n#  Optimization path\n#    Dimensions: x = 2/2, y = 2\n#    Length: 25\n#    Add x values transformed: FALSE\n#    Error messages: TRUE. Errors: 0 / 25.\n#    Exec times: TRUE. Range: 0.086 - 0.17. 0 NAs.\nopt.grid = as.data.frame(res$opt.path)\nhead(opt.grid)\n#       C sigma acc.test.mean acc.test.sd dob eol error.message exec.time\n#  1 0.25  0.25     0.9533333  0.03055050   1  NA           NA      0.095\n#  2  0.5  0.25     0.9466667  0.02309401   2  NA           NA      0.086\n#  3    1  0.25     0.9533333  0.01154701   3  NA           NA      0.102\n#  4    2  0.25     0.9533333  0.01154701   4  NA           NA      0.088\n#  5    4  0.25     0.9533333  0.01154701   5  NA           NA      0.170\n#  6 0.25   0.5     0.9333333  0.01154701   6  NA           NA      0.089  A quick visualization of the performance values on the search grid can be accomplished as follows:  library(ggplot2)\ng = ggplot(opt.grid, aes(x = C, y = sigma, fill = acc.test.mean, label = round(acc.test.sd, 3)))\ng + geom_tile() + geom_text(color =  white )     The colors of the tiles display the achieved accuracy, the tile labels show the standard deviation.  Using the optimal parameter values  After tuning you can generate a  Learner  with optimal hyperparameter settings\nas follows:  lrn = setHyperPars(makeLearner( classif.ksvm ), par.vals = res$x)\nlrn\n#  Learner classif.ksvm from package kernlab\n#  Type: classif\n#  Name: Support Vector Machines; Short name: ksvm\n#  Class: classif.ksvm\n#  Properties: twoclass,multiclass,numerics,factors,prob\n#  Predict-Type: response\n#  Hyperparameters: fit=FALSE,C=0.25,sigma=0.25  Then you can proceed as usual.\nHere we refit and predict the learner on the complete  iris  data\nset.  m = train(lrn, iris.task)\npredict(m, task = iris.task)\n#  Prediction: 150 observations\n#  predict.type: response\n#  threshold: \n#  time: 0.01\n#    id  truth response\n#  1  1 setosa   setosa\n#  2  2 setosa   setosa\n#  3  3 setosa   setosa\n#  4  4 setosa   setosa\n#  5  5 setosa   setosa\n#  6  6 setosa   setosa  Grid search without manual discretization  We can also specify the true numeric parameter types of  C  and  sigma  when creating the\nparameter set and use the  resolution  option of  makeTuneControlGrid  to\nautomatically discretize them.  Note how we also make use of the  trafo  option when creating the parameter set to easily\noptimize on a log-scale.  Trafos work like this: All optimizers basically see the parameters on their\noriginal scale (from -12 to 12) in this case and produce values on this scale during the search.\nRight before they are passed to the learning algorithm, the transformation function is applied.  ps = makeParamSet(\n  makeNumericParam( C , lower = -12, upper = 12, trafo = function(x) 2^x),\n  makeNumericParam( sigma , lower = -12, upper = 12, trafo = function(x) 2^x)\n)\nctrl = makeTuneControlGrid(resolution = 3L)\nrdesc = makeResampleDesc( CV , iters = 2L)\nres = tuneParams( classif.ksvm , iris.task, rdesc, par.set = ps, control = ctrl)\n#  [Tune] Started tuning learner classif.ksvm for parameter set:\n#           Type len Def    Constr Req Tunable Trafo\n#  C     numeric   -   - -12 to 12   -    TRUE     Y\n#  sigma numeric   -   - -12 to 12   -    TRUE     Y\n#  With control class: TuneControlGrid\n#  Imputation value: 1\n#  [Tune-x] 1: C=0.000244; sigma=0.000244\n#  [Tune-y] 1: mmce.test.mean=0.527; time: 0.0 min; memory: 139Mb use, 251Mb max\n#  [Tune-x] 2: C=1; sigma=0.000244\n#  [Tune-y] 2: mmce.test.mean=0.527; time: 0.0 min; memory: 139Mb use, 251Mb max\n#  [Tune-x] 3: C=4.1e+03; sigma=0.000244\n#  [Tune-y] 3: mmce.test.mean=0.04; time: 0.0 min; memory: 139Mb use, 251Mb max\n#  [Tune-x] 4: C=0.000244; sigma=1\n#  [Tune-y] 4: mmce.test.mean=0.527; time: 0.0 min; memory: 139Mb use, 251Mb max\n#  [Tune-x] 5: C=1; sigma=1\n#  [Tune-y] 5: mmce.test.mean=0.04; time: 0.0 min; memory: 139Mb use, 251Mb max\n#  [Tune-x] 6: C=4.1e+03; sigma=1\n#  [Tune-y] 6: mmce.test.mean=0.0667; time: 0.0 min; memory: 139Mb use, 251Mb max\n#  [Tune-x] 7: C=0.000244; sigma=4.1e+03\n#  [Tune-y] 7: mmce.test.mean=0.567; time: 0.0 min; memory: 139Mb use, 251Mb max\n#  [Tune-x] 8: C=1; sigma=4.1e+03\n#  [Tune-y] 8: mmce.test.mean=0.687; time: 0.0 min; memory: 139Mb use, 251Mb max\n#  [Tune-x] 9: C=4.1e+03; sigma=4.1e+03\n#  [Tune-y] 9: mmce.test.mean=0.687; time: 0.0 min; memory: 139Mb use, 251Mb max\n#  [Tune] Result: C=1; sigma=1 : mmce.test.mean=0.04\nres\n#  Tune result:\n#  Op. pars: C=1; sigma=1\n#  mmce.test.mean=0.04  Note that  res$opt.path  contains the parameter values  on the original scale .  as.data.frame(res$opt.path)\n#      C sigma mmce.test.mean dob eol error.message exec.time\n#  1 -12   -12     0.52666667   1  NA           NA      0.094\n#  2   0   -12     0.52666667   2  NA           NA      0.068\n#  3  12   -12     0.04000000   3  NA           NA      0.063\n#  4 -12     0     0.52666667   4  NA           NA      0.064\n#  5   0     0     0.04000000   5  NA           NA      0.067\n#  6  12     0     0.06666667   6  NA           NA      0.067\n#  7 -12    12     0.56666667   7  NA           NA      0.066\n#  8   0    12     0.68666667   8  NA           NA      0.067\n#  9  12    12     0.68666667   9  NA           NA      0.064  In order to get the  transformed  parameter values instead, use function trafoOptPath .  as.data.frame(trafoOptPath(res$opt.path))\n#               C        sigma mmce.test.mean dob eol\n#  1 2.441406e-04 2.441406e-04     0.52666667   1  NA\n#  2 1.000000e+00 2.441406e-04     0.52666667   2  NA\n#  3 4.096000e+03 2.441406e-04     0.04000000   3  NA\n#  4 2.441406e-04 1.000000e+00     0.52666667   4  NA\n#  5 1.000000e+00 1.000000e+00     0.04000000   5  NA\n#  6 4.096000e+03 1.000000e+00     0.06666667   6  NA\n#  7 2.441406e-04 4.096000e+03     0.56666667   7  NA\n#  8 1.000000e+00 4.096000e+03     0.68666667   8  NA\n#  9 4.096000e+03 4.096000e+03     0.68666667   9  NA", 
            "title": "Basics"
        }, 
        {
            "location": "/tune/index.html#iterated-f-racing-for-mixed-spaces-and-dependencies", 
            "text": "The package supports a larger number of tuning algorithms, which can all be looked up and\nselected via  TuneControl . One of the cooler algorithms is iterated F-racing from the  irace  package. This not only works for arbitrary parameter types (numeric, integer,\ndiscrete, logical), but also for so-called dependent / hierarchical parameters:  ps = makeParamSet(\n  makeNumericParam( C , lower = -12, upper = 12, trafo = function(x) 2^x),\n  makeDiscreteParam( kernel , values = c( vanilladot ,  polydot ,  rbfdot )),\n  makeNumericParam( sigma , lower = -12, upper = 12, trafo = function(x) 2^x,\n    requires = quote(kernel ==  rbfdot )),\n  makeIntegerParam( degree , lower = 2L, upper = 5L,\n    requires = quote(kernel ==  polydot ))\n)\nctrl = makeTuneControlIrace(maxExperiments = 200L)\nrdesc = makeResampleDesc( Holdout )\nres = tuneParams( classif.ksvm , iris.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)\nprint(head(as.data.frame(res$opt.path)))\n#            C     kernel      sigma degree mmce.test.mean dob eol\n#  1  4.226737 vanilladot         NA     NA           0.06   1  NA\n#  2  1.699061    polydot         NA      4           0.06   2  NA\n#  3 -2.001464     rbfdot -0.5400968     NA           0.04   3  NA\n#  4 10.508751     rbfdot -6.6748193     NA           0.06   4  NA\n#  5  7.772697 vanilladot         NA     NA           0.04   5  NA\n#  6 -6.093697 vanilladot         NA     NA           0.10   6  NA\n#    error.message exec.time\n#  1           NA      0.041\n#  2           NA      0.037\n#  3           NA      0.034\n#  4           NA      0.034\n#  5           NA      0.037\n#  6           NA      0.034  See how we made the kernel parameters like  sigma  and  degree  dependent on the  kernel \nselection parameters? This approach allows you to tune parameters of multiple kernels at once, \nefficiently concentrating on the ones which work best for your given data set.", 
            "title": "Iterated F-Racing for mixed spaces and dependencies"
        }, 
        {
            "location": "/tune/index.html#tuning-across-whole-model-spaces-with-modelmultiplexer", 
            "text": "We can now take the following example even one step further. If we use the ModelMultiplexer  we can tune over different model classes at once,\njust as we did with the SVM kernels above.  base.learners = list(\n  makeLearner( classif.ksvm ),\n  makeLearner( classif.randomForest )\n)\nlrn = makeModelMultiplexer(base.learners)  Function  makeModelMultiplexerParamSet  offers a simple way to contruct parameter set for tuning:\nThe parameter names are prefixed automatically and the  requires  element is set, too,\nto make all paramaters subordinate to  selected.learner .  ps = makeModelMultiplexerParamSet(lrn,\n  makeNumericParam( sigma , lower = -12, upper = 12, trafo = function(x) 2^x),\n  makeIntegerParam( ntree , lower = 1L, upper = 500L)\n)\nprint(ps)\n#                                 Type len Def\n#  selected.learner           discrete   -   -\n#  classif.ksvm.sigma          numeric   -   -\n#  classif.randomForest.ntree  integer   -   -\n#                                                        Constr Req Tunable\n#  selected.learner           classif.ksvm,classif.randomForest   -    TRUE\n#  classif.ksvm.sigma                                 -12 to 12   Y    TRUE\n#  classif.randomForest.ntree                          1 to 500   Y    TRUE\n#                             Trafo\n#  selected.learner               -\n#  classif.ksvm.sigma             Y\n#  classif.randomForest.ntree     -\nrdesc = makeResampleDesc( CV , iters = 2L)\nctrl = makeTuneControlIrace(maxExperiments = 200L)\nres = tuneParams(lrn, iris.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)\nprint(head(as.data.frame(res$opt.path)))\n#        selected.learner classif.ksvm.sigma classif.randomForest.ntree\n#  1 classif.randomForest                 NA                         81\n#  2         classif.ksvm          -2.758811                         NA\n#  3         classif.ksvm          -2.485181                         NA\n#  4         classif.ksvm          -6.056252                         NA\n#  5         classif.ksvm         -11.450293                         NA\n#  6         classif.ksvm           9.867756                         NA\n#    mmce.test.mean dob eol error.message exec.time\n#  1     0.03333333   1  NA           NA      0.074\n#  2     0.06666667   2  NA           NA      0.065\n#  3     0.06666667   3  NA           NA      0.068\n#  4     0.08666667   4  NA           NA      0.064\n#  5     0.52666667   5  NA           NA      0.068\n#  6     0.67333333   6  NA           NA      0.068", 
            "title": "Tuning across whole model spaces with ModelMultiplexer"
        }, 
        {
            "location": "/tune/index.html#nested-resampling", 
            "text": "As we continually optimize over the same data during tuning, the estimated\nperformance value  res$y  might be optimistically biased.\nA clean approach to ensure unbiased performance estimation is nested resampling,\nwhere we embed the whole model selection process into an outer resampling loop.  Actually, we can get this for free without programming any looping by simply using the wrapper functionality  of  mlr .\nSee also function  makeTuneWrapper .  Let's use cross-validation with 5 folds in the outer loop and use simple\nholdout test set estimation during tuning in the inner loop.  ps = makeParamSet(\n  makeDiscreteParam( C , values = 2^(-2:2)),\n  makeDiscreteParam( sigma , values = 2^(-2:2))\n)\nctrl = makeTuneControlGrid()\ninner = makeResampleDesc( Holdout )\nouter = makeResampleDesc( CV , iters = 5)\nlrn = makeTuneWrapper( classif.ksvm , inner, par.set = ps, control = ctrl, show.info = FALSE)\nr = resample(lrn, iris.task, resampling = outer, extract = getTuneResult, show.info = FALSE)  If we want to find out how good those configurations are on the entire data\nset, we can look at the measures that we already know from  resampling .\nWe receive 5 misclassification errors (one for each optimal parameter\nconfiguration per outer fold) and one aggregated version, i.e., the mean,\nof those 5 values.  r$measures.test\n#    iter       mmce\n#  1    1 0.10000000\n#  2    2 0.03333333\n#  3    3 0.03333333\n#  4    4 0.03333333\n#  5    5 0.03333333\nr$aggr\n#  mmce.test.mean \n#      0.04666667  Accessing the tuning result  We have kept the results of the tuning for further evaluations.\nFor example one might want to find out, if the best obtained configurations vary for the\ndifferent outer splits.\nAs storing entire models may be expensive we used the  extract  option of  resample .\nFunction  getTuneResult  returns the optimal parameter values and the optimization path\nfor each iteration of the outer resampling loop.  r$extract\n#  [[1]]\n#  Tune result:\n#  Op. pars: C=2; sigma=0.5\n#  mmce.test.mean=   0\n#  \n#  [[2]]\n#  Tune result:\n#  Op. pars: C=4; sigma=1\n#  mmce.test.mean=0.025\n#  \n#  [[3]]\n#  Tune result:\n#  Op. pars: C=1; sigma=0.5\n#  mmce.test.mean=0.05\n#  \n#  [[4]]\n#  Tune result:\n#  Op. pars: C=2; sigma=0.25\n#  mmce.test.mean=0.025\n#  \n#  [[5]]\n#  Tune result:\n#  Op. pars: C=1; sigma=0.25\n#  mmce.test.mean=0.025  We can compare the optimal parameter settings obtained in the 5 resampling iterations.\nAs you can see, the optimal configuration usually depends on the data. You may\nbe able to identify a  range  of parameter settings that achieve good\nperformance though, e.g., the values for  C  should be at least 1 and the values\nfor  sigma  should be between 0 and 1.  In the following we extract the  opt.path 's for each of the 5 cross-validation iterations.  opt.paths = lapply(r$extract, function(x) as.data.frame(x$opt.path))\nhead(opt.paths[[1]])\n#       C sigma mmce.test.mean dob eol error.message exec.time\n#  1 0.25  0.25          0.050   1  NA           NA      0.036\n#  2  0.5  0.25          0.025   2  NA           NA      0.039\n#  3    1  0.25          0.025   3  NA           NA      0.042\n#  4    2  0.25          0.025   4  NA           NA      0.044\n#  5    4  0.25          0.025   5  NA           NA      0.036\n#  6 0.25   0.5          0.050   6  NA           NA      0.037  With the following code you can visualize the  opt.path  of the first resampling iteration.  opt.mmce = lapply(opt.paths, function(x) x$mmce.test.mean)\nopt.grid = opt.paths[[1]][, 1:2]\nopt.grid$mmce.test.mean = apply(simplify2array(opt.mmce), 1, mean)\ng = ggplot(opt.grid, aes(x = C, y = sigma, fill = mmce.test.mean))\ng + geom_tile()", 
            "title": "Nested resampling"
        }, 
        {
            "location": "/tune/index.html#multi-criteria-evaluation-and-optimization", 
            "text": "During tuning you might want to optimize multiple, potentially conflicting, performance measures\nsimultaneously.  In the following example we aim to minimize both, the false positive and the false negative rates\n( fpr  and  fnr ).\nWe again tune the hyperparameters of an SVM (function  ksvm ) with a radial\nbasis kernel and use the  sonar classification task  for illustration.\nAs search strategy we choose a random search.  For all available multi-criteria tuning algorithms see  TuneMultiCritControl .  ps = makeParamSet(\n  makeNumericParam( C , lower = -12, upper = 12, trafo = function(x) 2^x),\n  makeNumericParam( sigma , lower = -12, upper = 12, trafo = function(x) 2^x)\n)\nctrl = makeTuneMultiCritControlRandom(maxit = 30L)\nrdesc = makeResampleDesc( Holdout )\nres = tuneParamsMultiCrit( classif.ksvm , task = sonar.task, resampling = rdesc, par.set = ps,\n  measures = list(fpr, fnr), control = ctrl, show.info = FALSE)\nres\n#  Tune multicrit result:\n#  Points on front: 6\nhead(as.data.frame(trafoOptPath(res$opt.path)))\n#               C        sigma fpr.test.mean fnr.test.mean dob eol\n#  1 3.580062e+00 1.116196e+00     1.0000000    0.00000000   1  NA\n#  2 2.380537e+01 1.704055e+00     1.0000000    0.00000000   2  NA\n#  3 3.394881e+01 6.680306e-04     0.4102564    0.06451613   3  NA\n#  4 2.617769e+02 3.708341e-01     0.9230769    0.00000000   4  NA\n#  5 2.459032e+01 5.099661e+01     1.0000000    0.00000000   5  NA\n#  6 1.031688e-03 1.843696e-02     1.0000000    0.00000000   6  NA  The results can be visualized with function  plotTuneMultiCritResult .\nThe plot shows the false positive and false negative rates for all parameter settings evaluated\nduring tuning. Points on the Pareto front are slightly increased.  plotTuneMultiCritResult(res)", 
            "title": "Multi-criteria evaluation and optimization"
        }, 
        {
            "location": "/tune/index.html#further-comments", 
            "text": "Tuning works for all other tasks like regression, survival analysis and so on in a completely\n  similar fashion.    In longer running tuning experiments it is very annoying if the computation stops due to\n  numerical or other errors. Have a look at  on.learner.error  in  configureMlr  as well as\n  the examples given in section  Configure mlr  of this tutorial.\n  You might also want to inform yourself about  impute.val  in  TuneControl .", 
            "title": "Further comments"
        }, 
        {
            "location": "/feature_selection/index.html", 
            "text": "Feature Selection\n\n\nOften, data sets include a large number of features.\nThe technique of extracting a subset of relevant features is called feature selection.\nFeature selection can enhance the interpretability of the model, speed up the learning\nprocess and improve the learner performance.\nThere exist different approaches to identify the relevant features.\n\nmlr\n supports \nfilter\n and \nwrapper methods\n.\n\n\nFilter methods\n\n\nFilter methods assign an importance value to each feature.\nBased on these values the features can be ranked and a feature subset can be selected.\n\n\nCalculating the feature importance\n\n\nDifferent methods for calculating the feature importance are built into \nmlr\n's function\n\ngenerateFilterValuesData\n (\ngetFilterValues\n has been deprecated in favor of \ngenerateFilterValuesData\n.). Currently, classification, regression and survival analysis tasks\nare supported. A table showing all available methods can be found \nhere\n.\n\n\nFunction \ngenerateFilterValuesData\n requires the \nTask\n and a character string specifying the filter\nmethod.\n\n\nfv = generateFilterValuesData(iris.task, method = \ninformation.gain\n)\nfv\n#\n FilterValues:\n#\n Task: iris-example\n#\n           name    type information.gain\n#\n 1 Sepal.Length numeric        0.4521286\n#\n 2  Sepal.Width numeric        0.2672750\n#\n 3 Petal.Length numeric        0.9402853\n#\n 4  Petal.Width numeric        0.9554360\n\n\n\n\nfv\n is a \nFilterValues\n object and \nfv$data\n contains a \ndata.frame\n\nthat gives the importance values for all features. Optionally, a vector of filter methods can be\npassed.\n\n\nfv2 = generateFilterValuesData(iris.task, method = c(\ninformation.gain\n, \nchi.squared\n))\nfv2$data\n#\n           name    type information.gain chi.squared\n#\n 1 Sepal.Length numeric        0.4521286   0.6288067\n#\n 2  Sepal.Width numeric        0.2672750   0.4922162\n#\n 3 Petal.Length numeric        0.9402853   0.9346311\n#\n 4  Petal.Width numeric        0.9554360   0.9432359\n\n\n\n\nA bar plot of importance values for the individual features can be obtained using\nfunction \nplotFilterValues\n.\n\n\nplotFilterValues(fv2)\n\n\n\n\n \n\n\nBy default \nplotFilterValues\n will create facetted subplots if multiple filter methods are passed as input to\n\ngenerateFilterValuesData\n.\n\n\nThere is also an experimental \nggvis\n plotting function, \nplotFilterValuesGGVIS\n. This takes the same\narguments as \nplotFilterValues\n with the addition of a \nlogical(1)\n argument \ninteractive\n, which, if used with\nan output from \ngenerateFilterValuesData\n that includes multiple filter values, produces a \nshiny\n application\nthat allows the interactive selection of the displayed filter method.\n\n\nplotFilterValuesGGVIS(fv2, interactive = TRUE)\n\n\n\n\nAccording to the \n\"information.gain\"\n measure, \nPetal.Width\n and \nPetal.Length\n\ncontain the most information about the target variable \nSpecies\n.\n\n\nSelecting a feature subset\n\n\nWith \nmlr\n's function \nfilterFeatures\n you can create a new \nTask\n by leaving out\nfeatures of lower importance.\n\n\nThere are several ways to select a feature subset based on feature importance values:\n\n\n\n\nKeep a certain \nabsolute number\n (\nabs\n) of features with highest importance.\n\n\nKeep a certain \npercentage\n (\nperc\n) of features with highest importance.\n\n\nKeep all features whose importance exceeds a certain \nthreshold value\n (\nthreshold\n).\n\n\n\n\nFunction \nfilterFeatures\n supports these three methods as shown in the following example.\nMoreover, you can either specify the \nmethod\n for calculating the feature importance or you can\nuse previously computed importance values via argument \nfval\n.\n\n\n## Keep the 2 most important features\nfiltered.task = filterFeatures(iris.task, method = \ninformation.gain\n, abs = 2)\n\n## Keep the 25% most important features\nfiltered.task = filterFeatures(iris.task, fval = fv, perc = 0.25)\n\n## Keep all features with importance greater than 0.5\nfiltered.task = filterFeatures(iris.task, fval = fv, threshold = 0.5)\nfiltered.task\n#\n Supervised task: iris-example\n#\n Type: classif\n#\n Target: Species\n#\n Observations: 150\n#\n Features:\n#\n numerics  factors  ordered \n#\n        2        0        0 \n#\n Missings: FALSE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n#\n Classes: 3\n#\n     setosa versicolor  virginica \n#\n         50         50         50 \n#\n Positive class: NA\n\n\n\n\nFuse a learner with a filter method\n\n\nOften feature selection based on a filter method is part of the data preprocessing and in\na subsequent step a learning method is applied to the filtered data.\nIn a proper experimental setup you might want to automate the selection of the\nfeatures so that it can be part of the validation method of your choice.\nA \nLearner\n can be fused with a filter method by function \nmakeFilterWrapper\n.\nThe resulting \nLearner\n has the additional class attribute \nFilterWrapper\n.\n\n\nIn the following example we calculate the 10-fold cross-validated error rate (\nmmce\n)\nof the \nk nearest neighbor classifier\n with preceding feature selection on the\n\niris\n data set.\nWe use \n\"information.gain\"\n as importance measure and select the 2 features with\nhighest importance.\nIn each resampling iteration feature selection is carried out on the corresponding training\ndata set before fitting the learner.\n\n\nlrn = makeFilterWrapper(learner = \nclassif.fnn\n, fw.method = \ninformation.gain\n, fw.abs = 2)\nrdesc = makeResampleDesc(\nCV\n, iters = 10)\nr = resample(learner = lrn, task = iris.task, resampling = rdesc, show.info = FALSE, models = TRUE)\nr$aggr\n#\n mmce.test.mean \n#\n           0.04\n\n\n\n\nYou may want to know which features have been used. Luckily, we have called\n\nresample\n with the argument \nmodels = TRUE\n, which means that \nr$models\n\ncontains a \nlist\n of \nmodels\n fitted in the individual resampling iterations.\nIn order to access the selected feature subsets we can call \ngetFilteredFeatures\n on each model.\n\n\nsfeats = sapply(r$models, getFilteredFeatures)\ntable(sfeats)\n#\n sfeats\n#\n Petal.Length  Petal.Width \n#\n           10           10\n\n\n\n\nThe selection of features seems to be very stable.\nThe features \nSepal.Length\n and \nSepal.Width\n did not make it into a single fold.\n\n\nTuning the size of the feature subset\n\n\nIn the above examples the number/percentage of features to select or the threshold value\nhave been arbitrarily chosen.\nIf filtering is a preprocessing step before applying a learning method optimal values\nwith regard to the learner performance can be found by \ntuning\n.\n\n\nIn the following regression example we consider the \nBostonHousing\n data set.\nWe use a \nlinear regression model\n and determine the optimal percentage value for feature selection\nsuch that the 3-fold cross-validated \nmean squared error\n of the learner is minimal.\nAs search strategy for tuning a grid search is used.\n\n\nlrn = makeFilterWrapper(learner = \nregr.lm\n, fw.method = \nchi.squared\n)\nps = makeParamSet(makeDiscreteParam(\nfw.perc\n, values = seq(0.2, 0.5, 0.05)))\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\nres = tuneParams(lrn, task = bh.task, resampling = rdesc, par.set = ps,\n  control = makeTuneControlGrid())\n#\n [Tune] Started tuning learner regr.lm.filtered for parameter set:\n#\n             Type len Def                         Constr Req Tunable Trafo\n#\n fw.perc discrete   -   - 0.2,0.25,0.3,0.35,0.4,0.45,0.5   -    TRUE     -\n#\n With control class: TuneControlGrid\n#\n Imputation value: Inf\n#\n [Tune-x] 1: fw.perc=0.2\n#\n [Tune-y] 1: mse.test.mean=40.6; time: 0.0 min; memory: 122Mb use, 457Mb max\n#\n [Tune-x] 2: fw.perc=0.25\n#\n [Tune-y] 2: mse.test.mean=40.6; time: 0.0 min; memory: 122Mb use, 457Mb max\n#\n [Tune-x] 3: fw.perc=0.3\n#\n [Tune-y] 3: mse.test.mean=37.1; time: 0.0 min; memory: 122Mb use, 457Mb max\n#\n [Tune-x] 4: fw.perc=0.35\n#\n [Tune-y] 4: mse.test.mean=35.8; time: 0.0 min; memory: 122Mb use, 457Mb max\n#\n [Tune-x] 5: fw.perc=0.4\n#\n [Tune-y] 5: mse.test.mean=35.8; time: 0.0 min; memory: 122Mb use, 457Mb max\n#\n [Tune-x] 6: fw.perc=0.45\n#\n [Tune-y] 6: mse.test.mean=27.4; time: 0.0 min; memory: 122Mb use, 457Mb max\n#\n [Tune-x] 7: fw.perc=0.5\n#\n [Tune-y] 7: mse.test.mean=27.4; time: 0.0 min; memory: 122Mb use, 457Mb max\n#\n [Tune] Result: fw.perc=0.5 : mse.test.mean=27.4\nres\n#\n Tune result:\n#\n Op. pars: fw.perc=0.5\n#\n mse.test.mean=27.4\n\n\n\n\nThe performance of all percentage values visited during tuning is:\n\n\nas.data.frame(res$opt.path)\n#\n   fw.perc mse.test.mean dob eol error.message exec.time\n#\n 1     0.2      40.59578   1  NA          \nNA\n     0.360\n#\n 2    0.25      40.59578   2  NA          \nNA\n     0.338\n#\n 3     0.3      37.05592   3  NA          \nNA\n     0.308\n#\n 4    0.35      35.83712   4  NA          \nNA\n     0.298\n#\n 5     0.4      35.83712   5  NA          \nNA\n     0.290\n#\n 6    0.45      27.39955   6  NA          \nNA\n     0.292\n#\n 7     0.5      27.39955   7  NA          \nNA\n     0.289\n\n\n\n\nThe optimal percentage and the corresponding performance can be accessed as follows:\n\n\nres$x\n#\n $fw.perc\n#\n [1] 0.5\nres$y\n#\n mse.test.mean \n#\n      27.39955\n\n\n\n\nAfter tuning we can generate a new wrapped learner with the optimal percentage value for\nfurther use.\n\n\nlrn = makeFilterWrapper(learner = \nregr.lm\n, fw.method = \nchi.squared\n, fw.perc = res$x$fw.perc)\nmod = train(lrn, bh.task)\nmod\n#\n Model for learner.id=regr.lm.filtered; learner.class=FilterWrapper\n#\n Trained on: task.id = BostonHousing-example; obs = 506; features = 13\n#\n Hyperparameters: fw.method=chi.squared,fw.perc=0.5\n\ngetFilteredFeatures(mod)\n#\n [1] \ncrim\n  \nzn\n    \nrm\n    \ndis\n   \nrad\n   \nlstat\n\n\n\n\n\nHere is another example using \nmulti-criteria tuning\n.\nWe consider \nlinear discriminant analysis\n with precedent feature selection based on\nthe Chi-squared statistic of independence (\n\"chi.squared\"\n) on the \nSonar\n\ndata set and tune the threshold value.\nDuring tuning both, the false positive and the false negative rate (\nfpr\n and\n\nfnr\n), are minimized. As search strategy we choose a random search\n(see \nmakeTuneMultiCritControlRandom\n).\n\n\nlrn = makeFilterWrapper(learner = \nclassif.lda\n, fw.method = \nchi.squared\n)\nps = makeParamSet(makeNumericParam(\nfw.threshold\n, lower = 0.1, upper = 0.9))\nrdesc = makeResampleDesc(\nCV\n, iters = 10)\nres = tuneParamsMultiCrit(lrn, task = sonar.task, resampling = rdesc, par.set = ps,\n  measures = list(fpr, fnr), control = makeTuneMultiCritControlRandom(maxit = 50L),\n  show.info = FALSE)\nres\n#\n Tune multicrit result:\n#\n Points on front: 13\nhead(as.data.frame(res$opt.path))\n#\n   fw.threshold fpr.test.mean fnr.test.mean dob eol error.message exec.time\n#\n 1    0.4892321     0.3092818     0.2639033   1  NA          \nNA\n     3.142\n#\n 2    0.2481696     0.2045499     0.2319697   2  NA          \nNA\n     3.166\n#\n 3    0.7691875     0.5128000     0.3459740   3  NA          \nNA\n     2.923\n#\n 4    0.1470133     0.2045499     0.2319697   4  NA          \nNA\n     3.187\n#\n 5    0.5958241     0.5028216     0.5239538   5  NA          \nNA\n     2.797\n#\n 6    0.6892421     0.6323959     0.4480808   6  NA          \nNA\n     2.803\n\n\n\n\nThe results can be visualized with function \nplotTuneMultiCritResult\n.\nThe plot shows the false positive and false negative rates for all parameter values visited\nduring tuning. The size of the points on the Pareto front is slightly increased.\n\n\nplotTuneMultiCritResult(res)\n\n\n\n\n \n\n\nWrapper methods\n\n\nWrapper methods use the performance of a learning algorithm to assess the usefulness of\na feature set.\nIn order to select a feature subset a learner is trained repeatedly on different feature subsets\nand the subset which leads to the best learner performance is chosen.\n\n\nIn order to use the wrapper approach we have to decide:\n\n\n\n\nHow to assess the performance: This involves choosing a performance measure that serves\n  as feature selection criterion and a resampling strategy.\n\n\nWhich learning method to use.\n\n\nHow to search the space of possible feature subsets.\n\n\n\n\nThe search strategy is defined by functions following the naming convention\n\nmakeFeatSelControl\nsearch_strategy\n.\nThe following search strategies are available:\n\n\n\n\nExhaustive search (\nmakeFeatSelControlExhaustive\n),\n\n\nGenetic algorithm (\nmakeFeatSelControlGA\n),\n\n\nRandom search (\nmakeFeatSelControlRandom\n),\n\n\nDeterministic forward or backward search (\nmakeFeatSelControlSequential\n).\n\n\n\n\nSelect a feature subset\n\n\nFeature selection can be conducted with function \nselectFeatures\n.\n\n\nIn the following example we perform an exhaustive search on the\n\nWisconsin Prognostic Breast Cancer\n data set.\nAs learning method we use the \nCox proportional hazards model\n.\nThe performance is assessed by the holdout estimate of the concordance index\n(\ncindex\n).\n\n\n## Specify the search strategy\nctrl = makeFeatSelControlRandom(maxit = 20L)\nctrl\n#\n FeatSel control: FeatSelControlRandom\n#\n Same resampling instance: TRUE\n#\n Imputation value: \nworst\n\n#\n Max. features: \nnot used\n\n#\n Max. iterations: 20\n#\n Tune threshold: FALSE\n#\n Further arguments: prob=0.5\n\n\n\n\nctrl\n is a \nFeatSelControl\n object that contains information about the search strategy\nand potential parameter values.\n\n\n## Resample description\nrdesc = makeResampleDesc(\nHoldout\n)\n\n## Select features\nsfeats = selectFeatures(learner = \nsurv.coxph\n, task = wpbc.task, resampling = rdesc,\n  control = ctrl, show.info = FALSE)\nsfeats\n#\n FeatSel result:\n#\n Features (17): mean_radius, mean_area, mean_smoothness, mean_concavepoints, mean_symmetry, mean_fractaldim, SE_texture, SE_perimeter, SE_smoothness, SE_compactness, SE_concavity, SE_concavepoints, worst_area, worst_compactness, worst_concavepoints, tsize, pnodes\n#\n cindex.test.mean=0.714\n\n\n\n\nsfeats\nis a \nFeatSelResult\n object.\nThe selected features and the corresponding performance can be accessed as follows:\n\n\nsfeats$x\n#\n  [1] \nmean_radius\n         \nmean_area\n           \nmean_smoothness\n    \n#\n  [4] \nmean_concavepoints\n  \nmean_symmetry\n       \nmean_fractaldim\n    \n#\n  [7] \nSE_texture\n          \nSE_perimeter\n        \nSE_smoothness\n      \n#\n [10] \nSE_compactness\n      \nSE_concavity\n        \nSE_concavepoints\n   \n#\n [13] \nworst_area\n          \nworst_compactness\n   \nworst_concavepoints\n\n#\n [16] \ntsize\n               \npnodes\n\nsfeats$y\n#\n cindex.test.mean \n#\n         0.713799\n\n\n\n\nIn a second example we fit a simple linear regression model to the \nBostonHousing\n\ndata set and use a sequential search to find a feature set that minimizes the mean squared\nerror (\nmse\n).\n\nmethod = \"sfs\"\n indicates that we want to conduct a sequential forward search where features\nare added to the model until the performance cannot be improved anymore.\nSee the documentation page \nmakeFeatSelControlSequential\n for other available\nsequential search methods.\nThe search is stopped if the improvement is smaller than \nalpha = 0.02\n.\n\n\n## Specify the search strategy\nctrl = makeFeatSelControlSequential(method = \nsfs\n, alpha = 0.02)\n\n## Select features\nrdesc = makeResampleDesc(\nCV\n, iters = 10)\nsfeats = selectFeatures(learner = \nregr.lm\n, task = bh.task, resampling = rdesc, control = ctrl,\n  show.info = FALSE)\nsfeats\n#\n FeatSel result:\n#\n Features (11): crim, zn, chas, nox, rm, dis, rad, tax, ptratio, b, lstat\n#\n mse.test.mean=23.7\n\n\n\n\nFurther information about the sequential feature selection process can be obtained by\nfunction \nanalyzeFeatSelResult\n.\n\n\nanalyzeFeatSelResult(sfeats)\n#\n Features         : 11\n#\n Performance      : mse.test.mean=23.7\n#\n crim, zn, chas, nox, rm, dis, rad, tax, ptratio, b, lstat\n#\n \n#\n Path to optimum:\n#\n - Features:    0  Init   :                       Perf = 84.831  Diff: NA  *\n#\n - Features:    1  Add    : lstat                 Perf = 38.894  Diff: 45.936  *\n#\n - Features:    2  Add    : rm                    Perf = 31.279  Diff: 7.6156  *\n#\n - Features:    3  Add    : ptratio               Perf = 28.108  Diff: 3.1703  *\n#\n - Features:    4  Add    : dis                   Perf = 27.48  Diff: 0.62813  *\n#\n - Features:    5  Add    : nox                   Perf = 26.079  Diff: 1.4008  *\n#\n - Features:    6  Add    : b                     Perf = 25.563  Diff: 0.51594  *\n#\n - Features:    7  Add    : chas                  Perf = 25.132  Diff: 0.43097  *\n#\n - Features:    8  Add    : zn                    Perf = 24.792  Diff: 0.34018  *\n#\n - Features:    9  Add    : rad                   Perf = 24.599  Diff: 0.19327  *\n#\n - Features:   10  Add    : tax                   Perf = 24.082  Diff: 0.51706  *\n#\n - Features:   11  Add    : crim                  Perf = 23.732  Diff: 0.35  *\n#\n \n#\n Stopped, because no improving feature was found.\n\n\n\n\nFuse a learner with feature selection\n\n\nA \nLearner\n can be fused with a feature selection strategy (i.e., a search\nstrategy, a performance measure and a resampling strategy) by function \nmakeFeatSelWrapper\n.\nDuring training features are selected according to the specified selection scheme. Then, the\nlearner is trained on the selected feature subset.\n\n\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\nlrn = makeFeatSelWrapper(\nsurv.coxph\n, resampling = rdesc,\n  control = makeFeatSelControlRandom(maxit = 10), show.info = FALSE)\nmod = train(lrn, task = wpbc.task)\nmod\n#\n Model for learner.id=surv.coxph.featsel; learner.class=FeatSelWrapper\n#\n Trained on: task.id = wpbc-example; obs = 194; features = 32\n#\n Hyperparameters:\n\n\n\n\nThe result of the feature selection can be extracted by function \ngetFeatSelResult\n.\n\n\nsfeats = getFeatSelResult(mod)\nsfeats\n#\n FeatSel result:\n#\n Features (19): mean_radius, mean_texture, mean_perimeter, mean_area, mean_smoothness, mean_compactness, mean_concavepoints, mean_fractaldim, SE_compactness, SE_concavity, SE_concavepoints, SE_symmetry, worst_texture, worst_perimeter, worst_area, worst_concavepoints, worst_symmetry, tsize, pnodes\n#\n cindex.test.mean=0.631\n\n\n\n\nThe selected features are:\n\n\nsfeats$x\n#\n  [1] \nmean_radius\n         \nmean_texture\n        \nmean_perimeter\n     \n#\n  [4] \nmean_area\n           \nmean_smoothness\n     \nmean_compactness\n   \n#\n  [7] \nmean_concavepoints\n  \nmean_fractaldim\n     \nSE_compactness\n     \n#\n [10] \nSE_concavity\n        \nSE_concavepoints\n    \nSE_symmetry\n        \n#\n [13] \nworst_texture\n       \nworst_perimeter\n     \nworst_area\n         \n#\n [16] \nworst_concavepoints\n \nworst_symmetry\n      \ntsize\n              \n#\n [19] \npnodes\n\n\n\n\n\nThe 5-fold cross-validated performance of the learner specified above can be computed as\nfollows:\n\n\nout.rdesc = makeResampleDesc(\nCV\n, iters = 5)\n\nr = resample(learner = lrn, task = wpbc.task, resampling = out.rdesc, models = TRUE,\n  show.info = FALSE)\nr$aggr\n#\n cindex.test.mean \n#\n         0.632357\n\n\n\n\nThe selected feature sets in the individual resampling iterations can be extracted as follows:\n\n\nlapply(r$models, getFeatSelResult)\n#\n [[1]]\n#\n FeatSel result:\n#\n Features (18): mean_texture, mean_area, mean_smoothness, mean_compactness, mean_concavity, mean_symmetry, SE_radius, SE_compactness, SE_concavity, SE_concavepoints, SE_fractaldim, worst_radius, worst_smoothness, worst_compactness, worst_concavity, worst_symmetry, tsize, pnodes\n#\n cindex.test.mean=0.66\n#\n \n#\n [[2]]\n#\n FeatSel result:\n#\n Features (12): mean_area, mean_compactness, mean_symmetry, mean_fractaldim, SE_perimeter, SE_area, SE_concavity, SE_symmetry, worst_texture, worst_smoothness, worst_fractaldim, tsize\n#\n cindex.test.mean=0.652\n#\n \n#\n [[3]]\n#\n FeatSel result:\n#\n Features (14): mean_compactness, mean_symmetry, mean_fractaldim, SE_radius, SE_perimeter, SE_smoothness, SE_concavity, SE_concavepoints, SE_fractaldim, worst_concavity, worst_concavepoints, worst_symmetry, worst_fractaldim, pnodes\n#\n cindex.test.mean=0.607\n#\n \n#\n [[4]]\n#\n FeatSel result:\n#\n Features (18): mean_radius, mean_texture, mean_perimeter, mean_compactness, mean_concavity, SE_texture, SE_area, SE_smoothness, SE_concavity, SE_symmetry, SE_fractaldim, worst_radius, worst_compactness, worst_concavepoints, worst_symmetry, worst_fractaldim, tsize, pnodes\n#\n cindex.test.mean=0.653\n#\n \n#\n [[5]]\n#\n FeatSel result:\n#\n Features (14): mean_radius, mean_texture, mean_compactness, mean_concavepoints, mean_symmetry, SE_texture, SE_compactness, SE_symmetry, SE_fractaldim, worst_radius, worst_smoothness, worst_compactness, worst_concavity, pnodes\n#\n cindex.test.mean=0.626", 
            "title": "Feature Selection"
        }, 
        {
            "location": "/feature_selection/index.html#feature-selection", 
            "text": "Often, data sets include a large number of features.\nThe technique of extracting a subset of relevant features is called feature selection.\nFeature selection can enhance the interpretability of the model, speed up the learning\nprocess and improve the learner performance.\nThere exist different approaches to identify the relevant features. mlr  supports  filter  and  wrapper methods .", 
            "title": "Feature Selection"
        }, 
        {
            "location": "/feature_selection/index.html#filter-methods", 
            "text": "Filter methods assign an importance value to each feature.\nBased on these values the features can be ranked and a feature subset can be selected.  Calculating the feature importance  Different methods for calculating the feature importance are built into  mlr 's function generateFilterValuesData  ( getFilterValues  has been deprecated in favor of  generateFilterValuesData .). Currently, classification, regression and survival analysis tasks\nare supported. A table showing all available methods can be found  here .  Function  generateFilterValuesData  requires the  Task  and a character string specifying the filter\nmethod.  fv = generateFilterValuesData(iris.task, method =  information.gain )\nfv\n#  FilterValues:\n#  Task: iris-example\n#            name    type information.gain\n#  1 Sepal.Length numeric        0.4521286\n#  2  Sepal.Width numeric        0.2672750\n#  3 Petal.Length numeric        0.9402853\n#  4  Petal.Width numeric        0.9554360  fv  is a  FilterValues  object and  fv$data  contains a  data.frame \nthat gives the importance values for all features. Optionally, a vector of filter methods can be\npassed.  fv2 = generateFilterValuesData(iris.task, method = c( information.gain ,  chi.squared ))\nfv2$data\n#            name    type information.gain chi.squared\n#  1 Sepal.Length numeric        0.4521286   0.6288067\n#  2  Sepal.Width numeric        0.2672750   0.4922162\n#  3 Petal.Length numeric        0.9402853   0.9346311\n#  4  Petal.Width numeric        0.9554360   0.9432359  A bar plot of importance values for the individual features can be obtained using\nfunction  plotFilterValues .  plotFilterValues(fv2)     By default  plotFilterValues  will create facetted subplots if multiple filter methods are passed as input to generateFilterValuesData .  There is also an experimental  ggvis  plotting function,  plotFilterValuesGGVIS . This takes the same\narguments as  plotFilterValues  with the addition of a  logical(1)  argument  interactive , which, if used with\nan output from  generateFilterValuesData  that includes multiple filter values, produces a  shiny  application\nthat allows the interactive selection of the displayed filter method.  plotFilterValuesGGVIS(fv2, interactive = TRUE)  According to the  \"information.gain\"  measure,  Petal.Width  and  Petal.Length \ncontain the most information about the target variable  Species .  Selecting a feature subset  With  mlr 's function  filterFeatures  you can create a new  Task  by leaving out\nfeatures of lower importance.  There are several ways to select a feature subset based on feature importance values:   Keep a certain  absolute number  ( abs ) of features with highest importance.  Keep a certain  percentage  ( perc ) of features with highest importance.  Keep all features whose importance exceeds a certain  threshold value  ( threshold ).   Function  filterFeatures  supports these three methods as shown in the following example.\nMoreover, you can either specify the  method  for calculating the feature importance or you can\nuse previously computed importance values via argument  fval .  ## Keep the 2 most important features\nfiltered.task = filterFeatures(iris.task, method =  information.gain , abs = 2)\n\n## Keep the 25% most important features\nfiltered.task = filterFeatures(iris.task, fval = fv, perc = 0.25)\n\n## Keep all features with importance greater than 0.5\nfiltered.task = filterFeatures(iris.task, fval = fv, threshold = 0.5)\nfiltered.task\n#  Supervised task: iris-example\n#  Type: classif\n#  Target: Species\n#  Observations: 150\n#  Features:\n#  numerics  factors  ordered \n#         2        0        0 \n#  Missings: FALSE\n#  Has weights: FALSE\n#  Has blocking: FALSE\n#  Classes: 3\n#      setosa versicolor  virginica \n#          50         50         50 \n#  Positive class: NA  Fuse a learner with a filter method  Often feature selection based on a filter method is part of the data preprocessing and in\na subsequent step a learning method is applied to the filtered data.\nIn a proper experimental setup you might want to automate the selection of the\nfeatures so that it can be part of the validation method of your choice.\nA  Learner  can be fused with a filter method by function  makeFilterWrapper .\nThe resulting  Learner  has the additional class attribute  FilterWrapper .  In the following example we calculate the 10-fold cross-validated error rate ( mmce )\nof the  k nearest neighbor classifier  with preceding feature selection on the iris  data set.\nWe use  \"information.gain\"  as importance measure and select the 2 features with\nhighest importance.\nIn each resampling iteration feature selection is carried out on the corresponding training\ndata set before fitting the learner.  lrn = makeFilterWrapper(learner =  classif.fnn , fw.method =  information.gain , fw.abs = 2)\nrdesc = makeResampleDesc( CV , iters = 10)\nr = resample(learner = lrn, task = iris.task, resampling = rdesc, show.info = FALSE, models = TRUE)\nr$aggr\n#  mmce.test.mean \n#            0.04  You may want to know which features have been used. Luckily, we have called resample  with the argument  models = TRUE , which means that  r$models \ncontains a  list  of  models  fitted in the individual resampling iterations.\nIn order to access the selected feature subsets we can call  getFilteredFeatures  on each model.  sfeats = sapply(r$models, getFilteredFeatures)\ntable(sfeats)\n#  sfeats\n#  Petal.Length  Petal.Width \n#            10           10  The selection of features seems to be very stable.\nThe features  Sepal.Length  and  Sepal.Width  did not make it into a single fold.  Tuning the size of the feature subset  In the above examples the number/percentage of features to select or the threshold value\nhave been arbitrarily chosen.\nIf filtering is a preprocessing step before applying a learning method optimal values\nwith regard to the learner performance can be found by  tuning .  In the following regression example we consider the  BostonHousing  data set.\nWe use a  linear regression model  and determine the optimal percentage value for feature selection\nsuch that the 3-fold cross-validated  mean squared error  of the learner is minimal.\nAs search strategy for tuning a grid search is used.  lrn = makeFilterWrapper(learner =  regr.lm , fw.method =  chi.squared )\nps = makeParamSet(makeDiscreteParam( fw.perc , values = seq(0.2, 0.5, 0.05)))\nrdesc = makeResampleDesc( CV , iters = 3)\nres = tuneParams(lrn, task = bh.task, resampling = rdesc, par.set = ps,\n  control = makeTuneControlGrid())\n#  [Tune] Started tuning learner regr.lm.filtered for parameter set:\n#              Type len Def                         Constr Req Tunable Trafo\n#  fw.perc discrete   -   - 0.2,0.25,0.3,0.35,0.4,0.45,0.5   -    TRUE     -\n#  With control class: TuneControlGrid\n#  Imputation value: Inf\n#  [Tune-x] 1: fw.perc=0.2\n#  [Tune-y] 1: mse.test.mean=40.6; time: 0.0 min; memory: 122Mb use, 457Mb max\n#  [Tune-x] 2: fw.perc=0.25\n#  [Tune-y] 2: mse.test.mean=40.6; time: 0.0 min; memory: 122Mb use, 457Mb max\n#  [Tune-x] 3: fw.perc=0.3\n#  [Tune-y] 3: mse.test.mean=37.1; time: 0.0 min; memory: 122Mb use, 457Mb max\n#  [Tune-x] 4: fw.perc=0.35\n#  [Tune-y] 4: mse.test.mean=35.8; time: 0.0 min; memory: 122Mb use, 457Mb max\n#  [Tune-x] 5: fw.perc=0.4\n#  [Tune-y] 5: mse.test.mean=35.8; time: 0.0 min; memory: 122Mb use, 457Mb max\n#  [Tune-x] 6: fw.perc=0.45\n#  [Tune-y] 6: mse.test.mean=27.4; time: 0.0 min; memory: 122Mb use, 457Mb max\n#  [Tune-x] 7: fw.perc=0.5\n#  [Tune-y] 7: mse.test.mean=27.4; time: 0.0 min; memory: 122Mb use, 457Mb max\n#  [Tune] Result: fw.perc=0.5 : mse.test.mean=27.4\nres\n#  Tune result:\n#  Op. pars: fw.perc=0.5\n#  mse.test.mean=27.4  The performance of all percentage values visited during tuning is:  as.data.frame(res$opt.path)\n#    fw.perc mse.test.mean dob eol error.message exec.time\n#  1     0.2      40.59578   1  NA           NA      0.360\n#  2    0.25      40.59578   2  NA           NA      0.338\n#  3     0.3      37.05592   3  NA           NA      0.308\n#  4    0.35      35.83712   4  NA           NA      0.298\n#  5     0.4      35.83712   5  NA           NA      0.290\n#  6    0.45      27.39955   6  NA           NA      0.292\n#  7     0.5      27.39955   7  NA           NA      0.289  The optimal percentage and the corresponding performance can be accessed as follows:  res$x\n#  $fw.perc\n#  [1] 0.5\nres$y\n#  mse.test.mean \n#       27.39955  After tuning we can generate a new wrapped learner with the optimal percentage value for\nfurther use.  lrn = makeFilterWrapper(learner =  regr.lm , fw.method =  chi.squared , fw.perc = res$x$fw.perc)\nmod = train(lrn, bh.task)\nmod\n#  Model for learner.id=regr.lm.filtered; learner.class=FilterWrapper\n#  Trained on: task.id = BostonHousing-example; obs = 506; features = 13\n#  Hyperparameters: fw.method=chi.squared,fw.perc=0.5\n\ngetFilteredFeatures(mod)\n#  [1]  crim    zn      rm      dis     rad     lstat   Here is another example using  multi-criteria tuning .\nWe consider  linear discriminant analysis  with precedent feature selection based on\nthe Chi-squared statistic of independence ( \"chi.squared\" ) on the  Sonar \ndata set and tune the threshold value.\nDuring tuning both, the false positive and the false negative rate ( fpr  and fnr ), are minimized. As search strategy we choose a random search\n(see  makeTuneMultiCritControlRandom ).  lrn = makeFilterWrapper(learner =  classif.lda , fw.method =  chi.squared )\nps = makeParamSet(makeNumericParam( fw.threshold , lower = 0.1, upper = 0.9))\nrdesc = makeResampleDesc( CV , iters = 10)\nres = tuneParamsMultiCrit(lrn, task = sonar.task, resampling = rdesc, par.set = ps,\n  measures = list(fpr, fnr), control = makeTuneMultiCritControlRandom(maxit = 50L),\n  show.info = FALSE)\nres\n#  Tune multicrit result:\n#  Points on front: 13\nhead(as.data.frame(res$opt.path))\n#    fw.threshold fpr.test.mean fnr.test.mean dob eol error.message exec.time\n#  1    0.4892321     0.3092818     0.2639033   1  NA           NA      3.142\n#  2    0.2481696     0.2045499     0.2319697   2  NA           NA      3.166\n#  3    0.7691875     0.5128000     0.3459740   3  NA           NA      2.923\n#  4    0.1470133     0.2045499     0.2319697   4  NA           NA      3.187\n#  5    0.5958241     0.5028216     0.5239538   5  NA           NA      2.797\n#  6    0.6892421     0.6323959     0.4480808   6  NA           NA      2.803  The results can be visualized with function  plotTuneMultiCritResult .\nThe plot shows the false positive and false negative rates for all parameter values visited\nduring tuning. The size of the points on the Pareto front is slightly increased.  plotTuneMultiCritResult(res)", 
            "title": "Filter methods"
        }, 
        {
            "location": "/feature_selection/index.html#wrapper-methods", 
            "text": "Wrapper methods use the performance of a learning algorithm to assess the usefulness of\na feature set.\nIn order to select a feature subset a learner is trained repeatedly on different feature subsets\nand the subset which leads to the best learner performance is chosen.  In order to use the wrapper approach we have to decide:   How to assess the performance: This involves choosing a performance measure that serves\n  as feature selection criterion and a resampling strategy.  Which learning method to use.  How to search the space of possible feature subsets.   The search strategy is defined by functions following the naming convention makeFeatSelControl search_strategy .\nThe following search strategies are available:   Exhaustive search ( makeFeatSelControlExhaustive ),  Genetic algorithm ( makeFeatSelControlGA ),  Random search ( makeFeatSelControlRandom ),  Deterministic forward or backward search ( makeFeatSelControlSequential ).   Select a feature subset  Feature selection can be conducted with function  selectFeatures .  In the following example we perform an exhaustive search on the Wisconsin Prognostic Breast Cancer  data set.\nAs learning method we use the  Cox proportional hazards model .\nThe performance is assessed by the holdout estimate of the concordance index\n( cindex ).  ## Specify the search strategy\nctrl = makeFeatSelControlRandom(maxit = 20L)\nctrl\n#  FeatSel control: FeatSelControlRandom\n#  Same resampling instance: TRUE\n#  Imputation value:  worst \n#  Max. features:  not used \n#  Max. iterations: 20\n#  Tune threshold: FALSE\n#  Further arguments: prob=0.5  ctrl  is a  FeatSelControl  object that contains information about the search strategy\nand potential parameter values.  ## Resample description\nrdesc = makeResampleDesc( Holdout )\n\n## Select features\nsfeats = selectFeatures(learner =  surv.coxph , task = wpbc.task, resampling = rdesc,\n  control = ctrl, show.info = FALSE)\nsfeats\n#  FeatSel result:\n#  Features (17): mean_radius, mean_area, mean_smoothness, mean_concavepoints, mean_symmetry, mean_fractaldim, SE_texture, SE_perimeter, SE_smoothness, SE_compactness, SE_concavity, SE_concavepoints, worst_area, worst_compactness, worst_concavepoints, tsize, pnodes\n#  cindex.test.mean=0.714  sfeats is a  FeatSelResult  object.\nThe selected features and the corresponding performance can be accessed as follows:  sfeats$x\n#   [1]  mean_radius           mean_area             mean_smoothness     \n#   [4]  mean_concavepoints    mean_symmetry         mean_fractaldim     \n#   [7]  SE_texture            SE_perimeter          SE_smoothness       \n#  [10]  SE_compactness        SE_concavity          SE_concavepoints    \n#  [13]  worst_area            worst_compactness     worst_concavepoints \n#  [16]  tsize                 pnodes \nsfeats$y\n#  cindex.test.mean \n#          0.713799  In a second example we fit a simple linear regression model to the  BostonHousing \ndata set and use a sequential search to find a feature set that minimizes the mean squared\nerror ( mse ). method = \"sfs\"  indicates that we want to conduct a sequential forward search where features\nare added to the model until the performance cannot be improved anymore.\nSee the documentation page  makeFeatSelControlSequential  for other available\nsequential search methods.\nThe search is stopped if the improvement is smaller than  alpha = 0.02 .  ## Specify the search strategy\nctrl = makeFeatSelControlSequential(method =  sfs , alpha = 0.02)\n\n## Select features\nrdesc = makeResampleDesc( CV , iters = 10)\nsfeats = selectFeatures(learner =  regr.lm , task = bh.task, resampling = rdesc, control = ctrl,\n  show.info = FALSE)\nsfeats\n#  FeatSel result:\n#  Features (11): crim, zn, chas, nox, rm, dis, rad, tax, ptratio, b, lstat\n#  mse.test.mean=23.7  Further information about the sequential feature selection process can be obtained by\nfunction  analyzeFeatSelResult .  analyzeFeatSelResult(sfeats)\n#  Features         : 11\n#  Performance      : mse.test.mean=23.7\n#  crim, zn, chas, nox, rm, dis, rad, tax, ptratio, b, lstat\n#  \n#  Path to optimum:\n#  - Features:    0  Init   :                       Perf = 84.831  Diff: NA  *\n#  - Features:    1  Add    : lstat                 Perf = 38.894  Diff: 45.936  *\n#  - Features:    2  Add    : rm                    Perf = 31.279  Diff: 7.6156  *\n#  - Features:    3  Add    : ptratio               Perf = 28.108  Diff: 3.1703  *\n#  - Features:    4  Add    : dis                   Perf = 27.48  Diff: 0.62813  *\n#  - Features:    5  Add    : nox                   Perf = 26.079  Diff: 1.4008  *\n#  - Features:    6  Add    : b                     Perf = 25.563  Diff: 0.51594  *\n#  - Features:    7  Add    : chas                  Perf = 25.132  Diff: 0.43097  *\n#  - Features:    8  Add    : zn                    Perf = 24.792  Diff: 0.34018  *\n#  - Features:    9  Add    : rad                   Perf = 24.599  Diff: 0.19327  *\n#  - Features:   10  Add    : tax                   Perf = 24.082  Diff: 0.51706  *\n#  - Features:   11  Add    : crim                  Perf = 23.732  Diff: 0.35  *\n#  \n#  Stopped, because no improving feature was found.  Fuse a learner with feature selection  A  Learner  can be fused with a feature selection strategy (i.e., a search\nstrategy, a performance measure and a resampling strategy) by function  makeFeatSelWrapper .\nDuring training features are selected according to the specified selection scheme. Then, the\nlearner is trained on the selected feature subset.  rdesc = makeResampleDesc( CV , iters = 3)\nlrn = makeFeatSelWrapper( surv.coxph , resampling = rdesc,\n  control = makeFeatSelControlRandom(maxit = 10), show.info = FALSE)\nmod = train(lrn, task = wpbc.task)\nmod\n#  Model for learner.id=surv.coxph.featsel; learner.class=FeatSelWrapper\n#  Trained on: task.id = wpbc-example; obs = 194; features = 32\n#  Hyperparameters:  The result of the feature selection can be extracted by function  getFeatSelResult .  sfeats = getFeatSelResult(mod)\nsfeats\n#  FeatSel result:\n#  Features (19): mean_radius, mean_texture, mean_perimeter, mean_area, mean_smoothness, mean_compactness, mean_concavepoints, mean_fractaldim, SE_compactness, SE_concavity, SE_concavepoints, SE_symmetry, worst_texture, worst_perimeter, worst_area, worst_concavepoints, worst_symmetry, tsize, pnodes\n#  cindex.test.mean=0.631  The selected features are:  sfeats$x\n#   [1]  mean_radius           mean_texture          mean_perimeter      \n#   [4]  mean_area             mean_smoothness       mean_compactness    \n#   [7]  mean_concavepoints    mean_fractaldim       SE_compactness      \n#  [10]  SE_concavity          SE_concavepoints      SE_symmetry         \n#  [13]  worst_texture         worst_perimeter       worst_area          \n#  [16]  worst_concavepoints   worst_symmetry        tsize               \n#  [19]  pnodes   The 5-fold cross-validated performance of the learner specified above can be computed as\nfollows:  out.rdesc = makeResampleDesc( CV , iters = 5)\n\nr = resample(learner = lrn, task = wpbc.task, resampling = out.rdesc, models = TRUE,\n  show.info = FALSE)\nr$aggr\n#  cindex.test.mean \n#          0.632357  The selected feature sets in the individual resampling iterations can be extracted as follows:  lapply(r$models, getFeatSelResult)\n#  [[1]]\n#  FeatSel result:\n#  Features (18): mean_texture, mean_area, mean_smoothness, mean_compactness, mean_concavity, mean_symmetry, SE_radius, SE_compactness, SE_concavity, SE_concavepoints, SE_fractaldim, worst_radius, worst_smoothness, worst_compactness, worst_concavity, worst_symmetry, tsize, pnodes\n#  cindex.test.mean=0.66\n#  \n#  [[2]]\n#  FeatSel result:\n#  Features (12): mean_area, mean_compactness, mean_symmetry, mean_fractaldim, SE_perimeter, SE_area, SE_concavity, SE_symmetry, worst_texture, worst_smoothness, worst_fractaldim, tsize\n#  cindex.test.mean=0.652\n#  \n#  [[3]]\n#  FeatSel result:\n#  Features (14): mean_compactness, mean_symmetry, mean_fractaldim, SE_radius, SE_perimeter, SE_smoothness, SE_concavity, SE_concavepoints, SE_fractaldim, worst_concavity, worst_concavepoints, worst_symmetry, worst_fractaldim, pnodes\n#  cindex.test.mean=0.607\n#  \n#  [[4]]\n#  FeatSel result:\n#  Features (18): mean_radius, mean_texture, mean_perimeter, mean_compactness, mean_concavity, SE_texture, SE_area, SE_smoothness, SE_concavity, SE_symmetry, SE_fractaldim, worst_radius, worst_compactness, worst_concavepoints, worst_symmetry, worst_fractaldim, tsize, pnodes\n#  cindex.test.mean=0.653\n#  \n#  [[5]]\n#  FeatSel result:\n#  Features (14): mean_radius, mean_texture, mean_compactness, mean_concavepoints, mean_symmetry, SE_texture, SE_compactness, SE_symmetry, SE_fractaldim, worst_radius, worst_smoothness, worst_compactness, worst_concavity, pnodes\n#  cindex.test.mean=0.626", 
            "title": "Wrapper methods"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html", 
            "text": "Cost-Sensitive Classification\n\n\nIn \nregular classification\n the aim is to minimize the misclassification rate and\nthus all types of misclassification errors are deemed equally severe.\nA more general setting is \ncost-sensitive classification\n where the costs caused by different\nkinds of errors are not assumed to be equal and the objective is to minimize the expected costs.\n\n\nIn case of \nclass-dependent costs\n the costs depend on the true and predicted class label.\nThe costs \nc\n(\nk\n, \nl\n) for predicting class \nk\n if the true label is \nl\n are usually organized\ninto a \nK\n times \nK\n cost matrix where \nK\n is the number of classes.\nNaturally, it is assumed that the cost of predicting the correct class label \ny\n is minimal\n(that is \nc\n(\ny\n, \ny\n) is smaller or equal to \nc\n(\nk\n, \ny\n) for all \nk\n).\n\n\nA further generalization of this scenario are \nexample-dependent misclassification costs\n where\neach example (\nx\n, \ny\n) is coupled with an individual cost vector of length \nK\n. Its \nk\n-th\ncomponent expresses the cost of assigning \nx\n to class \nk\n.\nA real-world example is fraud detection where the costs do not only depend on the true and\npredicted status fraud/non-fraud, but also on the amount of money involved in each case.\nNaturally, the cost of predicting the true class label \ny\n is assumed to be minimum.\nThe true class labels are redundant information, as they can be easily inferred from the\ncost vectors.\nMoreover, given the cost vector, the expected costs do not depend on the true class label \ny\n.\nThe classification problem is therefore completely defined by the feature values \nx\n and the\ncorresponding cost vectors.\n\n\nIn the following we show ways to handle cost-sensitive classification problems in \nmlr\n.\nSome of the functionality is currently experimental, and there may be changes in the future.\n\n\nClass-dependent misclassification costs\n\n\nThere are some classification methods that can accomodate misclassification costs\ndirectly.\nOne example is \nrpart\n.\n\n\nAlternatively, we can use cost-insensitive methods and manipulate the predictions or the\ntraining data in order to take misclassification costs into account.\n\nmlr\n supports \nthresholding\n and \nrebalancing\n.\n\n\n\n\n\n\nThresholding\n:\n  The thresholds used to turn posterior probabilities into class labels are chosen such that\n  the costs are minimized.\n  This requires a \nLearner\n that can predict posterior probabilities.\n  During training the costs are not taken into account.\n\n\n\n\n\n\nRebalancing\n:\n  The idea is to change the proportion of the classes in the training data set in order to\n  account for costs during training, either by \nweighting\n or by \nsampling\n.\n  Rebalancing does not require that the \nLearner\n can predict probabilities.\n\n\ni. For \nweighting\n we need a \nLearner\n that supports class weights or observation\n     weights.\n\n\nii. If the \nLearner\n cannot deal with weights the proportion of classes can\n     be changed by \nover-\n and \nundersampling\n.\n\n\n\n\n\n\nWe start with binary classification problems and afterwards deal with multi-class problems.\n\n\nBinary classification problems\n\n\nThe positive and negative classes are labeled 1 and -1, respectively, and we consider the\nfollowing cost matrix where the rows indicate true classes and the columns predicted classes:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrue\\pred.\n\n\n1\n\n\n-1\n\n\n\n\n\n\n1\n\n\nc\n(1,1)\n\n\nc\n(-1,1)\n\n\n\n\n\n\n-1\n\n\nc\n(1,-1)\n\n\nc\n(-1,-1)\n\n\n\n\n\n\n\n\nOften, the diagonal entries are zero or the cost matrix is rescaled to achieve zeros in the diagonal\n(see for example \nO'Brien et al, 2008\n).\n\n\nA well-known cost-sensitive classification problem is posed by the\n\nGerman Credit data set\n\n(see also the \nUCI Machine Learning Repository\n).\nThe corresponding cost matrix (though \nElkan (2001)\n\nargues that this matrix is economically unreasonable) is given as:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrue\\pred.\n\n\nBad\n\n\nGood\n\n\n\n\n\n\nBad\n\n\n0\n\n\n5\n\n\n\n\n\n\nGood\n\n\n1\n\n\n0\n\n\n\n\n\n\n\n\nAs in the table above, the rows indicate true and the columns predicted classes.\n\n\nFor class-dependent costs it is sufficient to generate an ordinary \nClassifTask\n.\nA \nCostSensTask\n is needed in case of example-dependent costs.\nIn the following we create the \nClassifTask\n, remove two constant features from the\ndata set and generate the cost matrix.\nPer default, Bad is the positive class.\n\n\ndata(GermanCredit, package = \ncaret\n)\ncredit.task = makeClassifTask(data = GermanCredit, target = \nClass\n)\ncredit.task = removeConstantFeatures(credit.task)\n#\n Removing 2 columns: Purpose.Vacation,Personal.Female.Single\ncredit.task\n#\n Supervised task: GermanCredit\n#\n Type: classif\n#\n Target: Class\n#\n Observations: 1000\n#\n Features:\n#\n numerics  factors  ordered \n#\n       59        0        0 \n#\n Missings: FALSE\n#\n Has weights: FALSE\n#\n Has blocking: FALSE\n#\n Classes: 2\n#\n  Bad Good \n#\n  300  700 \n#\n Positive class: Bad\n\ncosts = matrix(c(0, 1, 5, 0), 2)\ncolnames(costs) = rownames(costs) = getTaskDescription(credit.task)$class.levels\ncosts\n#\n      Bad Good\n#\n Bad    0    5\n#\n Good   1    0\n\n\n\n\n1. Thresholding\n\n\nWe start by fitting a \nlogistic regression model\n to the\n\nGerman credit data set\n and predicting posterior probabilities.\n\n\n## Train and predict posterior probabilities\nlrn = makeLearner(\nclassif.multinom\n, predict.type = \nprob\n, trace = FALSE)\nmod = train(lrn, credit.task)\npred = predict(mod, task = credit.task)\npred\n#\n Prediction: 1000 observations\n#\n predict.type: prob\n#\n threshold: Bad=0.50,Good=0.50\n#\n time: 0.02\n#\n   id truth   prob.Bad prob.Good response\n#\n 1  1  Good 0.03525092 0.9647491     Good\n#\n 2  2   Bad 0.63222363 0.3677764      Bad\n#\n 3  3  Good 0.02807414 0.9719259     Good\n#\n 4  4  Good 0.25182703 0.7481730     Good\n#\n 5  5   Bad 0.75193275 0.2480673      Bad\n#\n 6  6  Good 0.26230149 0.7376985     Good\n\n\n\n\nThe default thresholds for both classes are 0.5.\nBut according to the cost matrix we should predict class Good only if we are very sure that Good\nis the correct label and therefore increase the threshold for class Good and decrease the\nthreshold for class Bad.\n\n\ni. Theoretical thresholding\n\n\nThe theoretical threshold for the \npositive\n class can be calculated from the cost matrix as\n\n\nFor more details see \nElkan (2001)\n.\n\n\nBelow the theoretical threshold for the \nGerman credit example\n\nis calculated and used to predict class labels.\nSince the diagonal of the cost matrix is zero the formula given above simplifies accordingly.\n\n\n## Calculate the theoretical threshold for the positive class\nth = costs[2,1]/(costs[2,1] + costs[1,2])\nth\n#\n [1] 0.1666667\n\n\n\n\nAs you may recall you can change thresholds in \nmlr\n either before training by using the\n\npredict.threshold\n option of \nmakeLearner\n or after prediction by calling \nsetThreshold\n\non the \nPrediction\n object.\n\n\nAs we already have a prediction we use the \nsetThreshold\n function. It returns an altered\n\nPrediction\n object with class predictions for the theoretical threshold.\n\n\n## Predict class labels according to the theoretical threshold\npred.th = setThreshold(pred, th)\npred.th\n#\n Prediction: 1000 observations\n#\n predict.type: prob\n#\n threshold: Bad=0.17,Good=0.83\n#\n time: 0.02\n#\n   id truth   prob.Bad prob.Good response\n#\n 1  1  Good 0.03525092 0.9647491     Good\n#\n 2  2   Bad 0.63222363 0.3677764      Bad\n#\n 3  3  Good 0.02807414 0.9719259     Good\n#\n 4  4  Good 0.25182703 0.7481730      Bad\n#\n 5  5   Bad 0.75193275 0.2480673      Bad\n#\n 6  6  Good 0.26230149 0.7376985      Bad\n\n\n\n\nIn order to calculate the average costs over the entire data set we first need to create a new\nperformance \nMeasure\n. This can be done through function \nmakeCostMeasure\n\nwhich requires the \nClassifTask\n object and the cost matrix (argument \ncosts\n).\nIt is expected that the rows of the cost matrix indicate true and the columns predicted\nclass labels.\n\n\ncredit.costs = makeCostMeasure(id = \ncredit.costs\n, costs = costs, task = credit.task, best = 0, worst = 5)\ncredit.costs\n#\n Name: credit.costs\n#\n Performance measure: credit.costs\n#\n Properties: classif,classif.multi,req.pred,req.truth,predtype.response,predtype.prob\n#\n Minimize: TRUE\n#\n Best: 0; Worst: 5\n#\n Aggregated by: test.mean\n#\n Note:\n\n\n\n\nThen the average costs can be computed by function \nperformance\n.\nBelow we compare the average costs and the error rate (\nmmce\n) of the learning algorithm\nwith both default thresholds 0.5 and theoretical thresholds.\n\n\n## Performance with default thresholds 0.5\nperformance(pred, measures = list(credit.costs, mmce))\n#\n credit.costs         mmce \n#\n        0.774        0.214\n\n## Performance with theoretical thresholds\nperformance(pred.th, measures = list(credit.costs, mmce))\n#\n credit.costs         mmce \n#\n        0.478        0.346\n\n\n\n\nThese performance values may be overly optimistic as we used the same data set for training\nand prediction, and resampling strategies should be preferred.\nIn the R-code below we make use of the \npredict.threshold\n argument of \nmakeLearner\n to set\nthe threshold before doing 3-fold cross-validation.\nNote that we create a \nResampleInstance\n (\nrin\n) for the\n\nGerman credit data\n that is used throughout the next several code\nexamples to get comparable performance values.\n\n\n## Cross-validated performance with theoretical thresholds\nrin = makeResampleInstance(\nCV\n, iters = 3, task = credit.task)\nlrn = makeLearner(\nclassif.multinom\n, predict.type = \nprob\n, predict.threshold = th, trace = FALSE)\nr = resample(lrn, credit.task, resampling = rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: GermanCredit\n#\n Learner: classif.multinom\n#\n credit.costs.aggr: 0.56\n#\n credit.costs.mean: 0.56\n#\n credit.costs.sd: 0.03\n#\n mmce.aggr: 0.36\n#\n mmce.mean: 0.36\n#\n mmce.sd: 0.02\n#\n Runtime: 0.338198\n\n\n\n\nIf we are also interested in the cross-validated performance for the default threshold values\nwe can call \nsetThreshold\n on the \nresampled prediction\n \nr$pred\n.\n\n\n## Cross-validated performance with default thresholds\nperformance(setThreshold(r$pred, 0.5), measures = list(credit.costs, mmce))\n#\n credit.costs         mmce \n#\n        0.852        0.248\n\n\n\n\nTheoretical thresholding is only reliable if the predicted posterior probabilities are correct.\nIf there are systematic errors the thresholds have to be shifted accordingly.\n\n\nUseful in this regard is function \nplotThreshVsPerf\n that permits to plot the average costs\nas well as any other performance measure versus possible threshold values for the positive\nclass in [0,1]. The underlying data is generated from \ngenerateThreshVsPerfData\n.\n\n\nThe following plot show the cross-validated costs and error rate (\nmmce\n).\nThe theoretical threshold \nth\n calculated above is indicated by the vertical line.\nAs you can see the theoretical threshold seems a bit large.\n\n\nd = generateThreshVsPerfData(r, measures = list(credit.costs, mmce))\nplotThreshVsPerf(d, mark.th = th)\n\n\n\n\n \n\n\nii. Empirical thresholding\n\n\nIn \nempirical thresholding\n (see \nSheng and Ling (2006)\n)\ncost-optimal threshold values for a given learning method are selected based on the training data.\nIn contrast to \ntheoretical thresholding\n it suffices if the estimated posterior probabilities\nare order-correct.\n\n\nIn order to determine optimal threshold values you can use \nmlr\n's function \ntuneThreshold\n.\nAs tuning the threshold on the complete training data set can lead to overfitting, resampling\nstrategies should be used.\nBelow we perform 3-fold cross-validation and use \ntuneThreshold\n to calculate threshold values\nwith lowest average costs over the 3 test data sets.\n\n\nlrn = makeLearner(\nclassif.multinom\n, predict.type = \nprob\n, trace = FALSE)\n\n## 3-fold cross-validation\nr = resample(lrn, credit.task, resampling = rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: GermanCredit\n#\n Learner: classif.multinom\n#\n credit.costs.aggr: 0.85\n#\n credit.costs.mean: 0.85\n#\n credit.costs.sd: 0.17\n#\n mmce.aggr: 0.25\n#\n mmce.mean: 0.25\n#\n mmce.sd: 0.03\n#\n Runtime: 0.335323\n\n## Tune the threshold based on the predicted probabilities on the 3 test data sets\ntune.res = tuneThreshold(pred = r$pred, measure = credit.costs)\ntune.res\n#\n $th\n#\n [1] 0.1110669\n#\n \n#\n $perf\n#\n credit.costs \n#\n        0.508\n\n\n\n\ntuneThreshold\n returns the optimal threshold value for the positive class and the corresponding\nperformance.\nAs expected the tuned threshold is smaller than the theoretical threshold.\n\n\n2. Rebalancing\n\n\nIn order to minimize the average costs, observations from the less costly class should be\ngiven higher importance during training.\nThis can be achieved by \nweighting\n the classes if the classification method under consideration\naccepts class or observations weights.\nAlternatively, \nover- and undersampling\n techniques can be used.\n\n\ni. Weighting\n\n\nSimilar to \ntheoretical thresholding\n, \ntheoretical weights\n can be calculated from the\ncost matrix.\nIf \nt\n indicates the target threshold and \nt\n0\n the original threshold for the positive class the\nproportion of observations in the positive class has to be multiplied by\n\n\nAlternatively, the proportion of observations in the negative class can be multiplied by\nthe inverse.\nA proof is given by \nElkan (2001)\n.\n\n\nIn most cases, the original threshold \nt\n0\n is 0.5 and thus the second factor vanishes.\nIf additionally the target threshold \nt\n equals the theoretical threshold \nt\n*\n the\nproportion of observations in the positive class has to be multiplied by\n\n\n\nFunction \nmakeWeightedClassesWrapper\n allows to assign class weights to \nLearner\ns.\nNaturally, this is only possible for methods with either a 'class weights' or an 'observation weights' argument.\n(Have a look at the \ntable of integrated learners\n to see which methods\nsupport observation weights.)\n\n\nFunction \nmultinom\n accepts observation weights.\nThe weight given to all observations from the positive class is passed via argument \nwcw.weight\n.\nObservations from the negative class automatically receive weight 1.\n\n\n## Weight corresponding to theoretical treshold\nw = (1 - th)/th\nw\n#\n [1] 5\n\n## Weighted learner\nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE)\nlrn = makeWeightedClassesWrapper(lrn, wcw.weight = w)\nlrn\n#\n Learner weightedclasses.classif.multinom from package nnet\n#\n Type: classif\n#\n Name: ; Short name: \n#\n Class: WeightedClassesWrapper\n#\n Properties: numerics,factors,prob,twoclass,multiclass\n#\n Predict-Type: response\n#\n Hyperparameters: trace=FALSE,wcw.weight=5\n\n\n\n\nThe theoretical threshold corresponds to a weight of 5 for the positive class.\nBelow the wrapped learner is used to get predictions. Moreover, its cross-validated performance is\ncalculated.\n\n\nmod = train(lrn, credit.task)\npred = predict(mod, task = credit.task)\npred\n#\n Prediction: 1000 observations\n#\n predict.type: response\n#\n threshold: \n#\n time: 0.03\n#\n   id truth response\n#\n 1  1  Good     Good\n#\n 2  2   Bad      Bad\n#\n 3  3  Good     Good\n#\n 4  4  Good      Bad\n#\n 5  5   Bad      Bad\n#\n 6  6  Good      Bad\n\nr = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: GermanCredit\n#\n Learner: weightedclasses.classif.multinom\n#\n credit.costs.aggr: 0.53\n#\n credit.costs.mean: 0.53\n#\n credit.costs.sd: 0.04\n#\n mmce.aggr: 0.35\n#\n mmce.mean: 0.35\n#\n mmce.sd: 0.02\n#\n Runtime: 0.391155\n\n\n\n\nSome classification methods as the support vector machine (\nksvm\n) in package \nkernlab\n\nsupport class weights.\nWhen generating the wrapped \nLearner\n you have to pass the name of the relevant\nlearner parameter (\n\"class.weights\"\n for \nksvm\n) via argument \nwcw.param\n\nin addition to the weight.\n\n\nlrn = makeWeightedClassesWrapper(\nclassif.ksvm\n, wcw.param = \nclass.weights\n, wcw.weight = w)\nmod = train(lrn, credit.task)\npred = predict(mod, task = credit.task)\npred\n#\n Prediction: 1000 observations\n#\n predict.type: response\n#\n threshold: \n#\n time: 0.17\n#\n   id truth response\n#\n 1  1  Good     Good\n#\n 2  2   Bad      Bad\n#\n 3  3  Good     Good\n#\n 4  4  Good      Bad\n#\n 5  5   Bad      Bad\n#\n 6  6  Good     Good\n\nr = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: GermanCredit\n#\n Learner: weightedclasses.classif.ksvm\n#\n credit.costs.aggr: 0.58\n#\n credit.costs.mean: 0.58\n#\n credit.costs.sd: 0.04\n#\n mmce.aggr: 0.31\n#\n mmce.mean: 0.31\n#\n mmce.sd: 0.02\n#\n Runtime: 0.813984\n\n\n\n\nJust like the theoretical threshold, the theoretical weights may not always be suitable,\ntherefore you can tune the weight for the positive class as shown in the following example.\nCalculating the theoretical weight beforehand may help to narrow down the search interval.\n\n\nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE)\nlrn = makeWeightedClassesWrapper(lrn)\nps = makeParamSet(makeDiscreteParam(\nwcw.weight\n, seq(4, 12, 0.5)))\nctrl = makeTuneControlGrid()\ntune.res = tuneParams(lrn, credit.task, resampling = rin, par.set = ps,\n  measures = list(credit.costs, mmce), control = ctrl, show.info = FALSE)\ntune.res\n#\n Tune result:\n#\n Op. pars: wcw.weight=7.5\n#\n credit.costs.test.mean=0.501,mmce.test.mean=0.381\nas.data.frame(tune.res$opt.path)[1:3]\n#\n    wcw.weight credit.costs.test.mean mmce.test.mean\n#\n 1           4              0.5650291      0.3330127\n#\n 2         4.5              0.5550251      0.3430167\n#\n 3           5              0.5260320      0.3460197\n#\n 4         5.5              0.5130070      0.3530147\n#\n 5           6              0.5160100      0.3640137\n#\n 6         6.5              0.5160160      0.3720157\n#\n 7           7              0.5040250      0.3760167\n#\n 8         7.5              0.5010040      0.3810038\n#\n 9           8              0.5100130      0.3900128\n#\n 10        8.5              0.5100070      0.3940108\n#\n 11          9              0.5110080      0.4030078\n#\n 12        9.5              0.5160130      0.4080128\n#\n 13         10              0.5260140      0.4180138\n#\n 14       10.5              0.5240060      0.4200098\n#\n 15         11              0.5319991      0.4280029\n#\n 16       11.5              0.5289901      0.4330019\n#\n 17         12              0.5249801      0.4369999\n\n\n\n\nii. Over- and undersampling\n\n\nIf the \nLearner\n supports neither observation nor class weights the proportions\nof the classes in the training data can be changed by over- or undersampling.\n\n\nIn the \nGermanCredit data set\n the positive class Bad should receive\na theoretical weight of about \nw = (1 - th)/th = 5\n.\nThis can be achieved by oversampling class Bad with a \nrate\n of 5 (see also the documentation\npage of function \noversample\n).\n\n\ncredit.task.over = oversample(credit.task, rate = w)\nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE)\nmod = train(lrn, credit.task.over)\npred = predict(mod, task = credit.task)\nperformance(pred, measures = list(credit.costs, mmce))\n#\n credit.costs         mmce \n#\n        0.440        0.328\n\n\n\n\nNote that in the example above the learner was trained on the oversampled task \ncredit.task.over\n.\nIn order to get the training performance on the original task predictions were calculated for \ncredit.task\n.\n\n\nWe usually prefer resampled performance values, but simply calling \nresample\n on the oversampled\ntask does not work since predictions have to be done for the original task.\nThe solution is to create a wrapped \nLearner\n via function\n\nmakeOversampleWrapper\n.\nInternally, \noversample\n is called before training, but predictions are done on the original task.\n\n\nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE)\nlrn = makeOversampleWrapper(lrn, osw.rate = w)\nlrn\n#\n Learner classif.multinom.oversampled from package mlr,nnet\n#\n Type: classif\n#\n Name: ; Short name: \n#\n Class: OversampleWrapper\n#\n Properties: numerics,factors,weights,prob,twoclass,multiclass\n#\n Predict-Type: response\n#\n Hyperparameters: trace=FALSE,osw.rate=5\n\nr = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: GermanCredit\n#\n Learner: classif.multinom.oversampled\n#\n credit.costs.aggr: 0.56\n#\n credit.costs.mean: 0.56\n#\n credit.costs.sd: 0.05\n#\n mmce.aggr: 0.35\n#\n mmce.mean: 0.35\n#\n mmce.sd: 0.02\n#\n Runtime: 0.659542\n\n\n\n\nOf course, we can also tune the oversampling rate.\nFor this purpose we again have to create an \nOversampleWrapper\n.\nOptimal values for parameter \nosw.rate\n can be obtained using function \ntuneParams\n.\n\n\nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE)\nlrn = makeOversampleWrapper(lrn)\nps = makeParamSet(makeDiscreteParam(\nosw.rate\n, seq(3, 7, 0.25)))\nctrl = makeTuneControlGrid()\ntune.res = tuneParams(lrn, credit.task, rin, par.set = ps, measures = list(credit.costs, mmce),\n  control = ctrl, show.info = FALSE)\ntune.res\n#\n Tune result:\n#\n Op. pars: osw.rate=6\n#\n credit.costs.test.mean=0.496,mmce.test.mean=0.352\n\n\n\n\nMulti-class problems\n\n\nWe consider the \nwaveform\n data set from package \nmlbench\n and\nadd an artificial cost matrix:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrue\\pred.\n\n\n1\n\n\n2\n\n\n3\n\n\n\n\n\n\n1\n\n\n0\n\n\n30\n\n\n80\n\n\n\n\n\n\n2\n\n\n5\n\n\n0\n\n\n4\n\n\n\n\n\n\n3\n\n\n10\n\n\n8\n\n\n0\n\n\n\n\n\n\n\n\nWe start by creating the \nTask\n, the cost matrix and the corresponding performance measure.\n\n\n## Task\ndf = mlbench::mlbench.waveform(500)\nwf.task = makeClassifTask(id = \nwaveform\n, data = as.data.frame(df), target = \nclasses\n)\n\n## Cost matrix\ncosts = matrix(c(0, 5, 10, 30, 0, 8, 80, 4, 0), 3)\ncolnames(costs) = rownames(costs) = getTaskDescription(wf.task)$class.levels\n\n## Performance measure\nwf.costs = makeCostMeasure(id = \nwf.costs\n, costs = costs, task = wf.task, best = 0,\n  worst = 10)\n\n\n\n\nIn the multi-class case, both, \nthresholding\n and \nrebalancing\n correspond to cost matrices\nof a certain structure where \nc\n(\nk\n,\nl\n) = \nc\n(\nl\n) for \nk\n, \nl\n = 1, ..., \nK\n, \nk\n \n \nl\n.\nThis condition means that the cost of misclassifying an observation is independent of the\npredicted class label\n(see \nDomingos (1999)\n).\nGiven a cost matrix of this type, theoretical thresholds and weights can be derived\nin a similar manner as in the binary case.\nObviously, the cost matrix given above does not have this special structure.\n\n\n1. Thresholding\n\n\nGiven a vector of positive threshold values as long as the number of classes \nK\n, the predicted\nprobabilities for all classes are adjusted by dividing them by the corresponding threshold value.\nThen the class with the highest adjusted probability is predicted.\nThis way, as in the binary case, classes with a low threshold are predicted more easily than\nclasses with a larger threshold.\n\n\nAgain this can be done by function \nsetThreshold\n as shown in the following example (or\nalternatively by the \npredict.threshold\n option of \nmakeLearner\n).\nNote that the threshold vector needs to have names that correspond to the class labels.\n\n\nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n)\nrin = makeResampleInstance(\nCV\n, iters = 3, task = wf.task)\nr = resample(lrn, wf.task, rin, measures = list(wf.costs, mmce), show.info = FALSE)\nr\n#\n Resample Result\n#\n Task: waveform\n#\n Learner: classif.rpart\n#\n wf.costs.aggr: 8.26\n#\n wf.costs.mean: 8.26\n#\n wf.costs.sd: 2.85\n#\n mmce.aggr: 0.34\n#\n mmce.mean: 0.34\n#\n mmce.sd: 0.04\n#\n Runtime: 0.162872\n\n## Calculate thresholds as 1/(average costs of true classes)\nth = 2/rowSums(costs)\nnames(th) = getTaskDescription(wf.task)$class.levels\nth\n#\n          1          2          3 \n#\n 0.01818182 0.22222222 0.11111111\n\npred.th = setThreshold(r$pred, threshold = th)\nperformance(pred.th, measures = list(wf.costs, mmce))\n#\n wf.costs     mmce \n#\n    5.668    0.480\n\n\n\n\nThe threshold vector \nth\n in the above example is chosen according to the average costs\nof the true classes 55, 4.5 and 9.\nMore exactly, \nth\n corresponds to an artificial cost matrix of the structure mentioned\nabove with off-diagonal elements \nc\n(2,1) = \nc\n(3,1) = 55, \nc\n(1,2) = \nc\n(3,2) = 4.5 and\n\nc\n(1,3) = \nc\n(2,3) = 9.\nThis threshold vector may be not optimal but leads to smaller total costs on the data set than\nthe default.\n\n\nii. Empirical thresholding\n\n\nAs in the binary case it is possible to tune the threshold vector using function \ntuneThreshold\n.\nSince the scaling of the threshold vector does not change the predicted class labels\n\ntuneThreshold\n returns threshold values that lie in [0,1] and sum to unity.\n\n\ntune.res = tuneThreshold(pred = r$pred, measure = wf.costs)\ntune.res\n#\n $th\n#\n             1             2             3 \n#\n -1.657070e-09  3.914654e-01  6.085346e-01 \n#\n \n#\n $perf\n#\n [1] 5.05\n\n\n\n\nFor comparison we show the standardized version of the theoretically motivated threshold\nvector chosen above.\n\n\nth/sum(th)\n#\n          1          2          3 \n#\n 0.05172414 0.63218391 0.31609195\n\n\n\n\n2. Rebalancing\n\n\ni. Weighting\n\n\nIn the multi-class case you have to pass a vector of weights as long as the number of classes\n\nK\n to function \nmakeWeightedClassesWrapper\n.\nThe weight vector can be tuned using function \ntuneParams\n.\n\n\nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE)\nlrn = makeWeightedClassesWrapper(lrn)\n\nps = makeParamSet(makeNumericVectorParam(\nwcw.weight\n, len = 3, lower = 0, upper = 1))\nctrl = makeTuneControlRandom()\n\ntune.res = tuneParams(lrn, wf.task, resampling = rin, par.set = ps,\n  measures = list(wf.costs, mmce), control = ctrl, show.info = FALSE)\ntune.res\n#\n Tune result:\n#\n Op. pars: wcw.weight=0.673,0.12,0.0204\n#\n wf.costs.test.mean=2.63,mmce.test.mean=0.222\n\n\n\n\nExample-dependent misclassification costs\n\n\nIn case of example-dependent costs we have to create a special \nTask\n via function\n\nmakeCostSensTask\n.\nFor this purpose the feature values \nx\n and an \nn\n times \nK\n \ncost\n matrix that contains\nthe cost vectors for all \nn\n examples in the data set are required.\n\n\nWe use the \niris\n data and generate an artificial cost matrix.\n\n\ndf = iris\ncost = matrix(runif(150 * 3, 0, 2000), 150) * (1 - diag(3))[df$Species,] + runif(150, 0, 10)\ncolnames(cost) = levels(iris$Species)\nrownames(cost) = rownames(iris)\ndf$Species = NULL\n\ncostsens.task = makeCostSensTask(id = \niris\n, data = df, cost = cost)\ncostsens.task\n#\n Supervised task: iris\n#\n Type: costsens\n#\n Observations: 150\n#\n Features:\n#\n numerics  factors  ordered \n#\n        4        0        0 \n#\n Missings: FALSE\n#\n Has blocking: FALSE\n#\n Classes: 3\n#\n setosa, versicolor, virginica\n\n\n\n\nmlr\n provides several \nwrappers\n to turn regular classification or regression methods\ninto \nLearner\ns that can deal with example-dependent costs.\n\n\n\n\nmakeCostSensClassifWrapper\n (wraps a classification \nLearner\n):\n  This is a naive approach where the costs are coerced into class labels by choosing the\n  class label with minimum cost for each example. Then a regular classification method is\n  used.\n\n\nmakeCostSensRegrWrapper\n (wraps a regression \nLearner\n):\n  An individual regression model is fitted for the costs of each class.\n  In the prediction step first the costs are predicted for all classes and then the class with\n  the lowest predicted costs is selected.\n\n\nmakeCostSensWeightedPairsWrapper\n (wraps a classification \nLearner\n):\n  This is also known as \ncost-sensitive one-vs-one\n (CS-OVO) and the most sophisticated of\n  the currently supported methods.\n  For each pair of classes, a binary classifier is fitted.\n  For each observation the class label is defined as the element of the pair with minimal costs.\n  During fitting, the observations are weighted with the absolute difference in costs.\n  Prediction is performed by simple voting.\n\n\n\n\nIn the following example we use the third method. We create the wrapped \nLearner\n\nand train it on the \nCostSensTask\n defined above.\n\n\nlrn = makeLearner(\nclassif.multinom\n, trace = FALSE)\nlrn = makeCostSensWeightedPairsWrapper(lrn)\nlrn\n#\n Learner costsens.classif.multinom from package nnet\n#\n Type: costsens\n#\n Name: ; Short name: \n#\n Class: CostSensWeightedPairsWrapper\n#\n Properties: numerics,factors,twoclass,multiclass\n#\n Predict-Type: response\n#\n Hyperparameters: trace=FALSE\n\nmod = train(lrn, costsens.task)\nmod\n#\n Model for learner.id=costsens.classif.multinom; learner.class=CostSensWeightedPairsWrapper\n#\n Trained on: task.id = iris; obs = 150; features = 4\n#\n Hyperparameters: trace=FALSE\n\n\n\n\nThe models corresponding to the individual pairs can be accessed by function\n\ngetHomogeneousEnsembleModels\n.\n\n\ngetHomogeneousEnsembleModels(mod)\n#\n [[1]]\n#\n Model for learner.id=classif.multinom; learner.class=classif.multinom\n#\n Trained on: task.id = feats; obs = 150; features = 4\n#\n Hyperparameters: trace=FALSE\n#\n \n#\n [[2]]\n#\n Model for learner.id=classif.multinom; learner.class=classif.multinom\n#\n Trained on: task.id = feats; obs = 150; features = 4\n#\n Hyperparameters: trace=FALSE\n#\n \n#\n [[3]]\n#\n Model for learner.id=classif.multinom; learner.class=classif.multinom\n#\n Trained on: task.id = feats; obs = 150; features = 4\n#\n Hyperparameters: trace=FALSE\n\n\n\n\nmlr\n provides some performance measures for example-specific cost-sensitive classification.\nIn the following example we calculate the mean costs of the predicted class labels\n(\nmeancosts\n) and the misclassification penalty (\nmcp\n).\nThe latter measure is the average difference between the costs caused by the predicted\nclass labels, i.e., \nmeancosts\n, and the costs resulting from choosing the\nclass with lowest cost for each observation.\nIn order to compute these measures the costs for the test observations are required and\ntherefore the \nTask\n has to be passed to \nperformance\n.\n\n\npred = predict(mod, task = costsens.task)\npred\n#\n Prediction: 150 observations\n#\n predict.type: response\n#\n threshold: \n#\n time: 0.13\n#\n   id response\n#\n 1  1   setosa\n#\n 2  2   setosa\n#\n 3  3   setosa\n#\n 4  4   setosa\n#\n 5  5   setosa\n#\n 6  6   setosa\n\nperformance(pred, measures = list(meancosts, mcp), task = costsens.task)\n#\n meancosts       mcp \n#\n  144.8978  139.8698", 
            "title": "Cost-Sensitive Classification"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html#cost-sensitive-classification", 
            "text": "In  regular classification  the aim is to minimize the misclassification rate and\nthus all types of misclassification errors are deemed equally severe.\nA more general setting is  cost-sensitive classification  where the costs caused by different\nkinds of errors are not assumed to be equal and the objective is to minimize the expected costs.  In case of  class-dependent costs  the costs depend on the true and predicted class label.\nThe costs  c ( k ,  l ) for predicting class  k  if the true label is  l  are usually organized\ninto a  K  times  K  cost matrix where  K  is the number of classes.\nNaturally, it is assumed that the cost of predicting the correct class label  y  is minimal\n(that is  c ( y ,  y ) is smaller or equal to  c ( k ,  y ) for all  k ).  A further generalization of this scenario are  example-dependent misclassification costs  where\neach example ( x ,  y ) is coupled with an individual cost vector of length  K . Its  k -th\ncomponent expresses the cost of assigning  x  to class  k .\nA real-world example is fraud detection where the costs do not only depend on the true and\npredicted status fraud/non-fraud, but also on the amount of money involved in each case.\nNaturally, the cost of predicting the true class label  y  is assumed to be minimum.\nThe true class labels are redundant information, as they can be easily inferred from the\ncost vectors.\nMoreover, given the cost vector, the expected costs do not depend on the true class label  y .\nThe classification problem is therefore completely defined by the feature values  x  and the\ncorresponding cost vectors.  In the following we show ways to handle cost-sensitive classification problems in  mlr .\nSome of the functionality is currently experimental, and there may be changes in the future.", 
            "title": "Cost-Sensitive Classification"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html#class-dependent-misclassification-costs", 
            "text": "There are some classification methods that can accomodate misclassification costs\ndirectly.\nOne example is  rpart .  Alternatively, we can use cost-insensitive methods and manipulate the predictions or the\ntraining data in order to take misclassification costs into account. mlr  supports  thresholding  and  rebalancing .    Thresholding :\n  The thresholds used to turn posterior probabilities into class labels are chosen such that\n  the costs are minimized.\n  This requires a  Learner  that can predict posterior probabilities.\n  During training the costs are not taken into account.    Rebalancing :\n  The idea is to change the proportion of the classes in the training data set in order to\n  account for costs during training, either by  weighting  or by  sampling .\n  Rebalancing does not require that the  Learner  can predict probabilities.  i. For  weighting  we need a  Learner  that supports class weights or observation\n     weights.  ii. If the  Learner  cannot deal with weights the proportion of classes can\n     be changed by  over-  and  undersampling .    We start with binary classification problems and afterwards deal with multi-class problems.  Binary classification problems  The positive and negative classes are labeled 1 and -1, respectively, and we consider the\nfollowing cost matrix where the rows indicate true classes and the columns predicted classes:            true\\pred.  1  -1    1  c (1,1)  c (-1,1)    -1  c (1,-1)  c (-1,-1)     Often, the diagonal entries are zero or the cost matrix is rescaled to achieve zeros in the diagonal\n(see for example  O'Brien et al, 2008 ).  A well-known cost-sensitive classification problem is posed by the German Credit data set \n(see also the  UCI Machine Learning Repository ).\nThe corresponding cost matrix (though  Elkan (2001) \nargues that this matrix is economically unreasonable) is given as:            true\\pred.  Bad  Good    Bad  0  5    Good  1  0     As in the table above, the rows indicate true and the columns predicted classes.  For class-dependent costs it is sufficient to generate an ordinary  ClassifTask .\nA  CostSensTask  is needed in case of example-dependent costs.\nIn the following we create the  ClassifTask , remove two constant features from the\ndata set and generate the cost matrix.\nPer default, Bad is the positive class.  data(GermanCredit, package =  caret )\ncredit.task = makeClassifTask(data = GermanCredit, target =  Class )\ncredit.task = removeConstantFeatures(credit.task)\n#  Removing 2 columns: Purpose.Vacation,Personal.Female.Single\ncredit.task\n#  Supervised task: GermanCredit\n#  Type: classif\n#  Target: Class\n#  Observations: 1000\n#  Features:\n#  numerics  factors  ordered \n#        59        0        0 \n#  Missings: FALSE\n#  Has weights: FALSE\n#  Has blocking: FALSE\n#  Classes: 2\n#   Bad Good \n#   300  700 \n#  Positive class: Bad\n\ncosts = matrix(c(0, 1, 5, 0), 2)\ncolnames(costs) = rownames(costs) = getTaskDescription(credit.task)$class.levels\ncosts\n#       Bad Good\n#  Bad    0    5\n#  Good   1    0  1. Thresholding  We start by fitting a  logistic regression model  to the German credit data set  and predicting posterior probabilities.  ## Train and predict posterior probabilities\nlrn = makeLearner( classif.multinom , predict.type =  prob , trace = FALSE)\nmod = train(lrn, credit.task)\npred = predict(mod, task = credit.task)\npred\n#  Prediction: 1000 observations\n#  predict.type: prob\n#  threshold: Bad=0.50,Good=0.50\n#  time: 0.02\n#    id truth   prob.Bad prob.Good response\n#  1  1  Good 0.03525092 0.9647491     Good\n#  2  2   Bad 0.63222363 0.3677764      Bad\n#  3  3  Good 0.02807414 0.9719259     Good\n#  4  4  Good 0.25182703 0.7481730     Good\n#  5  5   Bad 0.75193275 0.2480673      Bad\n#  6  6  Good 0.26230149 0.7376985     Good  The default thresholds for both classes are 0.5.\nBut according to the cost matrix we should predict class Good only if we are very sure that Good\nis the correct label and therefore increase the threshold for class Good and decrease the\nthreshold for class Bad.  i. Theoretical thresholding  The theoretical threshold for the  positive  class can be calculated from the cost matrix as \nFor more details see  Elkan (2001) .  Below the theoretical threshold for the  German credit example \nis calculated and used to predict class labels.\nSince the diagonal of the cost matrix is zero the formula given above simplifies accordingly.  ## Calculate the theoretical threshold for the positive class\nth = costs[2,1]/(costs[2,1] + costs[1,2])\nth\n#  [1] 0.1666667  As you may recall you can change thresholds in  mlr  either before training by using the predict.threshold  option of  makeLearner  or after prediction by calling  setThreshold \non the  Prediction  object.  As we already have a prediction we use the  setThreshold  function. It returns an altered Prediction  object with class predictions for the theoretical threshold.  ## Predict class labels according to the theoretical threshold\npred.th = setThreshold(pred, th)\npred.th\n#  Prediction: 1000 observations\n#  predict.type: prob\n#  threshold: Bad=0.17,Good=0.83\n#  time: 0.02\n#    id truth   prob.Bad prob.Good response\n#  1  1  Good 0.03525092 0.9647491     Good\n#  2  2   Bad 0.63222363 0.3677764      Bad\n#  3  3  Good 0.02807414 0.9719259     Good\n#  4  4  Good 0.25182703 0.7481730      Bad\n#  5  5   Bad 0.75193275 0.2480673      Bad\n#  6  6  Good 0.26230149 0.7376985      Bad  In order to calculate the average costs over the entire data set we first need to create a new\nperformance  Measure . This can be done through function  makeCostMeasure \nwhich requires the  ClassifTask  object and the cost matrix (argument  costs ).\nIt is expected that the rows of the cost matrix indicate true and the columns predicted\nclass labels.  credit.costs = makeCostMeasure(id =  credit.costs , costs = costs, task = credit.task, best = 0, worst = 5)\ncredit.costs\n#  Name: credit.costs\n#  Performance measure: credit.costs\n#  Properties: classif,classif.multi,req.pred,req.truth,predtype.response,predtype.prob\n#  Minimize: TRUE\n#  Best: 0; Worst: 5\n#  Aggregated by: test.mean\n#  Note:  Then the average costs can be computed by function  performance .\nBelow we compare the average costs and the error rate ( mmce ) of the learning algorithm\nwith both default thresholds 0.5 and theoretical thresholds.  ## Performance with default thresholds 0.5\nperformance(pred, measures = list(credit.costs, mmce))\n#  credit.costs         mmce \n#         0.774        0.214\n\n## Performance with theoretical thresholds\nperformance(pred.th, measures = list(credit.costs, mmce))\n#  credit.costs         mmce \n#         0.478        0.346  These performance values may be overly optimistic as we used the same data set for training\nand prediction, and resampling strategies should be preferred.\nIn the R-code below we make use of the  predict.threshold  argument of  makeLearner  to set\nthe threshold before doing 3-fold cross-validation.\nNote that we create a  ResampleInstance  ( rin ) for the German credit data  that is used throughout the next several code\nexamples to get comparable performance values.  ## Cross-validated performance with theoretical thresholds\nrin = makeResampleInstance( CV , iters = 3, task = credit.task)\nlrn = makeLearner( classif.multinom , predict.type =  prob , predict.threshold = th, trace = FALSE)\nr = resample(lrn, credit.task, resampling = rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#  Resample Result\n#  Task: GermanCredit\n#  Learner: classif.multinom\n#  credit.costs.aggr: 0.56\n#  credit.costs.mean: 0.56\n#  credit.costs.sd: 0.03\n#  mmce.aggr: 0.36\n#  mmce.mean: 0.36\n#  mmce.sd: 0.02\n#  Runtime: 0.338198  If we are also interested in the cross-validated performance for the default threshold values\nwe can call  setThreshold  on the  resampled prediction   r$pred .  ## Cross-validated performance with default thresholds\nperformance(setThreshold(r$pred, 0.5), measures = list(credit.costs, mmce))\n#  credit.costs         mmce \n#         0.852        0.248  Theoretical thresholding is only reliable if the predicted posterior probabilities are correct.\nIf there are systematic errors the thresholds have to be shifted accordingly.  Useful in this regard is function  plotThreshVsPerf  that permits to plot the average costs\nas well as any other performance measure versus possible threshold values for the positive\nclass in [0,1]. The underlying data is generated from  generateThreshVsPerfData .  The following plot show the cross-validated costs and error rate ( mmce ).\nThe theoretical threshold  th  calculated above is indicated by the vertical line.\nAs you can see the theoretical threshold seems a bit large.  d = generateThreshVsPerfData(r, measures = list(credit.costs, mmce))\nplotThreshVsPerf(d, mark.th = th)     ii. Empirical thresholding  In  empirical thresholding  (see  Sheng and Ling (2006) )\ncost-optimal threshold values for a given learning method are selected based on the training data.\nIn contrast to  theoretical thresholding  it suffices if the estimated posterior probabilities\nare order-correct.  In order to determine optimal threshold values you can use  mlr 's function  tuneThreshold .\nAs tuning the threshold on the complete training data set can lead to overfitting, resampling\nstrategies should be used.\nBelow we perform 3-fold cross-validation and use  tuneThreshold  to calculate threshold values\nwith lowest average costs over the 3 test data sets.  lrn = makeLearner( classif.multinom , predict.type =  prob , trace = FALSE)\n\n## 3-fold cross-validation\nr = resample(lrn, credit.task, resampling = rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#  Resample Result\n#  Task: GermanCredit\n#  Learner: classif.multinom\n#  credit.costs.aggr: 0.85\n#  credit.costs.mean: 0.85\n#  credit.costs.sd: 0.17\n#  mmce.aggr: 0.25\n#  mmce.mean: 0.25\n#  mmce.sd: 0.03\n#  Runtime: 0.335323\n\n## Tune the threshold based on the predicted probabilities on the 3 test data sets\ntune.res = tuneThreshold(pred = r$pred, measure = credit.costs)\ntune.res\n#  $th\n#  [1] 0.1110669\n#  \n#  $perf\n#  credit.costs \n#         0.508  tuneThreshold  returns the optimal threshold value for the positive class and the corresponding\nperformance.\nAs expected the tuned threshold is smaller than the theoretical threshold.  2. Rebalancing  In order to minimize the average costs, observations from the less costly class should be\ngiven higher importance during training.\nThis can be achieved by  weighting  the classes if the classification method under consideration\naccepts class or observations weights.\nAlternatively,  over- and undersampling  techniques can be used.  i. Weighting  Similar to  theoretical thresholding ,  theoretical weights  can be calculated from the\ncost matrix.\nIf  t  indicates the target threshold and  t 0  the original threshold for the positive class the\nproportion of observations in the positive class has to be multiplied by \nAlternatively, the proportion of observations in the negative class can be multiplied by\nthe inverse.\nA proof is given by  Elkan (2001) .  In most cases, the original threshold  t 0  is 0.5 and thus the second factor vanishes.\nIf additionally the target threshold  t  equals the theoretical threshold  t *  the\nproportion of observations in the positive class has to be multiplied by  Function  makeWeightedClassesWrapper  allows to assign class weights to  Learner s.\nNaturally, this is only possible for methods with either a 'class weights' or an 'observation weights' argument.\n(Have a look at the  table of integrated learners  to see which methods\nsupport observation weights.)  Function  multinom  accepts observation weights.\nThe weight given to all observations from the positive class is passed via argument  wcw.weight .\nObservations from the negative class automatically receive weight 1.  ## Weight corresponding to theoretical treshold\nw = (1 - th)/th\nw\n#  [1] 5\n\n## Weighted learner\nlrn = makeLearner( classif.multinom , trace = FALSE)\nlrn = makeWeightedClassesWrapper(lrn, wcw.weight = w)\nlrn\n#  Learner weightedclasses.classif.multinom from package nnet\n#  Type: classif\n#  Name: ; Short name: \n#  Class: WeightedClassesWrapper\n#  Properties: numerics,factors,prob,twoclass,multiclass\n#  Predict-Type: response\n#  Hyperparameters: trace=FALSE,wcw.weight=5  The theoretical threshold corresponds to a weight of 5 for the positive class.\nBelow the wrapped learner is used to get predictions. Moreover, its cross-validated performance is\ncalculated.  mod = train(lrn, credit.task)\npred = predict(mod, task = credit.task)\npred\n#  Prediction: 1000 observations\n#  predict.type: response\n#  threshold: \n#  time: 0.03\n#    id truth response\n#  1  1  Good     Good\n#  2  2   Bad      Bad\n#  3  3  Good     Good\n#  4  4  Good      Bad\n#  5  5   Bad      Bad\n#  6  6  Good      Bad\n\nr = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#  Resample Result\n#  Task: GermanCredit\n#  Learner: weightedclasses.classif.multinom\n#  credit.costs.aggr: 0.53\n#  credit.costs.mean: 0.53\n#  credit.costs.sd: 0.04\n#  mmce.aggr: 0.35\n#  mmce.mean: 0.35\n#  mmce.sd: 0.02\n#  Runtime: 0.391155  Some classification methods as the support vector machine ( ksvm ) in package  kernlab \nsupport class weights.\nWhen generating the wrapped  Learner  you have to pass the name of the relevant\nlearner parameter ( \"class.weights\"  for  ksvm ) via argument  wcw.param \nin addition to the weight.  lrn = makeWeightedClassesWrapper( classif.ksvm , wcw.param =  class.weights , wcw.weight = w)\nmod = train(lrn, credit.task)\npred = predict(mod, task = credit.task)\npred\n#  Prediction: 1000 observations\n#  predict.type: response\n#  threshold: \n#  time: 0.17\n#    id truth response\n#  1  1  Good     Good\n#  2  2   Bad      Bad\n#  3  3  Good     Good\n#  4  4  Good      Bad\n#  5  5   Bad      Bad\n#  6  6  Good     Good\n\nr = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#  Resample Result\n#  Task: GermanCredit\n#  Learner: weightedclasses.classif.ksvm\n#  credit.costs.aggr: 0.58\n#  credit.costs.mean: 0.58\n#  credit.costs.sd: 0.04\n#  mmce.aggr: 0.31\n#  mmce.mean: 0.31\n#  mmce.sd: 0.02\n#  Runtime: 0.813984  Just like the theoretical threshold, the theoretical weights may not always be suitable,\ntherefore you can tune the weight for the positive class as shown in the following example.\nCalculating the theoretical weight beforehand may help to narrow down the search interval.  lrn = makeLearner( classif.multinom , trace = FALSE)\nlrn = makeWeightedClassesWrapper(lrn)\nps = makeParamSet(makeDiscreteParam( wcw.weight , seq(4, 12, 0.5)))\nctrl = makeTuneControlGrid()\ntune.res = tuneParams(lrn, credit.task, resampling = rin, par.set = ps,\n  measures = list(credit.costs, mmce), control = ctrl, show.info = FALSE)\ntune.res\n#  Tune result:\n#  Op. pars: wcw.weight=7.5\n#  credit.costs.test.mean=0.501,mmce.test.mean=0.381\nas.data.frame(tune.res$opt.path)[1:3]\n#     wcw.weight credit.costs.test.mean mmce.test.mean\n#  1           4              0.5650291      0.3330127\n#  2         4.5              0.5550251      0.3430167\n#  3           5              0.5260320      0.3460197\n#  4         5.5              0.5130070      0.3530147\n#  5           6              0.5160100      0.3640137\n#  6         6.5              0.5160160      0.3720157\n#  7           7              0.5040250      0.3760167\n#  8         7.5              0.5010040      0.3810038\n#  9           8              0.5100130      0.3900128\n#  10        8.5              0.5100070      0.3940108\n#  11          9              0.5110080      0.4030078\n#  12        9.5              0.5160130      0.4080128\n#  13         10              0.5260140      0.4180138\n#  14       10.5              0.5240060      0.4200098\n#  15         11              0.5319991      0.4280029\n#  16       11.5              0.5289901      0.4330019\n#  17         12              0.5249801      0.4369999  ii. Over- and undersampling  If the  Learner  supports neither observation nor class weights the proportions\nof the classes in the training data can be changed by over- or undersampling.  In the  GermanCredit data set  the positive class Bad should receive\na theoretical weight of about  w = (1 - th)/th = 5 .\nThis can be achieved by oversampling class Bad with a  rate  of 5 (see also the documentation\npage of function  oversample ).  credit.task.over = oversample(credit.task, rate = w)\nlrn = makeLearner( classif.multinom , trace = FALSE)\nmod = train(lrn, credit.task.over)\npred = predict(mod, task = credit.task)\nperformance(pred, measures = list(credit.costs, mmce))\n#  credit.costs         mmce \n#         0.440        0.328  Note that in the example above the learner was trained on the oversampled task  credit.task.over .\nIn order to get the training performance on the original task predictions were calculated for  credit.task .  We usually prefer resampled performance values, but simply calling  resample  on the oversampled\ntask does not work since predictions have to be done for the original task.\nThe solution is to create a wrapped  Learner  via function makeOversampleWrapper .\nInternally,  oversample  is called before training, but predictions are done on the original task.  lrn = makeLearner( classif.multinom , trace = FALSE)\nlrn = makeOversampleWrapper(lrn, osw.rate = w)\nlrn\n#  Learner classif.multinom.oversampled from package mlr,nnet\n#  Type: classif\n#  Name: ; Short name: \n#  Class: OversampleWrapper\n#  Properties: numerics,factors,weights,prob,twoclass,multiclass\n#  Predict-Type: response\n#  Hyperparameters: trace=FALSE,osw.rate=5\n\nr = resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE)\nr\n#  Resample Result\n#  Task: GermanCredit\n#  Learner: classif.multinom.oversampled\n#  credit.costs.aggr: 0.56\n#  credit.costs.mean: 0.56\n#  credit.costs.sd: 0.05\n#  mmce.aggr: 0.35\n#  mmce.mean: 0.35\n#  mmce.sd: 0.02\n#  Runtime: 0.659542  Of course, we can also tune the oversampling rate.\nFor this purpose we again have to create an  OversampleWrapper .\nOptimal values for parameter  osw.rate  can be obtained using function  tuneParams .  lrn = makeLearner( classif.multinom , trace = FALSE)\nlrn = makeOversampleWrapper(lrn)\nps = makeParamSet(makeDiscreteParam( osw.rate , seq(3, 7, 0.25)))\nctrl = makeTuneControlGrid()\ntune.res = tuneParams(lrn, credit.task, rin, par.set = ps, measures = list(credit.costs, mmce),\n  control = ctrl, show.info = FALSE)\ntune.res\n#  Tune result:\n#  Op. pars: osw.rate=6\n#  credit.costs.test.mean=0.496,mmce.test.mean=0.352  Multi-class problems  We consider the  waveform  data set from package  mlbench  and\nadd an artificial cost matrix:             true\\pred.  1  2  3    1  0  30  80    2  5  0  4    3  10  8  0     We start by creating the  Task , the cost matrix and the corresponding performance measure.  ## Task\ndf = mlbench::mlbench.waveform(500)\nwf.task = makeClassifTask(id =  waveform , data = as.data.frame(df), target =  classes )\n\n## Cost matrix\ncosts = matrix(c(0, 5, 10, 30, 0, 8, 80, 4, 0), 3)\ncolnames(costs) = rownames(costs) = getTaskDescription(wf.task)$class.levels\n\n## Performance measure\nwf.costs = makeCostMeasure(id =  wf.costs , costs = costs, task = wf.task, best = 0,\n  worst = 10)  In the multi-class case, both,  thresholding  and  rebalancing  correspond to cost matrices\nof a certain structure where  c ( k , l ) =  c ( l ) for  k ,  l  = 1, ...,  K ,  k     l .\nThis condition means that the cost of misclassifying an observation is independent of the\npredicted class label\n(see  Domingos (1999) ).\nGiven a cost matrix of this type, theoretical thresholds and weights can be derived\nin a similar manner as in the binary case.\nObviously, the cost matrix given above does not have this special structure.  1. Thresholding  Given a vector of positive threshold values as long as the number of classes  K , the predicted\nprobabilities for all classes are adjusted by dividing them by the corresponding threshold value.\nThen the class with the highest adjusted probability is predicted.\nThis way, as in the binary case, classes with a low threshold are predicted more easily than\nclasses with a larger threshold.  Again this can be done by function  setThreshold  as shown in the following example (or\nalternatively by the  predict.threshold  option of  makeLearner ).\nNote that the threshold vector needs to have names that correspond to the class labels.  lrn = makeLearner( classif.rpart , predict.type =  prob )\nrin = makeResampleInstance( CV , iters = 3, task = wf.task)\nr = resample(lrn, wf.task, rin, measures = list(wf.costs, mmce), show.info = FALSE)\nr\n#  Resample Result\n#  Task: waveform\n#  Learner: classif.rpart\n#  wf.costs.aggr: 8.26\n#  wf.costs.mean: 8.26\n#  wf.costs.sd: 2.85\n#  mmce.aggr: 0.34\n#  mmce.mean: 0.34\n#  mmce.sd: 0.04\n#  Runtime: 0.162872\n\n## Calculate thresholds as 1/(average costs of true classes)\nth = 2/rowSums(costs)\nnames(th) = getTaskDescription(wf.task)$class.levels\nth\n#           1          2          3 \n#  0.01818182 0.22222222 0.11111111\n\npred.th = setThreshold(r$pred, threshold = th)\nperformance(pred.th, measures = list(wf.costs, mmce))\n#  wf.costs     mmce \n#     5.668    0.480  The threshold vector  th  in the above example is chosen according to the average costs\nof the true classes 55, 4.5 and 9.\nMore exactly,  th  corresponds to an artificial cost matrix of the structure mentioned\nabove with off-diagonal elements  c (2,1) =  c (3,1) = 55,  c (1,2) =  c (3,2) = 4.5 and c (1,3) =  c (2,3) = 9.\nThis threshold vector may be not optimal but leads to smaller total costs on the data set than\nthe default.  ii. Empirical thresholding  As in the binary case it is possible to tune the threshold vector using function  tuneThreshold .\nSince the scaling of the threshold vector does not change the predicted class labels tuneThreshold  returns threshold values that lie in [0,1] and sum to unity.  tune.res = tuneThreshold(pred = r$pred, measure = wf.costs)\ntune.res\n#  $th\n#              1             2             3 \n#  -1.657070e-09  3.914654e-01  6.085346e-01 \n#  \n#  $perf\n#  [1] 5.05  For comparison we show the standardized version of the theoretically motivated threshold\nvector chosen above.  th/sum(th)\n#           1          2          3 \n#  0.05172414 0.63218391 0.31609195  2. Rebalancing  i. Weighting  In the multi-class case you have to pass a vector of weights as long as the number of classes K  to function  makeWeightedClassesWrapper .\nThe weight vector can be tuned using function  tuneParams .  lrn = makeLearner( classif.multinom , trace = FALSE)\nlrn = makeWeightedClassesWrapper(lrn)\n\nps = makeParamSet(makeNumericVectorParam( wcw.weight , len = 3, lower = 0, upper = 1))\nctrl = makeTuneControlRandom()\n\ntune.res = tuneParams(lrn, wf.task, resampling = rin, par.set = ps,\n  measures = list(wf.costs, mmce), control = ctrl, show.info = FALSE)\ntune.res\n#  Tune result:\n#  Op. pars: wcw.weight=0.673,0.12,0.0204\n#  wf.costs.test.mean=2.63,mmce.test.mean=0.222", 
            "title": "Class-dependent misclassification costs"
        }, 
        {
            "location": "/cost_sensitive_classif/index.html#example-dependent-misclassification-costs", 
            "text": "In case of example-dependent costs we have to create a special  Task  via function makeCostSensTask .\nFor this purpose the feature values  x  and an  n  times  K   cost  matrix that contains\nthe cost vectors for all  n  examples in the data set are required.  We use the  iris  data and generate an artificial cost matrix.  df = iris\ncost = matrix(runif(150 * 3, 0, 2000), 150) * (1 - diag(3))[df$Species,] + runif(150, 0, 10)\ncolnames(cost) = levels(iris$Species)\nrownames(cost) = rownames(iris)\ndf$Species = NULL\n\ncostsens.task = makeCostSensTask(id =  iris , data = df, cost = cost)\ncostsens.task\n#  Supervised task: iris\n#  Type: costsens\n#  Observations: 150\n#  Features:\n#  numerics  factors  ordered \n#         4        0        0 \n#  Missings: FALSE\n#  Has blocking: FALSE\n#  Classes: 3\n#  setosa, versicolor, virginica  mlr  provides several  wrappers  to turn regular classification or regression methods\ninto  Learner s that can deal with example-dependent costs.   makeCostSensClassifWrapper  (wraps a classification  Learner ):\n  This is a naive approach where the costs are coerced into class labels by choosing the\n  class label with minimum cost for each example. Then a regular classification method is\n  used.  makeCostSensRegrWrapper  (wraps a regression  Learner ):\n  An individual regression model is fitted for the costs of each class.\n  In the prediction step first the costs are predicted for all classes and then the class with\n  the lowest predicted costs is selected.  makeCostSensWeightedPairsWrapper  (wraps a classification  Learner ):\n  This is also known as  cost-sensitive one-vs-one  (CS-OVO) and the most sophisticated of\n  the currently supported methods.\n  For each pair of classes, a binary classifier is fitted.\n  For each observation the class label is defined as the element of the pair with minimal costs.\n  During fitting, the observations are weighted with the absolute difference in costs.\n  Prediction is performed by simple voting.   In the following example we use the third method. We create the wrapped  Learner \nand train it on the  CostSensTask  defined above.  lrn = makeLearner( classif.multinom , trace = FALSE)\nlrn = makeCostSensWeightedPairsWrapper(lrn)\nlrn\n#  Learner costsens.classif.multinom from package nnet\n#  Type: costsens\n#  Name: ; Short name: \n#  Class: CostSensWeightedPairsWrapper\n#  Properties: numerics,factors,twoclass,multiclass\n#  Predict-Type: response\n#  Hyperparameters: trace=FALSE\n\nmod = train(lrn, costsens.task)\nmod\n#  Model for learner.id=costsens.classif.multinom; learner.class=CostSensWeightedPairsWrapper\n#  Trained on: task.id = iris; obs = 150; features = 4\n#  Hyperparameters: trace=FALSE  The models corresponding to the individual pairs can be accessed by function getHomogeneousEnsembleModels .  getHomogeneousEnsembleModels(mod)\n#  [[1]]\n#  Model for learner.id=classif.multinom; learner.class=classif.multinom\n#  Trained on: task.id = feats; obs = 150; features = 4\n#  Hyperparameters: trace=FALSE\n#  \n#  [[2]]\n#  Model for learner.id=classif.multinom; learner.class=classif.multinom\n#  Trained on: task.id = feats; obs = 150; features = 4\n#  Hyperparameters: trace=FALSE\n#  \n#  [[3]]\n#  Model for learner.id=classif.multinom; learner.class=classif.multinom\n#  Trained on: task.id = feats; obs = 150; features = 4\n#  Hyperparameters: trace=FALSE  mlr  provides some performance measures for example-specific cost-sensitive classification.\nIn the following example we calculate the mean costs of the predicted class labels\n( meancosts ) and the misclassification penalty ( mcp ).\nThe latter measure is the average difference between the costs caused by the predicted\nclass labels, i.e.,  meancosts , and the costs resulting from choosing the\nclass with lowest cost for each observation.\nIn order to compute these measures the costs for the test observations are required and\ntherefore the  Task  has to be passed to  performance .  pred = predict(mod, task = costsens.task)\npred\n#  Prediction: 150 observations\n#  predict.type: response\n#  threshold: \n#  time: 0.13\n#    id response\n#  1  1   setosa\n#  2  2   setosa\n#  3  3   setosa\n#  4  4   setosa\n#  5  5   setosa\n#  6  6   setosa\n\nperformance(pred, measures = list(meancosts, mcp), task = costsens.task)\n#  meancosts       mcp \n#   144.8978  139.8698", 
            "title": "Example-dependent misclassification costs"
        }, 
        {
            "location": "/over_and_undersampling/index.html", 
            "text": "Imbalanced Classification Problems\n\n\nIn case of \nbinary classification\n strongly imbalanced classes\noften lead to unsatisfactory results regarding the prediction of new\nobservations, especially for the small class.\nIn this context \nimbalanced classes\n simply means that the number of\nobservations of one class (usu. positive or majority class) by far exceeds\nthe number of observations of the other class (usu. negative or minority class).\nThis setting can be observed fairly often in practice and in various disciplines\nlike credit scoring, fraud detection, medical diagnostics or churn management.\n\n\nMost classification methods work best when the number of observations per\nclass are roughly equal. The problem with \nimbalanced classes\n is that because\nof the dominance of the majority class classifiers tend to ignore cases of\nthe minority class as noise and therefore predict the majority class far more\noften. In order to lay more weight on the cases of the minority class, there are\nnumerous correction methods which tackle the \nimbalanced classification problem\n.\nThese methods can generally be divided into \ncost- and sampling-based approaches\n.\nBelow all methods supported by \nmlr\n are introduced.\n\n\nSampling-based approaches\n\n\nThe basic idea of \nsampling methods\n is to simply adjust the proportion of\nthe classes in order to increase the weight of the minority class observations\nwithin the model.\n\n\nThe \nsampling-based approaches\n can be divided further into three different categories:\n\n\n\n\n\n\nUndersampling methods\n:\n   Elimination of randomly chosen cases of the majority class to decrease their\n   effect on the classifier. All cases of the minority class are kept.\n\n\n\n\n\n\nOversampling methods\n:\n   Generation of additional cases (copies, artificial observations) of the minority\n   class to increase their effect on the classifier. All cases of the majority\n   class are kept.\n\n\n\n\n\n\nHybrid methods\n:\n   Mixture of under- and oversampling strategies.\n\n\n\n\n\n\nAll these methods directly access the underlying data and \"rearrange\" it.\nIn this way the sampling is done as part of the \npreprocesssing\n and can therefore\nbe combined with every appropriate classifier.\n\n\nmlr\n currently supports the first two approaches.\n\n\n(Simple) over- and undersampling\n\n\nAs mentioned above \nundersampling\n always refers to the majority class, while\n\noversampling\n affects the minority class. By the use of \nundersampling\n, randomly\nchosen observations of the majority class are eliminated. Through (simple)\n\noversampling\n all observations of the minority class are considered at least\nonce when fitting the model. In addition, exact copies of minority class cases are created\nby random sampling with repetitions.\n\n\nFirst, let's take a look at the effect for a classification \ntask\n.\nBased on a simulated \nClassifTask\n with imbalanced classes two new\ntasks (\ntask.over\n, \ntask.under\n) are created via \nmlr\n functions\n\noversample\n and \nundersample\n, respectively.\n\n\ndata.imbal.train = rbind(\n  data.frame(x = rnorm(100, mean = 1), class = \nA\n),\n  data.frame(x = rnorm(5000, mean = 2), class = \nB\n)\n)\ntask = makeClassifTask(data = data.imbal.train, target = \nclass\n)\ntask.over = oversample(task, rate = 8)\ntask.under = undersample(task, rate = 1/8)\ntable(getTaskTargets(task))\n#\n \n#\n    A    B \n#\n  100 5000\ntable(getTaskTargets(task.over))\n#\n \n#\n    A    B \n#\n  800 5000\ntable(getTaskTargets(task.under))\n#\n \n#\n   A   B \n#\n 100 625\n\n\n\n\nPlease note that the \nundersampling rate\n has to be between 0 and 1, where 1 means\nno undersampling and 0.5 implies a reduction of the majority class size to 50 percent.\nCorrespondingly, the \noversampling rate\n must be greater or equal to 1,\nwhere 1 means no oversampling and 2 would result in doubling the minority\nclass size.\n\n\nAs a result the \nperformance\n should improve if the model is applied to new data.\n\n\nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nprob\n)\nmod = train(lrn, task)\nmod.over = train(lrn, task.over)\nmod.under = train(lrn, task.under)\ndata.imbal.test = rbind(\n  data.frame(x = rnorm(10, mean = 1), class = \nA\n),\n  data.frame(x = rnorm(500, mean = 2), class = \nB\n)\n)\nperformance(predict(mod, newdata = data.imbal.test), measures = list(mmce, auc))\n#\n       mmce        auc \n#\n 0.01960784 0.50000000\nperformance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, auc))\n#\n       mmce        auc \n#\n 0.03529412 0.59000000\nperformance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, auc))\n#\n       mmce        auc \n#\n 0.05686275 0.65370000\n\n\n\n\nIn this case the \nperformance measure\n has to be considered very carefully.\nAs the \nmisclassification rate\n (\nmmce\n) evaluates the overall\naccuracy of the predictions, the \narea under the ROC Curve\n (\nauc\n) \nmight be more suitable here, as the misclassifications within each class\nare separately taken into account.\n\n\nOver- and undersampling wrappers\n\n\nAlternatively, \nmlr\n also offers the integration of over- and undersampling\nvia a \nwrapping-based approach\n (\nwrappers\n). In this way\nover- and undersampling can be applied to already existing \nlearners\n\nto extend their functionality. \n\n\nThe example given above is repeated once again, but this time with extended\nlearners instead of modified tasks (see \nmakeOversampleWrapper\n\nand \nmakeUndersampleWrapper\n).\nJust like before the \nundersampling rate\n has to be between 0 and 1, while the\n\noversampling rate\n has a lower boundary of 1.\n\n\nlrn.over = makeOversampleWrapper(lrn, osw.rate = 8)\nlrn.under = makeUndersampleWrapper(lrn, usw.rate = 1/8)\nmod = train(lrn, task)\nmod.over = train(lrn.over, task)\nmod.under = train(lrn.under, task)\nperformance(predict(mod, newdata = data.imbal.test), measures = list(mmce, auc))\n#\n       mmce        auc \n#\n 0.01960784 0.50000000\nperformance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, auc))\n#\n       mmce        auc \n#\n 0.03529412 0.67860000\nperformance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, auc))\n#\n       mmce        auc \n#\n 0.03529412 0.59000000\n\n\n\n\nExtensions to oversampling\n\n\nTwo extensions to (simple) oversampling are available in \nmlr\n.\n\n\n1. SMOTE (Synthetic Minority Oversampling Technique)\n\n\nAs the duplicating of the minority class observations can lead to overfitting,\nwithin \nSMOTE\n the \"new cases\" are constructed in a different way. For each\nnew observation, one randomly chosen minority class observation as well as\none of its \nrandomly chosen next neighbours\n are interpolated, so that finally\na new \nartificial observation\n of the minority class is created.\nThe \nsmote\n function in \nmlr\n handles numeric as well as factor features, as\nthe gower distance is used for nearest neighbour calculation. The factor level\nof the new artificial case is sampled from the given levels of the two\ninput observations.\n\n\nAnalogous to oversampling, \nSMOTE preprocessing\n is possible via modification\nof the task.\n\n\ntask.smote = smote(task, rate = 8, nn = 5)\ntable(getTaskTargets(task))\n#\n \n#\n    A    B \n#\n  100 5000\ntable(getTaskTargets(task.smote))\n#\n \n#\n    A    B \n#\n  800 5000\n\n\n\n\nAlternatively, a new wrapped learner can be created via \nmakeSMOTEWrapper\n.\n\n\nlrn.smote = makeSMOTEWrapper(lrn, sw.rate = 8, sw.nn = 5)\nmod.smote = train(lrn.smote, task)\nperformance(predict(mod.smote, newdata = data.imbal.test), measures = auc)\n#\n    auc \n#\n 0.6718\n\n\n\n\nBy default the number of nearest neighbours considered within the algorithm is\nset to 5.\n\n\n2. Overbagging\n\n\nAnother extension of oversampling consists in the combination of sampling with\nthe \nbagging-approach\n. For each iteration of the bagging process,\nminority class observations are oversampled with a given rate in \nobw.rate\n.\nThe majority class cases can either all be taken into account for each\niteration (\nobw.maxcl = \"all\"\n) or bootstrapped with replacement to increase\nvariability between training data sets during iterations (\nobw.maxcl = \"boot\"\n).\n\n\nThe construction of the \nOverbagging Wrapper\n works similar\nto \nmakeBaggingWrapper\n.\nFirst an existing \nmlr\n learner has to be passed to \nmakeOverBaggingWrapper\n.\nThe number of iterations or fitted models can be set via \nobw.iters\n.\n\n\nlrn = makeLearner(\nclassif.rpart\n, predict.type = \nresponse\n)\nobw.lrn = makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3)\n\n\n\n\nFor \nbinary classification\n the prediction is based on majority voting to create\na discrete label. Corresponding probabilities are predicted by considering\nthe proportions of all the predicted labels.\nPlease note that the benefit of the sampling process is \nhighly dependent\n\non the specific learner as shown in the following example.\n\n\nFirst, let's take a look at the tree learner with and without overbagging:\n\n\nlrn = setPredictType(lrn, \nprob\n)\nrdesc = makeResampleDesc(\nCV\n, iters = 5)\nr1 = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE, measures = auc)\nr1$aggr\n#\n auc.test.mean \n#\n           0.5\n\nobw.lrn = setPredictType(obw.lrn, \nprob\n)\nr2 = resample(learner = obw.lrn, task = task, resampling = rdesc, show.info = FALSE, measures = auc)\nr2$aggr\n#\n auc.test.mean \n#\n     0.5870533\n\n\n\n\nNow let's consider a \nrandom forest\n as initial learner:\n\n\nlrn = makeLearner(\nclassif.randomForest\n)\nobw.lrn = makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3)\n\nlrn = setPredictType(lrn, \nprob\n)\nres1 = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE, measures = auc)\nres1$aggr\n#\n auc.test.mean \n#\n     0.5794236\n\nobw.lrn = setPredictType(obw.lrn, \nprob\n)\nres2 = resample(learner = obw.lrn, task = task, resampling = rdesc, show.info = FALSE,  measures = auc)\nres2$aggr\n#\n auc.test.mean \n#\n     0.5290915\n\n\n\n\nWhile \noverbagging\n slighty improves the performance of the \ndecision tree\n,\nthe auc decreases in the second example when additional overbagging is applied.\nAs the \nrandom forest\n itself is already a strong learner (and a bagged one\nas well), a further bagging step isn't very helpful here and usually won't\nimprove the model.\n\n\nCost-based approaches\n\n\nIn contrast to sampling, \ncost-based approaches\n usually require particular \nlearners, which can deal with different \nclass-dependent costs\n\n(\nCost-Sensitive Classification\n).\n\n\nWeighted classes wrapper\n\n\nAnother approach independent of the underlying classifier is to \nassign the costs as \nclass weights\n, so that each observation receives a weight,\ndepending on the class it belongs to. Similar to the sampling-based approaches,\nthe effect of the minority class observations is thereby increased simply by a\nhigher weight of these instances and vice versa for majority class observations.\n\n\nIn this way every learner which supports weights can be extended through\nthe \nwrapping-approach\n.\nIf the learner does not have a direct parameter for class weights,\nbut supports observation weights, the weights depending on the class are\ninternally set in the wrapper.\n\n\nlrn = makeLearner(\nclassif.logreg\n)\nwcw.lrn = makeWeightedClassesWrapper(lrn, wcw.weight = 0.01)\n\n\n\n\nFor binary classification, the single number passed to the classifier corresponds\nto the weight of the positive / majority class, while the negative / minority \nclass receives a weight of 1. So actually, no real costs are used within\nthis approach, but the cost ratio is taken into account.\n\n\nIf the underlying learner already has a parameter for class weighting (e.g.,\n\nclass.weights\n in \nclassif.ksvm\n), the \nwcw.weight\n is basically bound\nto the specific class weighting parameter. The latter has to be defined as an\nadditional parameter (\nwcw.param\n).\n\n\nlrn = makeLearner(\nclassif.ksvm\n)\nwcw.lrn = makeWeightedClassesWrapper(lrn, wcw.param = \nclass.weights\n, wcw.weight = 0.01)", 
            "title": "Imbalanced Classification Problems"
        }, 
        {
            "location": "/over_and_undersampling/index.html#imbalanced-classification-problems", 
            "text": "In case of  binary classification  strongly imbalanced classes\noften lead to unsatisfactory results regarding the prediction of new\nobservations, especially for the small class.\nIn this context  imbalanced classes  simply means that the number of\nobservations of one class (usu. positive or majority class) by far exceeds\nthe number of observations of the other class (usu. negative or minority class).\nThis setting can be observed fairly often in practice and in various disciplines\nlike credit scoring, fraud detection, medical diagnostics or churn management.  Most classification methods work best when the number of observations per\nclass are roughly equal. The problem with  imbalanced classes  is that because\nof the dominance of the majority class classifiers tend to ignore cases of\nthe minority class as noise and therefore predict the majority class far more\noften. In order to lay more weight on the cases of the minority class, there are\nnumerous correction methods which tackle the  imbalanced classification problem .\nThese methods can generally be divided into  cost- and sampling-based approaches .\nBelow all methods supported by  mlr  are introduced.", 
            "title": "Imbalanced Classification Problems"
        }, 
        {
            "location": "/over_and_undersampling/index.html#sampling-based-approaches", 
            "text": "The basic idea of  sampling methods  is to simply adjust the proportion of\nthe classes in order to increase the weight of the minority class observations\nwithin the model.  The  sampling-based approaches  can be divided further into three different categories:    Undersampling methods :\n   Elimination of randomly chosen cases of the majority class to decrease their\n   effect on the classifier. All cases of the minority class are kept.    Oversampling methods :\n   Generation of additional cases (copies, artificial observations) of the minority\n   class to increase their effect on the classifier. All cases of the majority\n   class are kept.    Hybrid methods :\n   Mixture of under- and oversampling strategies.    All these methods directly access the underlying data and \"rearrange\" it.\nIn this way the sampling is done as part of the  preprocesssing  and can therefore\nbe combined with every appropriate classifier.  mlr  currently supports the first two approaches.  (Simple) over- and undersampling  As mentioned above  undersampling  always refers to the majority class, while oversampling  affects the minority class. By the use of  undersampling , randomly\nchosen observations of the majority class are eliminated. Through (simple) oversampling  all observations of the minority class are considered at least\nonce when fitting the model. In addition, exact copies of minority class cases are created\nby random sampling with repetitions.  First, let's take a look at the effect for a classification  task .\nBased on a simulated  ClassifTask  with imbalanced classes two new\ntasks ( task.over ,  task.under ) are created via  mlr  functions oversample  and  undersample , respectively.  data.imbal.train = rbind(\n  data.frame(x = rnorm(100, mean = 1), class =  A ),\n  data.frame(x = rnorm(5000, mean = 2), class =  B )\n)\ntask = makeClassifTask(data = data.imbal.train, target =  class )\ntask.over = oversample(task, rate = 8)\ntask.under = undersample(task, rate = 1/8)\ntable(getTaskTargets(task))\n#  \n#     A    B \n#   100 5000\ntable(getTaskTargets(task.over))\n#  \n#     A    B \n#   800 5000\ntable(getTaskTargets(task.under))\n#  \n#    A   B \n#  100 625  Please note that the  undersampling rate  has to be between 0 and 1, where 1 means\nno undersampling and 0.5 implies a reduction of the majority class size to 50 percent.\nCorrespondingly, the  oversampling rate  must be greater or equal to 1,\nwhere 1 means no oversampling and 2 would result in doubling the minority\nclass size.  As a result the  performance  should improve if the model is applied to new data.  lrn = makeLearner( classif.rpart , predict.type =  prob )\nmod = train(lrn, task)\nmod.over = train(lrn, task.over)\nmod.under = train(lrn, task.under)\ndata.imbal.test = rbind(\n  data.frame(x = rnorm(10, mean = 1), class =  A ),\n  data.frame(x = rnorm(500, mean = 2), class =  B )\n)\nperformance(predict(mod, newdata = data.imbal.test), measures = list(mmce, auc))\n#        mmce        auc \n#  0.01960784 0.50000000\nperformance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, auc))\n#        mmce        auc \n#  0.03529412 0.59000000\nperformance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, auc))\n#        mmce        auc \n#  0.05686275 0.65370000  In this case the  performance measure  has to be considered very carefully.\nAs the  misclassification rate  ( mmce ) evaluates the overall\naccuracy of the predictions, the  area under the ROC Curve  ( auc ) \nmight be more suitable here, as the misclassifications within each class\nare separately taken into account.  Over- and undersampling wrappers  Alternatively,  mlr  also offers the integration of over- and undersampling\nvia a  wrapping-based approach  ( wrappers ). In this way\nover- and undersampling can be applied to already existing  learners \nto extend their functionality.   The example given above is repeated once again, but this time with extended\nlearners instead of modified tasks (see  makeOversampleWrapper \nand  makeUndersampleWrapper ).\nJust like before the  undersampling rate  has to be between 0 and 1, while the oversampling rate  has a lower boundary of 1.  lrn.over = makeOversampleWrapper(lrn, osw.rate = 8)\nlrn.under = makeUndersampleWrapper(lrn, usw.rate = 1/8)\nmod = train(lrn, task)\nmod.over = train(lrn.over, task)\nmod.under = train(lrn.under, task)\nperformance(predict(mod, newdata = data.imbal.test), measures = list(mmce, auc))\n#        mmce        auc \n#  0.01960784 0.50000000\nperformance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, auc))\n#        mmce        auc \n#  0.03529412 0.67860000\nperformance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, auc))\n#        mmce        auc \n#  0.03529412 0.59000000  Extensions to oversampling  Two extensions to (simple) oversampling are available in  mlr .  1. SMOTE (Synthetic Minority Oversampling Technique)  As the duplicating of the minority class observations can lead to overfitting,\nwithin  SMOTE  the \"new cases\" are constructed in a different way. For each\nnew observation, one randomly chosen minority class observation as well as\none of its  randomly chosen next neighbours  are interpolated, so that finally\na new  artificial observation  of the minority class is created.\nThe  smote  function in  mlr  handles numeric as well as factor features, as\nthe gower distance is used for nearest neighbour calculation. The factor level\nof the new artificial case is sampled from the given levels of the two\ninput observations.  Analogous to oversampling,  SMOTE preprocessing  is possible via modification\nof the task.  task.smote = smote(task, rate = 8, nn = 5)\ntable(getTaskTargets(task))\n#  \n#     A    B \n#   100 5000\ntable(getTaskTargets(task.smote))\n#  \n#     A    B \n#   800 5000  Alternatively, a new wrapped learner can be created via  makeSMOTEWrapper .  lrn.smote = makeSMOTEWrapper(lrn, sw.rate = 8, sw.nn = 5)\nmod.smote = train(lrn.smote, task)\nperformance(predict(mod.smote, newdata = data.imbal.test), measures = auc)\n#     auc \n#  0.6718  By default the number of nearest neighbours considered within the algorithm is\nset to 5.  2. Overbagging  Another extension of oversampling consists in the combination of sampling with\nthe  bagging-approach . For each iteration of the bagging process,\nminority class observations are oversampled with a given rate in  obw.rate .\nThe majority class cases can either all be taken into account for each\niteration ( obw.maxcl = \"all\" ) or bootstrapped with replacement to increase\nvariability between training data sets during iterations ( obw.maxcl = \"boot\" ).  The construction of the  Overbagging Wrapper  works similar\nto  makeBaggingWrapper .\nFirst an existing  mlr  learner has to be passed to  makeOverBaggingWrapper .\nThe number of iterations or fitted models can be set via  obw.iters .  lrn = makeLearner( classif.rpart , predict.type =  response )\nobw.lrn = makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3)  For  binary classification  the prediction is based on majority voting to create\na discrete label. Corresponding probabilities are predicted by considering\nthe proportions of all the predicted labels.\nPlease note that the benefit of the sampling process is  highly dependent \non the specific learner as shown in the following example.  First, let's take a look at the tree learner with and without overbagging:  lrn = setPredictType(lrn,  prob )\nrdesc = makeResampleDesc( CV , iters = 5)\nr1 = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE, measures = auc)\nr1$aggr\n#  auc.test.mean \n#            0.5\n\nobw.lrn = setPredictType(obw.lrn,  prob )\nr2 = resample(learner = obw.lrn, task = task, resampling = rdesc, show.info = FALSE, measures = auc)\nr2$aggr\n#  auc.test.mean \n#      0.5870533  Now let's consider a  random forest  as initial learner:  lrn = makeLearner( classif.randomForest )\nobw.lrn = makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3)\n\nlrn = setPredictType(lrn,  prob )\nres1 = resample(learner = lrn, task = task, resampling = rdesc, show.info = FALSE, measures = auc)\nres1$aggr\n#  auc.test.mean \n#      0.5794236\n\nobw.lrn = setPredictType(obw.lrn,  prob )\nres2 = resample(learner = obw.lrn, task = task, resampling = rdesc, show.info = FALSE,  measures = auc)\nres2$aggr\n#  auc.test.mean \n#      0.5290915  While  overbagging  slighty improves the performance of the  decision tree ,\nthe auc decreases in the second example when additional overbagging is applied.\nAs the  random forest  itself is already a strong learner (and a bagged one\nas well), a further bagging step isn't very helpful here and usually won't\nimprove the model.", 
            "title": "Sampling-based approaches"
        }, 
        {
            "location": "/over_and_undersampling/index.html#cost-based-approaches", 
            "text": "In contrast to sampling,  cost-based approaches  usually require particular \nlearners, which can deal with different  class-dependent costs \n( Cost-Sensitive Classification ).  Weighted classes wrapper  Another approach independent of the underlying classifier is to \nassign the costs as  class weights , so that each observation receives a weight,\ndepending on the class it belongs to. Similar to the sampling-based approaches,\nthe effect of the minority class observations is thereby increased simply by a\nhigher weight of these instances and vice versa for majority class observations.  In this way every learner which supports weights can be extended through\nthe  wrapping-approach .\nIf the learner does not have a direct parameter for class weights,\nbut supports observation weights, the weights depending on the class are\ninternally set in the wrapper.  lrn = makeLearner( classif.logreg )\nwcw.lrn = makeWeightedClassesWrapper(lrn, wcw.weight = 0.01)  For binary classification, the single number passed to the classifier corresponds\nto the weight of the positive / majority class, while the negative / minority \nclass receives a weight of 1. So actually, no real costs are used within\nthis approach, but the cost ratio is taken into account.  If the underlying learner already has a parameter for class weighting (e.g., class.weights  in  classif.ksvm ), the  wcw.weight  is basically bound\nto the specific class weighting parameter. The latter has to be defined as an\nadditional parameter ( wcw.param ).  lrn = makeLearner( classif.ksvm )\nwcw.lrn = makeWeightedClassesWrapper(lrn, wcw.param =  class.weights , wcw.weight = 0.01)", 
            "title": "Cost-based approaches"
        }, 
        {
            "location": "/roc_analysis/index.html", 
            "text": "ROC Analysis and Performance Curves\n\n\nFor binary scoring classifiers a \nthreshold\n (in the following also called \ncutoff\n) value\ncontrols how predicted posterior probabilities are turned into class labels.\nROC curves and other performance plots serve to visualize and analyse the relationship between\none or two performance measures and the threshold.\n\n\nThis section is mainly devoted to \nreceiver operating characteristic\n (ROC) curves that\nplot the \ntrue positive rate\n (sensitivity) on the vertical axis against the \nfalse positive rate\n\n(1 - specificity, fall-out) on the horizontal axis for all possible threshold values.\nCreating other performance plots like \nlift charts\n or \nprecision/recall graphs\n works\nanalogously and is only shown briefly.\n\n\nIn addition to performance visualization ROC curves are helpful in\n\n\n\n\ndetermining an optimal decision threshold for given class prior probabilities and\n  misclassification costs (for alternatives see also the sections about\n  \ncost-sensitive classification\n and\n  \nimbalanced classification problems\n in this tutorial),\n\n\nidentifying regions where one classifier outperforms another and building suitable multi-classifier\n  systems,\n\n\nobtaining calibrated estimates of the posterior probabilities.\n\n\n\n\nFor more information see the tutorials and introductory papers by\n\nFawcett (2004)\n,\n\nFawcett (2006)\n\nas well as \nFlach (ICML 2004)\n.\n\n\nIn many applications as, e.g., diagnostic tests or spam detection, there is uncertainty\nabout the class priors or the misclassification costs at the time of prediction, for example\nbecause it's hard to quantify the costs or because costs and class priors vary over time.\nUnder these circumstances the classifier is expected to work well for a whole range of\ndecision thresholds and the area under the ROC curve (AUC) provides a scalar performance\nmeasure for comparing and selecting classifiers.\n\nmlr\n provides the AUC for binary classification (\nauc\n based on package\n\nROCR\n) an also a generalization of the AUC for the\nmulti-class case (\nmulticlass.auc\n based on package \npROC\n).\n\n\nmlr\n offers three ways to plot ROC and other performance curves.\n\n\n\n\nmlr\n's function \ngenerateROCRCurvesData\n is a convenient interface to \nROCR\n's\n   \nperformance\n methods with an associated plotting function,\n   \nplotROCRCurves\n which uses \nggplot2\n.\n\n\nThe \nmlr\n function \nasROCRPrediction\n converts \nmlr\n \nPrediction\n objects to objects\n   of \nROCR\n's class \nprediction\n.\n   Then, \nROCR\n's functionality can be used to further analyse the results and generate\n   performance plots.\n\n\nmlr\n's function \nplotViperCharts\n provides an interface to\n   \nViperCharts\n.\n\n\n\n\nLet's have a look at some examples demonstrating the three possible methods.\n\n\nNote that the \nlearners\n have to be capable of predicting probabilities.\nHave a look at the \ntable of learners\n\nor run \nlistLearners(prob = TRUE)\n to get a list of all learners that support this.\n\n\nPerformance plots with generateROCRCurvesData and plotROCRCurves\n\n\nAs mentioned above \ngenerateROCRCurvesData\n is an interface to \nROCR\n's\n\nperformance\n methods.\nIt provides S3 methods for objects of class \nPrediction\n, \nResampleResult\n\nand \nBenchmarkResult\n (resulting from calling \npredict\n, \nresample\n\nor \nbenchmark\n). \nplotROCRCurves\n plots output from \ngenerateROCRCurvesData\n using \nggplot2\n.\n\n\nExample 1: Single predictions\n\n\nWe consider the \nSonar\n data set from package \nmlbench\n, which poses a\nbinary classification problem (\nsonar.task\n) and apply \nlinear discriminant analysis\n.\n\n\nlrn1 = makeLearner(\nclassif.lda\n, predict.type = \nprob\n)\nmod1 = train(lrn1, sonar.task)\npred1 = predict(mod1, task = sonar.task)\nroc_data = generateROCRCurvesData(pred1)\nroc_data\n#\n      learner False positive rate True positive rate    Cutoff\n#\n 1 prediction                   0        0.000000000 1.0048061\n#\n 2 prediction                   0        0.009009009 0.9999755\n#\n 3 prediction                   0        0.018018018 0.9998530\n#\n 4 prediction                   0        0.027027027 0.9998145\n#\n 5 prediction                   0        0.036036036 0.9997841\n\n\n\n\ngenerateROCRCurvesData\n returns an object of class \"ROCRCurvesData\" which contains the results from \nROCR\n's \nperformance\n method (which depends on arguments \nmeas1\n and \nmeas2\n). The data can be extracted by accessing the data element of the object. The object also contains information about the input arguments to \ngenerateROCRCurvesData\n which may be useful.\n\n\nPer default, \nplotROCRCurves\n draws a ROC curve and optionally adds a diagonal line that represents\nthe performance of a random classifier.\n\n\ndroc = generateROCRCurvesData(pred1)\nplotROCRCurves(droc, diagonal = TRUE)\n\n\n\n\n \nThere is also an experimental plotting function \nplotROCRCurvesGGVIS\n which uses \nggvis\n to create similar\nfigures with the addition of (optional) interactive tooltips (displayed on hover) that display the threshold\nat that point in the curve.\n\n\nplotROCRCurvesGGVIS(droc, cutoffs = TRUE)\n\n\n\n\nThe corresponding area under curve (\nauc\n) can be calculated as usual by calling\n\nperformance\n.\n\n\nperformance(pred1, auc)\n#\n       auc \n#\n 0.9717656\n\n\n\n\nIn addition to \nlinear discriminant analysis\n we try a support vector machine\nwith RBF kernel (\nksvm\n).\n\n\nlrn2 = makeLearner(\nclassif.ksvm\n, predict.type = \nprob\n)\nmod2 = train(lrn2, sonar.task)\n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel\npred2 = predict(mod2, task = sonar.task)\n\n\n\n\nIn order to compare the performance of the two learners you might want to display the two\ncorresponding ROC curves in one plot.\nFor this purpose just pass a named \nlist\n of \nPrediction\ns to \nplotROCRCurves\n.\n\n\nplotROCRCurves(generateROCRCurvesData(list(lda = pred1, ksvm = pred2)))\n\n\n\n\n \n\n\nIt's clear from the plot above that \nksvm\n has a slightly higher AUC than\n\nlda\n.\n\n\nperformance(pred2, auc)\n#\n      auc \n#\n 0.992477\n\n\n\n\nIt is easily possible to generate other performance plots by passing the appropriate performance\nmeasures to \nplotROCRCurves\n.\nNote that arguments \nmeas1\n and \nmeas2\n do not refer to \nmlr\n's performance measures,\nbut to measures provided by \nROCR\n and listed \nhere\n.\nBelow is code for a \nlift chart\n which shows the lift value (\n\"lift\"\n) versus the rate of\npositive predictions (\n\"rpp\"\n).\n\n\nout = generateROCRCurvesData(list(lda = pred1, ksvm = pred2), meas1 = \nlift\n, meas2 = \nrpp\n)\nplotROCRCurves(out)\n\n\n\n\n \n\n\nA plot of a single performance measure (accuracy in the example code below) versus the\nthreshold can be generated by setting \nmeas2 = \"cutoff\"\n.\n\n\nout = generateROCRCurvesData(list(lda = pred1, ksvm = pred2), meas1 = \nacc\n, meas2 = \ncutoff\n)\nplotROCRCurves(out)\n\n\n\n\n \n\n\nAs you may recall, an alternative function for plotting performance values versus the decision\nthreshold is \nplotThreshVsPerf\n.\nWhile \nplotThreshVsPerf\n permits to plot several performance measures at once,\n\nplotROCRCurves\n makes it easy to superpose the performance curves of multiple learners.\n\n\nExample 2: Benchmark experiment\n\n\nThe analysis in the example above can be improved in several regards.\nWe only considered the training performance and, ideally, the support vector machine should\nhave been \ntuned\n.\nMoreover, we wrote individual code for training/prediction of each learner, which can become\ntedious very quickly.\nA more practical way to apply several learners to a \nTask\n and compare their performance is\nprovided by function \nbenchmark\n (see also \nBenchmark Experiments\n).\n\n\nWe again consider the \nSonar\n data set and apply \nlda\n\nas well as \nksvm\n.\nWe first generate a \ntuning wrapper\n for \nksvm\n.\nThe cost parameter is tuned on a (for demonstration purposes small) parameter grid.\nWe assume that we are interested in a good performance over the complete threshold range\nand therefore tune with regard to the \nauc\n.\nThe error rate (\nmmce\n) for threshold 0.5 is reported as well.\n\n\n## Tune wrapper for ksvm\nrdesc.inner = makeResampleDesc(\nHoldout\n)\nms = list(auc, mmce)\nps = makeParamSet(\n  makeDiscreteParam(\nC\n, 2^(-1:1))\n)\nctrl = makeTuneControlGrid()\nlrn2 = makeTuneWrapper(lrn2, rdesc.inner, ms, ps, ctrl, show.info = FALSE)\n\n\n\n\nBelow the actual benchmark experiment is conducted.\nAs resampling strategy we use 5-fold cross-validation and again calculate the \nauc\n\nas well as the error rate (for a threshold/cutoff value of 0.5).\n\n\n## Benchmark experiment\nlrns = list(lrn1, lrn2)\nrdesc.outer = makeResampleDesc(\nCV\n, iters = 5)\n\nres = benchmark(lrns, tasks = sonar.task, resampling = rdesc.outer, measures = ms, show.info = FALSE)\n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#\n Using automatic sigma estimation (sigest) for RBF or laplace kernel\nres\n#\n         task.id         learner.id auc.test.mean mmce.test.mean\n#\n 1 Sonar-example        classif.lda     0.7835442      0.2592334\n#\n 2 Sonar-example classif.ksvm.tuned     0.9454418      0.1390244\n\n\n\n\nCalling \nplotROCRCurves\n on the \nresult\n of the benchmark experiment\nproduces a plot with ROC curves for all learners in the experiment.\n\n\nplotROCRCurves(generateROCRCurvesData(res))\n\n\n\n\n \n\n\nPer default, threshold-averaged ROC curves are shown.\nSince we used 5-fold cross-validation we have predictions on 5 test data sets and therefore\n5 ROC curves per classifier.\nFor each threshold value the means of the corresponding 5 false and true positive rates are\ncalculated and plotted against each other.\n\n\nIf you want to plot the individual ROC curves for each resample iteration set \navg = \"none\"\n.\nOther available options are \navg = \"horizontal\"\n and \navg = \"vertical\"\n.\n\n\nplotROCRCurves(generateROCRCurvesData(res, avg = \nnone\n))\n\n\n\n\n \n\n\nAn alternative to averaging is to just merge the 5 test folds and draw a single ROC curve.\nMerging can be achieved by manually changing the \nclass\n attribute of\nthe prediction objects from \nResamplePrediction\n to \nPrediction\n.\n\n\nBelow the predictions are extracted from the \nBenchmarkResult\n via function \ngetBMRPredictions\n,\nthe \nclass\n is changed and the ROC curves are created.\n\n\nAveraging methods are normally preferred\n(cp. \nFawcett, 2006\n),\nas they permit to assess the variability, which is needed to properly compare classifier\nperformance.\n\n\n## Extract predictions\npreds = getBMRPredictions(res)[[1]]\n\n## Change the class attribute\npreds2 = lapply(preds, function(x) {class(x) = \nPrediction\n; return(x)})\n\n## Draw ROC curves\nplotROCRCurves(generateROCRCurvesData(preds2, avg = \nnone\n))\n\n\n\n\n \n\n\nAgain, you can easily create other standard evaluation plots by calling \nplotROCRCurves\n\non the \nBenchmarkResult\n with the appropriate performance measures (see \nROCR::performance\n).\n\n\nPerformance plots with asROCRPrediction\n\n\nDrawing performance plots with package \nROCR\n works through three basic commands:\n\n\n\n\nROCR::prediction\n: Create a \nROCR\n \nprediction\n object.\n\n\nROCR::performance\n: Calculate one or more performance measures for the\n   given prediction object.\n\n\nA reimplementation of \nROCR::plot\n which uses \nggplot2\n.\n\n\n\n\nmlr\n's function \nasROCRPrediction\n converts an \nmlr\n \nPrediction\n object to\na \nROCR\n \nprediction\n object.\nIn order to create performance plots steps 2. and 3. have to be run by the user.\n\n\nThis is obviously less convenient than calling \nplotROCRCurves\n (which extracts predictions,\ncalls \nasROCRPrediction\n and executes steps 2. and 3. internally).\nOn the other hand this way provides more control over the generated plots by, e.g., using graphical\nparameters that are not (yet) accessible via \nplotROCRCurves\n.\nMoreover, you can directly benefit from any enhancements in \nROCR\n, use your own\n\nROCR\n-based code or other packages that depend on \nROCR\n, and use \nROCR\n's \nplot\n.\nFor more details see the \nROCR\n documentation and \ndemo(ROCR)\n.\n\n\nAn addditional alternative is to call \nplotROCRCurves\n, extract the data from the \nggplot2\n object using the \ndata\n element of the object, e.g., \nobj$data\n, and then plot the data using whatever method you prefer.\n\n\nExample 1: Single predictions (continued)\n\n\nWe go back to out first example where we trained and predicted \nlda\n on the\n\nsonar classification task\n.\n\n\n## Train and predict linear discriminant analysis\nlrn1 = makeLearner(\nclassif.lda\n, predict.type = \nprob\n)\nmod1 = train(lrn1, sonar.task)\npred1 = predict(mod1, task = sonar.task)\n\n\n\n\nBelow we use \nasROCRPrediction\n to convert the lda prediction, let \nROCR\n calculate the\ntrue and false positive rate and plot the ROC curve.\n\n\n## Convert prediction\nROCRpred1 = asROCRPrediction(pred1)\n\n## Calculate true and false positive rate\nROCRperf1 = ROCR::performance(ROCRpred1, \ntpr\n, \nfpr\n)\n\n## Draw ROC curve\nROCR::plot(ROCRperf1)\n\n\n\n\n \n\n\nBelow is the same ROC curve, but we make use of some more graphical parameters:\nThe ROC curve is color-coded by the threshold and selected threshold values are printed on\nthe curve. Additionally, the convex hull (black broken line) of the ROC curve is drawn.\n\n\n## Draw ROC curve\nROCR::plot(ROCRperf1, colorize = TRUE, print.cutoffs.at = seq(0.1, 0.9, 0.1), lwd = 2)\n\n## Draw convex hull of ROC curve\nch = ROCR::performance(ROCRpred1, \nrch\n)\nROCR::plot(ch, add = TRUE, lty = 2)\n\n\n\n\n \n\n\nExample 2: Benchmark experiments (continued)\n\n\nWe again consider the benchmark experiment conducted earlier.\nWe first extract the predictions by \ngetBMRPredictions\n and then convert them via function\n\nasROCRPrediction\n.\n\n\n## Extract predictions\npreds = getBMRPredictions(res)[[1]]\n\n## Convert predictions\nROCRpreds = lapply(preds, asROCRPrediction)\n\n## Calculate true and false positive rate\nROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x, \ntpr\n, \nfpr\n))\n\n\n\n\nWe draw the horizontally averaged ROC curves (solid lines) as well as the ROC curves for\nthe individual resampling iterations (broken lines).\nMoreover, standard error bars are plotted for selected true positive rates (0.1, 0.2, ..., 0.9).\nSee \nROCR\n's \nplot\n function for details.\n\n\n## lda average ROC curve\nplot(ROCRperfs[[1]], col = \nblue\n, avg = \nhorizontal\n, spread.estimate = \nstderror\n,\n  show.spread.at = seq(0.1, 0.9, 0.1), plotCI.col = \nblue\n, plotCI.lwd = 2, lwd = 2)\n## lda individual ROC curves\nplot(ROCRperfs[[1]], col = \nblue\n, lty = 2, lwd = 0.25, add = TRUE)\n\n## ksvm average ROC curve\nplot(ROCRperfs[[2]], col = \nred\n, avg = \nhorizontal\n, spread.estimate = \nstderror\n,\n  show.spread.at = seq(0.4, 0.9, 0.1), plotCI.col = \nred\n, plotCI.lwd = 2, lwd = 2, add = TRUE)\n## ksvm individual ROC curves\nplot(ROCRperfs[[2]], col = \nred\n, lty = 2, lwd = 0.25, add = TRUE)\n\nlegend(\nbottomright\n, legend = getBMRLearnerIds(res), lty = 1, lwd = 2, col = c(\nblue\n, \nred\n))\n\n\n\n\n \n\n\nIn order to create other evaluation plots like \nprecision/recall graphs\n you just have to change\nthe performance measures when calling \nROCR::performance\n.\n\n\n## Extract and convert predictions\npreds = getBMRPredictions(res)[[1]]\nROCRpreds = lapply(preds, asROCRPrediction)\n\n## Calculate precision and recall\nROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x, \nprec\n, \nrec\n))\n\n## Draw performance plot\nplot(ROCRperfs[[1]], col = \nblue\n, avg = \nthreshold\n)\nplot(ROCRperfs[[2]], col = \nred\n, avg = \nthreshold\n, add = TRUE)\nlegend(\nbottomleft\n, legend = getBMRLearnerIds(res), lty = 1, col = c(\nblue\n, \nred\n))\n\n\n\n\n \n\n\nIf you want to plot a performance measure versus the threshold, specify only one measure when\ncalling \nROCR::performance\n.\nBelow the average accuracy over the 5 cross-validation iterations is plotted against the\nthreshold. Moreover, boxplots for certain threshold values (0.1, 0.2, ..., 0.9) are drawn.\n\n\n## Extract and convert predictions\npreds = getBMRPredictions(res)[[1]]\nROCRpreds = lapply(preds, asROCRPrediction)\n\n## Calculate accuracy\nROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x, \nacc\n))\n\n## Plot accuracy versus threshold\nplot(ROCRperfs[[1]], avg = \nvertical\n, spread.estimate = \nboxplot\n, lwd = 2, col = \nblue\n,\n  show.spread.at = seq(0.1, 0.9, 0.1), ylim = c(0,1), xlab = \nThreshold\n)\n\n\n\n\n \n\n\nViper charts\n\n\nmlr\n also supports \nViperCharts\n for plotting ROC and other performance\ncurves. Like \nplotROCRCurves\n it has S3 methods for objects of class \nPrediction\n,\n\nResampleResult\n and \nBenchmarkResult\n.\nBelow plots for the benchmark experiment (Example 2) are generated.\n\n\nz = plotViperCharts(res, chart = \nrocc\n, browse = FALSE)\n\n\n\n\nYou can see the plot created this way \nhere\n.\nNote that besides ROC curves you get several other plots like lift charts or cost curves.\nFor details, see \nplotViperCharts\n.", 
            "title": "ROC Analysis"
        }, 
        {
            "location": "/roc_analysis/index.html#roc-analysis-and-performance-curves", 
            "text": "For binary scoring classifiers a  threshold  (in the following also called  cutoff ) value\ncontrols how predicted posterior probabilities are turned into class labels.\nROC curves and other performance plots serve to visualize and analyse the relationship between\none or two performance measures and the threshold.  This section is mainly devoted to  receiver operating characteristic  (ROC) curves that\nplot the  true positive rate  (sensitivity) on the vertical axis against the  false positive rate \n(1 - specificity, fall-out) on the horizontal axis for all possible threshold values.\nCreating other performance plots like  lift charts  or  precision/recall graphs  works\nanalogously and is only shown briefly.  In addition to performance visualization ROC curves are helpful in   determining an optimal decision threshold for given class prior probabilities and\n  misclassification costs (for alternatives see also the sections about\n   cost-sensitive classification  and\n   imbalanced classification problems  in this tutorial),  identifying regions where one classifier outperforms another and building suitable multi-classifier\n  systems,  obtaining calibrated estimates of the posterior probabilities.   For more information see the tutorials and introductory papers by Fawcett (2004) , Fawcett (2006) \nas well as  Flach (ICML 2004) .  In many applications as, e.g., diagnostic tests or spam detection, there is uncertainty\nabout the class priors or the misclassification costs at the time of prediction, for example\nbecause it's hard to quantify the costs or because costs and class priors vary over time.\nUnder these circumstances the classifier is expected to work well for a whole range of\ndecision thresholds and the area under the ROC curve (AUC) provides a scalar performance\nmeasure for comparing and selecting classifiers. mlr  provides the AUC for binary classification ( auc  based on package ROCR ) an also a generalization of the AUC for the\nmulti-class case ( multiclass.auc  based on package  pROC ).  mlr  offers three ways to plot ROC and other performance curves.   mlr 's function  generateROCRCurvesData  is a convenient interface to  ROCR 's\n    performance  methods with an associated plotting function,\n    plotROCRCurves  which uses  ggplot2 .  The  mlr  function  asROCRPrediction  converts  mlr   Prediction  objects to objects\n   of  ROCR 's class  prediction .\n   Then,  ROCR 's functionality can be used to further analyse the results and generate\n   performance plots.  mlr 's function  plotViperCharts  provides an interface to\n    ViperCharts .   Let's have a look at some examples demonstrating the three possible methods.  Note that the  learners  have to be capable of predicting probabilities.\nHave a look at the  table of learners \nor run  listLearners(prob = TRUE)  to get a list of all learners that support this.", 
            "title": "ROC Analysis and Performance Curves"
        }, 
        {
            "location": "/roc_analysis/index.html#performance-plots-with-generaterocrcurvesdata-and-plotrocrcurves", 
            "text": "As mentioned above  generateROCRCurvesData  is an interface to  ROCR 's performance  methods.\nIt provides S3 methods for objects of class  Prediction ,  ResampleResult \nand  BenchmarkResult  (resulting from calling  predict ,  resample \nor  benchmark ).  plotROCRCurves  plots output from  generateROCRCurvesData  using  ggplot2 .  Example 1: Single predictions  We consider the  Sonar  data set from package  mlbench , which poses a\nbinary classification problem ( sonar.task ) and apply  linear discriminant analysis .  lrn1 = makeLearner( classif.lda , predict.type =  prob )\nmod1 = train(lrn1, sonar.task)\npred1 = predict(mod1, task = sonar.task)\nroc_data = generateROCRCurvesData(pred1)\nroc_data\n#       learner False positive rate True positive rate    Cutoff\n#  1 prediction                   0        0.000000000 1.0048061\n#  2 prediction                   0        0.009009009 0.9999755\n#  3 prediction                   0        0.018018018 0.9998530\n#  4 prediction                   0        0.027027027 0.9998145\n#  5 prediction                   0        0.036036036 0.9997841  generateROCRCurvesData  returns an object of class \"ROCRCurvesData\" which contains the results from  ROCR 's  performance  method (which depends on arguments  meas1  and  meas2 ). The data can be extracted by accessing the data element of the object. The object also contains information about the input arguments to  generateROCRCurvesData  which may be useful.  Per default,  plotROCRCurves  draws a ROC curve and optionally adds a diagonal line that represents\nthe performance of a random classifier.  droc = generateROCRCurvesData(pred1)\nplotROCRCurves(droc, diagonal = TRUE)   \nThere is also an experimental plotting function  plotROCRCurvesGGVIS  which uses  ggvis  to create similar\nfigures with the addition of (optional) interactive tooltips (displayed on hover) that display the threshold\nat that point in the curve.  plotROCRCurvesGGVIS(droc, cutoffs = TRUE)  The corresponding area under curve ( auc ) can be calculated as usual by calling performance .  performance(pred1, auc)\n#        auc \n#  0.9717656  In addition to  linear discriminant analysis  we try a support vector machine\nwith RBF kernel ( ksvm ).  lrn2 = makeLearner( classif.ksvm , predict.type =  prob )\nmod2 = train(lrn2, sonar.task)\n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel\npred2 = predict(mod2, task = sonar.task)  In order to compare the performance of the two learners you might want to display the two\ncorresponding ROC curves in one plot.\nFor this purpose just pass a named  list  of  Prediction s to  plotROCRCurves .  plotROCRCurves(generateROCRCurvesData(list(lda = pred1, ksvm = pred2)))     It's clear from the plot above that  ksvm  has a slightly higher AUC than lda .  performance(pred2, auc)\n#       auc \n#  0.992477  It is easily possible to generate other performance plots by passing the appropriate performance\nmeasures to  plotROCRCurves .\nNote that arguments  meas1  and  meas2  do not refer to  mlr 's performance measures,\nbut to measures provided by  ROCR  and listed  here .\nBelow is code for a  lift chart  which shows the lift value ( \"lift\" ) versus the rate of\npositive predictions ( \"rpp\" ).  out = generateROCRCurvesData(list(lda = pred1, ksvm = pred2), meas1 =  lift , meas2 =  rpp )\nplotROCRCurves(out)     A plot of a single performance measure (accuracy in the example code below) versus the\nthreshold can be generated by setting  meas2 = \"cutoff\" .  out = generateROCRCurvesData(list(lda = pred1, ksvm = pred2), meas1 =  acc , meas2 =  cutoff )\nplotROCRCurves(out)     As you may recall, an alternative function for plotting performance values versus the decision\nthreshold is  plotThreshVsPerf .\nWhile  plotThreshVsPerf  permits to plot several performance measures at once, plotROCRCurves  makes it easy to superpose the performance curves of multiple learners.  Example 2: Benchmark experiment  The analysis in the example above can be improved in several regards.\nWe only considered the training performance and, ideally, the support vector machine should\nhave been  tuned .\nMoreover, we wrote individual code for training/prediction of each learner, which can become\ntedious very quickly.\nA more practical way to apply several learners to a  Task  and compare their performance is\nprovided by function  benchmark  (see also  Benchmark Experiments ).  We again consider the  Sonar  data set and apply  lda \nas well as  ksvm .\nWe first generate a  tuning wrapper  for  ksvm .\nThe cost parameter is tuned on a (for demonstration purposes small) parameter grid.\nWe assume that we are interested in a good performance over the complete threshold range\nand therefore tune with regard to the  auc .\nThe error rate ( mmce ) for threshold 0.5 is reported as well.  ## Tune wrapper for ksvm\nrdesc.inner = makeResampleDesc( Holdout )\nms = list(auc, mmce)\nps = makeParamSet(\n  makeDiscreteParam( C , 2^(-1:1))\n)\nctrl = makeTuneControlGrid()\nlrn2 = makeTuneWrapper(lrn2, rdesc.inner, ms, ps, ctrl, show.info = FALSE)  Below the actual benchmark experiment is conducted.\nAs resampling strategy we use 5-fold cross-validation and again calculate the  auc \nas well as the error rate (for a threshold/cutoff value of 0.5).  ## Benchmark experiment\nlrns = list(lrn1, lrn2)\nrdesc.outer = makeResampleDesc( CV , iters = 5)\n\nres = benchmark(lrns, tasks = sonar.task, resampling = rdesc.outer, measures = ms, show.info = FALSE)\n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel \n#  Using automatic sigma estimation (sigest) for RBF or laplace kernel\nres\n#          task.id         learner.id auc.test.mean mmce.test.mean\n#  1 Sonar-example        classif.lda     0.7835442      0.2592334\n#  2 Sonar-example classif.ksvm.tuned     0.9454418      0.1390244  Calling  plotROCRCurves  on the  result  of the benchmark experiment\nproduces a plot with ROC curves for all learners in the experiment.  plotROCRCurves(generateROCRCurvesData(res))     Per default, threshold-averaged ROC curves are shown.\nSince we used 5-fold cross-validation we have predictions on 5 test data sets and therefore\n5 ROC curves per classifier.\nFor each threshold value the means of the corresponding 5 false and true positive rates are\ncalculated and plotted against each other.  If you want to plot the individual ROC curves for each resample iteration set  avg = \"none\" .\nOther available options are  avg = \"horizontal\"  and  avg = \"vertical\" .  plotROCRCurves(generateROCRCurvesData(res, avg =  none ))     An alternative to averaging is to just merge the 5 test folds and draw a single ROC curve.\nMerging can be achieved by manually changing the  class  attribute of\nthe prediction objects from  ResamplePrediction  to  Prediction .  Below the predictions are extracted from the  BenchmarkResult  via function  getBMRPredictions ,\nthe  class  is changed and the ROC curves are created.  Averaging methods are normally preferred\n(cp.  Fawcett, 2006 ),\nas they permit to assess the variability, which is needed to properly compare classifier\nperformance.  ## Extract predictions\npreds = getBMRPredictions(res)[[1]]\n\n## Change the class attribute\npreds2 = lapply(preds, function(x) {class(x) =  Prediction ; return(x)})\n\n## Draw ROC curves\nplotROCRCurves(generateROCRCurvesData(preds2, avg =  none ))     Again, you can easily create other standard evaluation plots by calling  plotROCRCurves \non the  BenchmarkResult  with the appropriate performance measures (see  ROCR::performance ).", 
            "title": "Performance plots with generateROCRCurvesData and plotROCRCurves"
        }, 
        {
            "location": "/roc_analysis/index.html#performance-plots-with-asrocrprediction", 
            "text": "Drawing performance plots with package  ROCR  works through three basic commands:   ROCR::prediction : Create a  ROCR   prediction  object.  ROCR::performance : Calculate one or more performance measures for the\n   given prediction object.  A reimplementation of  ROCR::plot  which uses  ggplot2 .   mlr 's function  asROCRPrediction  converts an  mlr   Prediction  object to\na  ROCR   prediction  object.\nIn order to create performance plots steps 2. and 3. have to be run by the user.  This is obviously less convenient than calling  plotROCRCurves  (which extracts predictions,\ncalls  asROCRPrediction  and executes steps 2. and 3. internally).\nOn the other hand this way provides more control over the generated plots by, e.g., using graphical\nparameters that are not (yet) accessible via  plotROCRCurves .\nMoreover, you can directly benefit from any enhancements in  ROCR , use your own ROCR -based code or other packages that depend on  ROCR , and use  ROCR 's  plot .\nFor more details see the  ROCR  documentation and  demo(ROCR) .  An addditional alternative is to call  plotROCRCurves , extract the data from the  ggplot2  object using the  data  element of the object, e.g.,  obj$data , and then plot the data using whatever method you prefer.  Example 1: Single predictions (continued)  We go back to out first example where we trained and predicted  lda  on the sonar classification task .  ## Train and predict linear discriminant analysis\nlrn1 = makeLearner( classif.lda , predict.type =  prob )\nmod1 = train(lrn1, sonar.task)\npred1 = predict(mod1, task = sonar.task)  Below we use  asROCRPrediction  to convert the lda prediction, let  ROCR  calculate the\ntrue and false positive rate and plot the ROC curve.  ## Convert prediction\nROCRpred1 = asROCRPrediction(pred1)\n\n## Calculate true and false positive rate\nROCRperf1 = ROCR::performance(ROCRpred1,  tpr ,  fpr )\n\n## Draw ROC curve\nROCR::plot(ROCRperf1)     Below is the same ROC curve, but we make use of some more graphical parameters:\nThe ROC curve is color-coded by the threshold and selected threshold values are printed on\nthe curve. Additionally, the convex hull (black broken line) of the ROC curve is drawn.  ## Draw ROC curve\nROCR::plot(ROCRperf1, colorize = TRUE, print.cutoffs.at = seq(0.1, 0.9, 0.1), lwd = 2)\n\n## Draw convex hull of ROC curve\nch = ROCR::performance(ROCRpred1,  rch )\nROCR::plot(ch, add = TRUE, lty = 2)     Example 2: Benchmark experiments (continued)  We again consider the benchmark experiment conducted earlier.\nWe first extract the predictions by  getBMRPredictions  and then convert them via function asROCRPrediction .  ## Extract predictions\npreds = getBMRPredictions(res)[[1]]\n\n## Convert predictions\nROCRpreds = lapply(preds, asROCRPrediction)\n\n## Calculate true and false positive rate\nROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x,  tpr ,  fpr ))  We draw the horizontally averaged ROC curves (solid lines) as well as the ROC curves for\nthe individual resampling iterations (broken lines).\nMoreover, standard error bars are plotted for selected true positive rates (0.1, 0.2, ..., 0.9).\nSee  ROCR 's  plot  function for details.  ## lda average ROC curve\nplot(ROCRperfs[[1]], col =  blue , avg =  horizontal , spread.estimate =  stderror ,\n  show.spread.at = seq(0.1, 0.9, 0.1), plotCI.col =  blue , plotCI.lwd = 2, lwd = 2)\n## lda individual ROC curves\nplot(ROCRperfs[[1]], col =  blue , lty = 2, lwd = 0.25, add = TRUE)\n\n## ksvm average ROC curve\nplot(ROCRperfs[[2]], col =  red , avg =  horizontal , spread.estimate =  stderror ,\n  show.spread.at = seq(0.4, 0.9, 0.1), plotCI.col =  red , plotCI.lwd = 2, lwd = 2, add = TRUE)\n## ksvm individual ROC curves\nplot(ROCRperfs[[2]], col =  red , lty = 2, lwd = 0.25, add = TRUE)\n\nlegend( bottomright , legend = getBMRLearnerIds(res), lty = 1, lwd = 2, col = c( blue ,  red ))     In order to create other evaluation plots like  precision/recall graphs  you just have to change\nthe performance measures when calling  ROCR::performance .  ## Extract and convert predictions\npreds = getBMRPredictions(res)[[1]]\nROCRpreds = lapply(preds, asROCRPrediction)\n\n## Calculate precision and recall\nROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x,  prec ,  rec ))\n\n## Draw performance plot\nplot(ROCRperfs[[1]], col =  blue , avg =  threshold )\nplot(ROCRperfs[[2]], col =  red , avg =  threshold , add = TRUE)\nlegend( bottomleft , legend = getBMRLearnerIds(res), lty = 1, col = c( blue ,  red ))     If you want to plot a performance measure versus the threshold, specify only one measure when\ncalling  ROCR::performance .\nBelow the average accuracy over the 5 cross-validation iterations is plotted against the\nthreshold. Moreover, boxplots for certain threshold values (0.1, 0.2, ..., 0.9) are drawn.  ## Extract and convert predictions\npreds = getBMRPredictions(res)[[1]]\nROCRpreds = lapply(preds, asROCRPrediction)\n\n## Calculate accuracy\nROCRperfs = lapply(ROCRpreds, function(x) ROCR::performance(x,  acc ))\n\n## Plot accuracy versus threshold\nplot(ROCRperfs[[1]], avg =  vertical , spread.estimate =  boxplot , lwd = 2, col =  blue ,\n  show.spread.at = seq(0.1, 0.9, 0.1), ylim = c(0,1), xlab =  Threshold )", 
            "title": "Performance plots with asROCRPrediction"
        }, 
        {
            "location": "/roc_analysis/index.html#viper-charts", 
            "text": "mlr  also supports  ViperCharts  for plotting ROC and other performance\ncurves. Like  plotROCRCurves  it has S3 methods for objects of class  Prediction , ResampleResult  and  BenchmarkResult .\nBelow plots for the benchmark experiment (Example 2) are generated.  z = plotViperCharts(res, chart =  rocc , browse = FALSE)  You can see the plot created this way  here .\nNote that besides ROC curves you get several other plots like lift charts or cost curves.\nFor details, see  plotViperCharts .", 
            "title": "Viper charts"
        }, 
        {
            "location": "/learning_curve/index.html", 
            "text": "Learning Curve Analysis\n\n\nTo analyse how the increase of observations in the training set improves the performance of\na learner the \nlearning curve\n is an appropriate visual tool.\nThe experiment is conducted with an increasing subsample size and the performance is measured.\nIn the plot the x-axis represents the relative subsample size whereas the y-axis represents\nthe performance.\n\n\nNote that this function internally uses \nbenchmark\n in combination with \nmakeDownsampleWrapper\n,\nso for every run new observations are drawn.\nThus the results are noisy.\nTo reduce noise increase the number of resampling iterations.\nYou can define the resampling method in the \nresampling\n argument of \ngenerateLearningCurveData\n.\nIt is also possible to pass a \nResampleInstance\n (which is a result\nof \nmakeResampleInstance\n) to make resampling consistent for all passed learners and each\nstep of increasing the number of observations.\n\n\nPlotting the learning curve\n\n\nThe \nmlr\n function \ngenerateLearningCurveData\n can generate the data for \nlearning curves\n\nfor multiple \nlearners\n and multiple \nperformance measures\n\nat once.\nWith \nplotLearningCurve\n the result of \ngenerateLearningCurveData\n can be plotted using \nggplot2\n.\n\nplotLearningCurve\n has an argument \nfacet\n which can be either \"measure\" or \"learner\". By default\n\nfacet = \"measure\"\n and facetted subplots are created for each measure input to \ngenerateLearningCurveData\n.\nIf \nfacet = \"measure\"\n learners are mapped to color, and vice versa.\n\n\nr = generateLearningCurveData(\n  learners = list(\nclassif.rpart\n, \nclassif.knn\n),\n  task = sonar.task,\n  percs = seq(0.1, 1, by = 0.2),\n  measures = list(tp, fp, tn, fn),\n  resampling = makeResampleDesc(method = \nCV\n, iters = 5),\n  show.info = FALSE)\nplotLearningCurve(r)\n\n\n\n\n \n\n\nWhat happens in \ngenerateLearningCurveData\n is the following:\nEach learner will be internally wrapped in a \nDownsampleWrapper\n.\nTo measure the performance at the first step of \npercs\n, say \n0.1\n, first the data will be\nsplit into a \ntraining\n and a \ntest set\n according to the given \nresampling strategy\n.\nThen a random sample containing 10% of the observations of the \ntraining set\n will be drawn\nand used to train the learner.\nThe performance will be measured on the \ncomplete test set\n.\nThese steps will be repeated as defined by the given \nresampling method\n and for each value\nof \npercs\n.\n\n\nIn the first example a simplified usage of the \nlearners\n argument was used, so that it's\nsufficient to give the \nname\n.\nIt is also possible to create a learner the usual way and even to mix it.\nMake sure to give different \nid\ns in this case.\n\n\nlrns = list(\n  makeLearner(cl = \nclassif.ksvm\n, id = \nksvm1\n , sigma = 0.2, C = 2),\n  makeLearner(cl = \nclassif.ksvm\n, id = \nksvm2\n , sigma = 0.1, C = 1),\n  \nclassif.randomForest\n\n)\nrin = makeResampleDesc(method = \nCV\n, iters = 5)\nlc = generateLearningCurveData(learners = lrns, task = sonar.task,\n                               percs = seq(0.1, 1, by = 0.1), measures = acc,\n                               resampling = rin, show.info = FALSE)\nplotLearningCurve(lc)\n\n\n\n\n \n\n\nThere is also an experimental \nggvis\n plotting function, \nplotLearningCurveGGVIS\n. Instead of the \nfacet\n\nargument to \nplotLearningCurve\n there is an argument \ninteractive\n which plays a similar role. As subplots\nare not available in \nggvis\n, measures or learners are mapped to an interactive sidebar which allows selection\nof the displayed measures or learners. The other feature is mapped to color.\n\n\nplotLearningCurveGGVIS(r, interactive = \nmeasure\n)", 
            "title": "Learning Curve"
        }, 
        {
            "location": "/learning_curve/index.html#learning-curve-analysis", 
            "text": "To analyse how the increase of observations in the training set improves the performance of\na learner the  learning curve  is an appropriate visual tool.\nThe experiment is conducted with an increasing subsample size and the performance is measured.\nIn the plot the x-axis represents the relative subsample size whereas the y-axis represents\nthe performance.  Note that this function internally uses  benchmark  in combination with  makeDownsampleWrapper ,\nso for every run new observations are drawn.\nThus the results are noisy.\nTo reduce noise increase the number of resampling iterations.\nYou can define the resampling method in the  resampling  argument of  generateLearningCurveData .\nIt is also possible to pass a  ResampleInstance  (which is a result\nof  makeResampleInstance ) to make resampling consistent for all passed learners and each\nstep of increasing the number of observations.", 
            "title": "Learning Curve Analysis"
        }, 
        {
            "location": "/learning_curve/index.html#plotting-the-learning-curve", 
            "text": "The  mlr  function  generateLearningCurveData  can generate the data for  learning curves \nfor multiple  learners  and multiple  performance measures \nat once.\nWith  plotLearningCurve  the result of  generateLearningCurveData  can be plotted using  ggplot2 . plotLearningCurve  has an argument  facet  which can be either \"measure\" or \"learner\". By default facet = \"measure\"  and facetted subplots are created for each measure input to  generateLearningCurveData .\nIf  facet = \"measure\"  learners are mapped to color, and vice versa.  r = generateLearningCurveData(\n  learners = list( classif.rpart ,  classif.knn ),\n  task = sonar.task,\n  percs = seq(0.1, 1, by = 0.2),\n  measures = list(tp, fp, tn, fn),\n  resampling = makeResampleDesc(method =  CV , iters = 5),\n  show.info = FALSE)\nplotLearningCurve(r)     What happens in  generateLearningCurveData  is the following:\nEach learner will be internally wrapped in a  DownsampleWrapper .\nTo measure the performance at the first step of  percs , say  0.1 , first the data will be\nsplit into a  training  and a  test set  according to the given  resampling strategy .\nThen a random sample containing 10% of the observations of the  training set  will be drawn\nand used to train the learner.\nThe performance will be measured on the  complete test set .\nThese steps will be repeated as defined by the given  resampling method  and for each value\nof  percs .  In the first example a simplified usage of the  learners  argument was used, so that it's\nsufficient to give the  name .\nIt is also possible to create a learner the usual way and even to mix it.\nMake sure to give different  id s in this case.  lrns = list(\n  makeLearner(cl =  classif.ksvm , id =  ksvm1  , sigma = 0.2, C = 2),\n  makeLearner(cl =  classif.ksvm , id =  ksvm2  , sigma = 0.1, C = 1),\n   classif.randomForest \n)\nrin = makeResampleDesc(method =  CV , iters = 5)\nlc = generateLearningCurveData(learners = lrns, task = sonar.task,\n                               percs = seq(0.1, 1, by = 0.1), measures = acc,\n                               resampling = rin, show.info = FALSE)\nplotLearningCurve(lc)     There is also an experimental  ggvis  plotting function,  plotLearningCurveGGVIS . Instead of the  facet \nargument to  plotLearningCurve  there is an argument  interactive  which plays a similar role. As subplots\nare not available in  ggvis , measures or learners are mapped to an interactive sidebar which allows selection\nof the displayed measures or learners. The other feature is mapped to color.  plotLearningCurveGGVIS(r, interactive =  measure )", 
            "title": "Plotting the learning curve"
        }, 
        {
            "location": "/create_learner/index.html", 
            "text": "Integrating another learner\n\n\nIn order to create a new learner in \nmlr\n, interface code to the \nR\n function\nmust be written. Three functions are required for each learner. First, you must\ndefine the learner itself with a name, description, capabilities, parameters,\nand a few other things. Second, you need to provide a function that calls the\nlearner function and builds the model given data. Finally, a prediction function\nthat returns predicted values given new data is needed.\n\n\nAll learners should inherit from \nrlearner.classif\n, \nrlearner.regr\n,\n\nrlearner.surv\n, \nrlearner.costsens\n, or \nrlearner.cluster\n. While it is\nalso possible to define a new type of learner that has special properties and\ndoes not fit into one of the existing schemes, this is much more advanced and\nnot covered here.\n\n\nClassification\n\n\nWe show how the \nLinear Discriminant Analysis\n from\npackage \nMASS\n has been integrated\ninto the classification learner \nclassif.lda\n in \nmlr\n as an example.\n\n\nDefinition of the learner\n\n\nThe minimal information required to define a learner is the \nmlr\n name of the\nlearner, its package, the parameter set, and the set of properties of your\nlearner. In addition, you may provide a human-readable name, a short name and a\nnote with information relevant to users of the learner.\n\n\nFirst, name your learner. The naming conventions in \nmlr\n are\n\nclassif.\nR_method_name\n for classification, \nregr.\nR_method_name\n for\nregression, \nsurv.\nR_method_name\n for survival analysis,\n\ncostsens.\nR_method_name\n for cost sensitive learning, and\n\ncluster.\nR_method_name\n for clustering. So in this example, the name starts with\n\nclassif.\n and we choose \nclassif.lda\n.\n\n\nSecond, we need to define the parameters of the learner. These are any options\nthat can be set when running it to change how it learns, how input is\ninterpreted, how and what output is generated, and so on. \nmlr\n provides a\nnumber of functions to define parameters, a complete list can be found in the\ndocumentation of \nLearnerParam\n of the\n\nParamHelpers\n package.\n\n\nIn our example, we have discrete and numeric parameters, so we use\n\nmakeDiscreteLearnerParam\n and\n\nmakeNumericLearnerParam\n to incorporate the\ncomplete description of the parameters. We include all possible values for\ndiscrete parameters and lower and upper bounds for numeric parameters. Strictly\nspeaking it is not necessary to provide bounds for all parameters and if this\ninformation is not available they can be estimated, but providing accurate and\nspecific information here makes it possible to tune the learner much better (see\nthe section on \ntuning\n).\n\n\nNext, we add information on the properties of the learner (see also the section\non \nlearners\n). Which types of features are supported (numerics,\nfactors)? Are case weights supported? Can the method deal with missing values in\nthe features and deal with NA's in a meaningful way (not \nna.omit\n)? Are\none-class, two-class, multi-class problems supported? Can the learner predict\nposterior probabilities?\n\n\nBelow is the complete code for the definition of the LDA learner. It has one\ndiscrete parameter, \nmethod\n, and two continuous ones, \nnu\n and \ntol\n. It\nsupports classification problems with two or more classes and can deal with\nnumeric and factor explanatory variables. It can predict posterior\nprobabilities.\n\n\nmakeRLearner.classif.lda = function() {\n  makeRLearnerClassif(\n    cl = \nclassif.lda\n,\n    package = \nMASS\n,\n    par.set = makeParamSet(\n      makeDiscreteLearnerParam(id = \nmethod\n, default = \nmoment\n, values = c(\nmoment\n, \nmle\n, \nmve\n, \nt\n)),\n      makeNumericLearnerParam(id = \nnu\n, lower = 2, requires = expression(method == \nt\n)),\n      makeNumericLearnerParam(id = \ntol\n, default = 1.0e-4, lower = 0)\n    ),\n    properties = c(\ntwoclass\n, \nmulticlass\n, \nnumerics\n, \nfactors\n, \nprob\n)\n  )\n}\n\n\n\n\nCreating the training function of the learner\n\n\nOnce the learner has been defined, we need to tell \nmlr\n how to call it to\ntrain a model. The name of the function has to start with \ntrainLearner.\n,\nfollowed by the \nmlr\n name of the learner as defined above (\nclassif.lda\n\nhere). The prototype of the function looks as follows.\n\n\nfunction(.learner, .task, .subset, .weights = NULL, ...) { }\n\n\n\n\nThis function must fit a model on the data of the task \n.task\n with regard to\nthe subset defined in the integer vector \n.subset\n and the parameters passed\nin the \n...\n arguments. Usually, the data should be extracted from the task\nusing \ngetTaskData\n. This will take care of any subsetting as well. It must\nreturn the fitted model. \nmlr\n assumes no special data type for the return\nvalue -- it will be passed to the predict function we are going to define below,\nso any special code the learner may need can be encapsulated there.\n\n\nFor our example, the definition of the function looks like this. In addition to\nthe data of the task, we also need the formula that describes what to predict.\nWe use the function \ngetTaskFormula\n to extract this from the task.\n\n\ntrainLearner.classif.lda = function(.learner, .task, .subset, .weights,  ...) {\n  f = getTaskFormula(.task)\n  lda(f, data = getTaskData(.task, .subset), ...)\n}\n\n\n\n\nCreating the prediction method\n\n\nFinally, the prediction function needs to be defined. The name of this function\nstarts with \npredictLearner.\n, followed again by the \nmlr\n name of the\nlearner. The prototype of the function is as follows.\n\n\nfunction(.learner, .model, .newdata, ...) { }\n\n\n\n\nIt must predict for the new observations in the \ndata.frame\n \n.newdata\n with\nthe wrapped model \n.model\n, which is returned from the training function.\nThe actual model the learner built is stored in the \n$learner.model\n member\nand can be accessed simply through \n.model$learner.model\n.\n\n\nFor classification, you have to return a factor of predicted classes if\n\n.learner$predict.type\n is \n\"response\"\n, or a matrix of predicted\nprobabilities if \n.learner$predict.type\n is \n\"prob\"\n and this type of\nprediction is supported by the learner. In the latter case the matrix must have\nthe same number of columns as there are classes in the task and the columns have\nto be named by the class names.\n\n\nThe definition for LDA looks like this. It is pretty much just a straight\npass-through of the arguments to the \npredict\n function and some extraction of\nprediction data depending on the type of prediction requested.\n\n\npredictLearner.classif.lda = function(.learner, .model, .newdata, ...) {\n  p = predict(.model$learner.model, newdata = .newdata, ...)\n  if(.learner$predict.type == \nresponse\n)\n    return(p$class)\n  else\n    return(p$posterior)\n}\n\n\n\n\nRegression\n\n\nThe main difference for regression is that the type of predictions are different\n(numeric instead of labels or probabilities) and that not all of the properties\nare relevant. In particular, whether one-, two-, or multi-class problems and\nposterior probabilities are supported is not applicable.\n\n\nApart from this, everything explained above applies. Below is the definition for\nthe \nearth\n learner from the\n\nearth\n package.\n\n\nmakeRLearner.regr.earth = function() {\n  makeRLearnerRegr(\n    cl = \nregr.earth\n,\n    package = \nearth\n,\n    par.set = makeParamSet(\n      makeIntegerLearnerParam(id = \ndegree\n, default = 1L, lower = 1L),\n      makeNumericLearnerParam(id = \npenalty\n),\n      makeIntegerLearnerParam(id = \nnprune\n)\n    ),\n    properties = c(\nnumerics\n, \nfactors\n),\n    name = \nMultivariate Adaptive Regression Splines\n,\n    short.name = \nearth\n,\n    note = \n\n  )\n}\n\ntrainLearner.regr.earth = function(.learner, .task, .subset, .weights = NULL,  ...) {\n  f = getTaskFormula(.task)\n  earth::earth(f, data = getTaskData(.task, .subset), ...)\n}\n\npredictLearner.regr.earth = function(.learner, .model, .newdata, ...) {\n  predict(.model$learner.model, newdata = .newdata)[, 1L]\n}\n\n\n\n\nAgain most of the data is passed straight through to/from the train/predict\nfunctions of the learner.\n\n\nSurvival Analysis\n\n\nFor survival analysis, you have to return so-called linear predictors in order to compute\nthe default measure for this task type, the \ncindex\n (for\n\n.learner$predict.type\n == \n\"response\"\n). For \n.learner$predict.type\n == \n\"prob\"\n,\nthere is no substantially meaningful measure (yet). You may either ignore this case or return\nsomething like predicted survival curves (cf. example below).\n\n\nThere are three properties that are specific to survival learners:\n\"rcens\", \"lcens\" and \"icens\", defining the type(s) of censoring a learner can handle -- right,\nleft and/or interval censored.\n\n\nLet's have a look at how the \nCox Proportional Hazard Model\n from\npackage \nsurvival\n has been integrated\ninto the survival learner \nsurv.coxph\n in \nmlr\n as an example:\n\n\nmakeRLearner.surv.coxph = function() {\n  makeRLearnerSurv(\n    cl = \nsurv.coxph\n,\n    package = \nsurvival\n,\n    par.set = makeParamSet(\n      makeDiscreteLearnerParam(id = \nties\n, default = \nefron\n, values = c(\nefron\n, \nbreslow\n, \nexact\n)),\n      makeLogicalLearnerParam(id = \nsingular.ok\n, default = TRUE),\n      makeNumericLearnerParam(id = \neps\n, default = 1e-09, lower = 0),\n      makeNumericLearnerParam(id = \ntoler.chol\n, default = .Machine$double.eps^0.75, lower = 0),\n      makeIntegerLearnerParam(id = \niter.max\n, default = 20L, lower = 1L),\n      makeNumericLearnerParam(id = \ntoler.inf\n, default = sqrt(.Machine$double.eps^0.75), lower = 0),\n      makeIntegerLearnerParam(id = \nouter.max\n, default = 10L, lower = 1L)\n    ),\n    properties = c(\nmissings\n, \nnumerics\n, \nfactors\n, \nweights\n, \nprob\n, \nrcens\n),\n    name = \nCox Proportional Hazard Model\n,\n    short.name = \ncoxph\n,\n    note = \n\n  )\n}\n\ntrainLearner.surv.coxph = function(.learner, .task, .subset, .weights = NULL,  ...) {\n  f = as.formula(getTaskFormulaAsString(.task))\n  data = getTaskData(.task, subset = .subset)\n  if (is.null(.weights)) {\n    mod = survival::coxph(formula = f, data = data, ...)\n  } else  {\n    mod = survival::coxph(formula = f, data = data, weights = .weights, ...)\n  }\n  if (.learner$predict.type == \nprob\n)\n    mod = attachTrainingInfo(mod, list(surv.range = range(getTaskTargets(.task)[, 1L])))\n  mod\n}\n\npredictLearner.surv.coxph = function(.learner, .model, .newdata, ...) {\n  if(.learner$predict.type == \nresponse\n) {\n    predict(.model$learner.model, newdata = .newdata, type = \nlp\n, ...)\n  } else if (.learner$predict.type == \nprob\n) {\n    surv.range = getTrainingInfo(.model$learner.model)$surv.range\n    times = seq(from = surv.range[1L], to = surv.range[2L], length.out = 1000)\n    t(summary(survival::survfit(.model$learner.model, newdata = .newdata, se.fit = FALSE, conf.int = FALSE), times = times)$surv)\n  } else {\n    stop(\nUnknown predict type\n)\n  }\n}\n\n\n\n\nClustering\n\n\nFor clustering, you have to return a numeric vector with the IDs of the clusters\nthat the respective datum has been assigned to. The numbering should start at 1.\n\n\nBelow is the definition for the \nFarthestFirst\n learner\nfrom the \nRWeka\n package. Weka\nstarts the IDs of the clusters at 0, so we add 1 to the predicted clusters.\nRWeka has a different way of setting learner parameters; we use the special\n\nWeka_control\n function to do this.\n\n\nmakeRLearner.cluster.FarthestFirst = function() {\n  makeRLearnerCluster(\n    cl = \ncluster.FarthestFirst\n,\n    package = \nRWeka\n,\n    par.set = makeParamSet(\n      makeIntegerLearnerParam(id = \nN\n, default = 2L, lower = 1L),\n      makeIntegerLearnerParam(id = \nS\n, default = 1L, lower = 1L)\n    ),\n    properties = c(\nnumerics\n),\n    name = \nFarthestFirst Clustering Algorithm\n,\n    short.name = \nfarthestfirst\n\n  )\n}\n\ntrainLearner.cluster.FarthestFirst = function(.learner, .task, .subset, .weights = NULL,  ...) {\n  ctrl = RWeka::Weka_control(...)\n  RWeka::FarthestFirst(getTaskData(.task, .subset), control = ctrl)\n}\n\npredictLearner.cluster.FarthestFirst = function(.learner, .model, .newdata, ...) {\n  predict(.model$learner.model, .newdata, ...) + 1\n}", 
            "title": "Create Custom Learners"
        }, 
        {
            "location": "/create_learner/index.html#integrating-another-learner", 
            "text": "In order to create a new learner in  mlr , interface code to the  R  function\nmust be written. Three functions are required for each learner. First, you must\ndefine the learner itself with a name, description, capabilities, parameters,\nand a few other things. Second, you need to provide a function that calls the\nlearner function and builds the model given data. Finally, a prediction function\nthat returns predicted values given new data is needed.  All learners should inherit from  rlearner.classif ,  rlearner.regr , rlearner.surv ,  rlearner.costsens , or  rlearner.cluster . While it is\nalso possible to define a new type of learner that has special properties and\ndoes not fit into one of the existing schemes, this is much more advanced and\nnot covered here.", 
            "title": "Integrating another learner"
        }, 
        {
            "location": "/create_learner/index.html#classification", 
            "text": "We show how the  Linear Discriminant Analysis  from\npackage  MASS  has been integrated\ninto the classification learner  classif.lda  in  mlr  as an example.  Definition of the learner  The minimal information required to define a learner is the  mlr  name of the\nlearner, its package, the parameter set, and the set of properties of your\nlearner. In addition, you may provide a human-readable name, a short name and a\nnote with information relevant to users of the learner.  First, name your learner. The naming conventions in  mlr  are classif. R_method_name  for classification,  regr. R_method_name  for\nregression,  surv. R_method_name  for survival analysis, costsens. R_method_name  for cost sensitive learning, and cluster. R_method_name  for clustering. So in this example, the name starts with classif.  and we choose  classif.lda .  Second, we need to define the parameters of the learner. These are any options\nthat can be set when running it to change how it learns, how input is\ninterpreted, how and what output is generated, and so on.  mlr  provides a\nnumber of functions to define parameters, a complete list can be found in the\ndocumentation of  LearnerParam  of the ParamHelpers  package.  In our example, we have discrete and numeric parameters, so we use makeDiscreteLearnerParam  and makeNumericLearnerParam  to incorporate the\ncomplete description of the parameters. We include all possible values for\ndiscrete parameters and lower and upper bounds for numeric parameters. Strictly\nspeaking it is not necessary to provide bounds for all parameters and if this\ninformation is not available they can be estimated, but providing accurate and\nspecific information here makes it possible to tune the learner much better (see\nthe section on  tuning ).  Next, we add information on the properties of the learner (see also the section\non  learners ). Which types of features are supported (numerics,\nfactors)? Are case weights supported? Can the method deal with missing values in\nthe features and deal with NA's in a meaningful way (not  na.omit )? Are\none-class, two-class, multi-class problems supported? Can the learner predict\nposterior probabilities?  Below is the complete code for the definition of the LDA learner. It has one\ndiscrete parameter,  method , and two continuous ones,  nu  and  tol . It\nsupports classification problems with two or more classes and can deal with\nnumeric and factor explanatory variables. It can predict posterior\nprobabilities.  makeRLearner.classif.lda = function() {\n  makeRLearnerClassif(\n    cl =  classif.lda ,\n    package =  MASS ,\n    par.set = makeParamSet(\n      makeDiscreteLearnerParam(id =  method , default =  moment , values = c( moment ,  mle ,  mve ,  t )),\n      makeNumericLearnerParam(id =  nu , lower = 2, requires = expression(method ==  t )),\n      makeNumericLearnerParam(id =  tol , default = 1.0e-4, lower = 0)\n    ),\n    properties = c( twoclass ,  multiclass ,  numerics ,  factors ,  prob )\n  )\n}  Creating the training function of the learner  Once the learner has been defined, we need to tell  mlr  how to call it to\ntrain a model. The name of the function has to start with  trainLearner. ,\nfollowed by the  mlr  name of the learner as defined above ( classif.lda \nhere). The prototype of the function looks as follows.  function(.learner, .task, .subset, .weights = NULL, ...) { }  This function must fit a model on the data of the task  .task  with regard to\nthe subset defined in the integer vector  .subset  and the parameters passed\nin the  ...  arguments. Usually, the data should be extracted from the task\nusing  getTaskData . This will take care of any subsetting as well. It must\nreturn the fitted model.  mlr  assumes no special data type for the return\nvalue -- it will be passed to the predict function we are going to define below,\nso any special code the learner may need can be encapsulated there.  For our example, the definition of the function looks like this. In addition to\nthe data of the task, we also need the formula that describes what to predict.\nWe use the function  getTaskFormula  to extract this from the task.  trainLearner.classif.lda = function(.learner, .task, .subset, .weights,  ...) {\n  f = getTaskFormula(.task)\n  lda(f, data = getTaskData(.task, .subset), ...)\n}  Creating the prediction method  Finally, the prediction function needs to be defined. The name of this function\nstarts with  predictLearner. , followed again by the  mlr  name of the\nlearner. The prototype of the function is as follows.  function(.learner, .model, .newdata, ...) { }  It must predict for the new observations in the  data.frame   .newdata  with\nthe wrapped model  .model , which is returned from the training function.\nThe actual model the learner built is stored in the  $learner.model  member\nand can be accessed simply through  .model$learner.model .  For classification, you have to return a factor of predicted classes if .learner$predict.type  is  \"response\" , or a matrix of predicted\nprobabilities if  .learner$predict.type  is  \"prob\"  and this type of\nprediction is supported by the learner. In the latter case the matrix must have\nthe same number of columns as there are classes in the task and the columns have\nto be named by the class names.  The definition for LDA looks like this. It is pretty much just a straight\npass-through of the arguments to the  predict  function and some extraction of\nprediction data depending on the type of prediction requested.  predictLearner.classif.lda = function(.learner, .model, .newdata, ...) {\n  p = predict(.model$learner.model, newdata = .newdata, ...)\n  if(.learner$predict.type ==  response )\n    return(p$class)\n  else\n    return(p$posterior)\n}", 
            "title": "Classification"
        }, 
        {
            "location": "/create_learner/index.html#regression", 
            "text": "The main difference for regression is that the type of predictions are different\n(numeric instead of labels or probabilities) and that not all of the properties\nare relevant. In particular, whether one-, two-, or multi-class problems and\nposterior probabilities are supported is not applicable.  Apart from this, everything explained above applies. Below is the definition for\nthe  earth  learner from the earth  package.  makeRLearner.regr.earth = function() {\n  makeRLearnerRegr(\n    cl =  regr.earth ,\n    package =  earth ,\n    par.set = makeParamSet(\n      makeIntegerLearnerParam(id =  degree , default = 1L, lower = 1L),\n      makeNumericLearnerParam(id =  penalty ),\n      makeIntegerLearnerParam(id =  nprune )\n    ),\n    properties = c( numerics ,  factors ),\n    name =  Multivariate Adaptive Regression Splines ,\n    short.name =  earth ,\n    note =  \n  )\n}\n\ntrainLearner.regr.earth = function(.learner, .task, .subset, .weights = NULL,  ...) {\n  f = getTaskFormula(.task)\n  earth::earth(f, data = getTaskData(.task, .subset), ...)\n}\n\npredictLearner.regr.earth = function(.learner, .model, .newdata, ...) {\n  predict(.model$learner.model, newdata = .newdata)[, 1L]\n}  Again most of the data is passed straight through to/from the train/predict\nfunctions of the learner.", 
            "title": "Regression"
        }, 
        {
            "location": "/create_learner/index.html#survival-analysis", 
            "text": "For survival analysis, you have to return so-called linear predictors in order to compute\nthe default measure for this task type, the  cindex  (for .learner$predict.type  ==  \"response\" ). For  .learner$predict.type  ==  \"prob\" ,\nthere is no substantially meaningful measure (yet). You may either ignore this case or return\nsomething like predicted survival curves (cf. example below).  There are three properties that are specific to survival learners:\n\"rcens\", \"lcens\" and \"icens\", defining the type(s) of censoring a learner can handle -- right,\nleft and/or interval censored.  Let's have a look at how the  Cox Proportional Hazard Model  from\npackage  survival  has been integrated\ninto the survival learner  surv.coxph  in  mlr  as an example:  makeRLearner.surv.coxph = function() {\n  makeRLearnerSurv(\n    cl =  surv.coxph ,\n    package =  survival ,\n    par.set = makeParamSet(\n      makeDiscreteLearnerParam(id =  ties , default =  efron , values = c( efron ,  breslow ,  exact )),\n      makeLogicalLearnerParam(id =  singular.ok , default = TRUE),\n      makeNumericLearnerParam(id =  eps , default = 1e-09, lower = 0),\n      makeNumericLearnerParam(id =  toler.chol , default = .Machine$double.eps^0.75, lower = 0),\n      makeIntegerLearnerParam(id =  iter.max , default = 20L, lower = 1L),\n      makeNumericLearnerParam(id =  toler.inf , default = sqrt(.Machine$double.eps^0.75), lower = 0),\n      makeIntegerLearnerParam(id =  outer.max , default = 10L, lower = 1L)\n    ),\n    properties = c( missings ,  numerics ,  factors ,  weights ,  prob ,  rcens ),\n    name =  Cox Proportional Hazard Model ,\n    short.name =  coxph ,\n    note =  \n  )\n}\n\ntrainLearner.surv.coxph = function(.learner, .task, .subset, .weights = NULL,  ...) {\n  f = as.formula(getTaskFormulaAsString(.task))\n  data = getTaskData(.task, subset = .subset)\n  if (is.null(.weights)) {\n    mod = survival::coxph(formula = f, data = data, ...)\n  } else  {\n    mod = survival::coxph(formula = f, data = data, weights = .weights, ...)\n  }\n  if (.learner$predict.type ==  prob )\n    mod = attachTrainingInfo(mod, list(surv.range = range(getTaskTargets(.task)[, 1L])))\n  mod\n}\n\npredictLearner.surv.coxph = function(.learner, .model, .newdata, ...) {\n  if(.learner$predict.type ==  response ) {\n    predict(.model$learner.model, newdata = .newdata, type =  lp , ...)\n  } else if (.learner$predict.type ==  prob ) {\n    surv.range = getTrainingInfo(.model$learner.model)$surv.range\n    times = seq(from = surv.range[1L], to = surv.range[2L], length.out = 1000)\n    t(summary(survival::survfit(.model$learner.model, newdata = .newdata, se.fit = FALSE, conf.int = FALSE), times = times)$surv)\n  } else {\n    stop( Unknown predict type )\n  }\n}", 
            "title": "Survival Analysis"
        }, 
        {
            "location": "/create_learner/index.html#clustering", 
            "text": "For clustering, you have to return a numeric vector with the IDs of the clusters\nthat the respective datum has been assigned to. The numbering should start at 1.  Below is the definition for the  FarthestFirst  learner\nfrom the  RWeka  package. Weka\nstarts the IDs of the clusters at 0, so we add 1 to the predicted clusters.\nRWeka has a different way of setting learner parameters; we use the special Weka_control  function to do this.  makeRLearner.cluster.FarthestFirst = function() {\n  makeRLearnerCluster(\n    cl =  cluster.FarthestFirst ,\n    package =  RWeka ,\n    par.set = makeParamSet(\n      makeIntegerLearnerParam(id =  N , default = 2L, lower = 1L),\n      makeIntegerLearnerParam(id =  S , default = 1L, lower = 1L)\n    ),\n    properties = c( numerics ),\n    name =  FarthestFirst Clustering Algorithm ,\n    short.name =  farthestfirst \n  )\n}\n\ntrainLearner.cluster.FarthestFirst = function(.learner, .task, .subset, .weights = NULL,  ...) {\n  ctrl = RWeka::Weka_control(...)\n  RWeka::FarthestFirst(getTaskData(.task, .subset), control = ctrl)\n}\n\npredictLearner.cluster.FarthestFirst = function(.learner, .model, .newdata, ...) {\n  predict(.model$learner.model, .newdata, ...) + 1\n}", 
            "title": "Clustering"
        }, 
        {
            "location": "/create_measure/index.html", 
            "text": "Integrating another Measure\n\n\nIn some cases, you may want to evaluate a prediction with a measure which is not yet implemented\nin \nmlr\n. This could be either a performance measure which is not included in the in \nmlr\n\nlist of \nmeasures\n or a measure that uses a non-standard misclassification cost matrix.\n\n\nConstruct a performance measure\n\n\nThe \nmakeMeasure\n function provides a simple way of constructing your own performance\nmeasure. Below, this is exemplified by an implementation of the mean\nmisclassification error (\nmmce\n) for the iris dataset. We write a simple\nfunction that computes the measure on the basis of the predictions and subsequently wrap it\nin a \nMeasure\n object. Then, we work with it as usual with the\n\nperformance\n function.\nSee the \nR\n documentation of the \nmakeMeasure\n function for details on the\nvarious parameters.\n\n\n## Define the measure\nmy.mmce = function(task, model, pred, feats, extra.args) {\n  tb = table(pred$data$response, pred$data$truth)\n  1 - sum(diag(tb)) / sum(tb)\n}\n\n## Encapsulate the function with a Measure object\nmy.mmce = makeMeasure(id = \nmy.mmce\n, minimize = TRUE, classif = TRUE,\n  allowed.pred.types = \nresponse\n, fun = my.mmce)\n\n## Create classification task and learner\ntask = makeClassifTask(data = iris, target = \nSpecies\n)\nlrn = makeLearner(\nclassif.lda\n)\nmod = train(lrn, task)\npred = predict(mod, newdata= iris)\n\n## Compare predicted and true label with our measure\nperformance(pred, measures = my.mmce)\n\n## Apparently the result coincides with the mlr implementaion\nperformance(pred, measures = mmce)\n\n\n\n\nConstruct a measure for non-standard misclassification costs\n\n\nTo create a measure that involves non-standard misclassification costs you can use\nthe \nmakeCostMeasure\n function. In order to do this, you first need to define the cost\nmatrix you want to use and include all class labels. The cost matrix can then be\nwrapped in a \nMeasure\n object and a prediction can be evaluated as usual with the\n\nperformance\n function. See the \nR\n documentation of the \nmakeCostMeasure\n function for\ndetails on the various parameters.\n\n\n## Create misclassification cost matrix\nmcm = matrix(c(0, 2, 2, 3, 0, 2, 1, 1, 0), ncol = 3)\nrownames(mcm) = colnames(mcm) = c(\nsetosa\n, \nversicolor\n, \nvirginica\n)\n\n## Create classification task and learner\ntask = makeClassifTask(data = iris, target = \nSpecies\n)\nlrn = makeLearner(\nclassif.lda\n)\nmod = train(lrn, task)\npred = predict(mod, newdata = iris)\n\n## Encapsulate the cost matrix in a Measure object\nmy.costs = makeCostMeasure(id = \ncosts\n, minimize = TRUE, costs = mcm, task, aggregate = mean)\n\n## Compare predicted and true label with our measure\nperformance(pred, measures = my.costs)\n\n\n\n\nCreate an aggregation function\n\n\nIt is possible to create your own aggregation function by calling the\ninternal \nmlr\n function \nmakeAggregation\n.\nYou can use internal (not exported) functions of \nR\n packages by prefixing the\nname of the function with the name of the package, i.e., \npackagename:::function()\n.\nIn this case:\n\n\nmlr:::makeAggregation(id = \nsome.name\n,\n  fun = function (task, perf.test, perf.train, measure, group, pred) {\n    ## stuff you want to do with perf.test or perf.train\n  }\n)\n#\n Aggregation function: some.name\n\n\n\n\nRemember: It is important that the head of the function looks exactly as above!\n\nperf.test\n and \nperf.train\n are both numerical vectors containing the measure values.\nIn the usual case (e.g. cross validation), the \nperf.train\n vector is empty.\n\n\nExample: Evaluate the range of measures\n\n\nLet's say you are interested in the range of the obtained measures:\n\n\nmy.range.aggr = mlr:::makeAggregation(id = \ntest.range\n,\n  fun = function (task, perf.test, perf.train, measure, group, pred)\n    diff(range(perf.test))\n)\n\n\n\n\nNow we can run a feature selection based on the first measure in the provided\nlist and see how the other measures turn out.\n\n\nlibrary(ggplot2)\nms1 = mmce\nms2 = setAggregation(ms1, my.range.aggr)\nms1min = setAggregation(ms1, test.min)\nms1max = setAggregation(ms1, test.max)\nrdesc = makeResampleDesc(\nCV\n, iters = 3)\nres = selectFeatures(\nclassif.rpart\n, iris.task, rdesc, measures = list(ms1, ms2, ms1min, ms1max),\n  control = makeFeatSelControlExhaustive(), show.info = FALSE)\nperf.data = as.data.frame(res$opt.path)\np = ggplot(aes(x = mmce.test.mean, y = mmce.test.range, xmax = mmce.test.max, xmin = mmce.test.min,\n  color = as.factor(Sepal.Width), pch = as.factor(Petal.Width)), data = perf.data) +\n  geom_point(size = 4) +\n  geom_errorbarh(height = 0)\nprint(p)", 
            "title": "Create Custom Measures"
        }, 
        {
            "location": "/create_measure/index.html#integrating-another-measure", 
            "text": "In some cases, you may want to evaluate a prediction with a measure which is not yet implemented\nin  mlr . This could be either a performance measure which is not included in the in  mlr \nlist of  measures  or a measure that uses a non-standard misclassification cost matrix.", 
            "title": "Integrating another Measure"
        }, 
        {
            "location": "/create_measure/index.html#construct-a-performance-measure", 
            "text": "The  makeMeasure  function provides a simple way of constructing your own performance\nmeasure. Below, this is exemplified by an implementation of the mean\nmisclassification error ( mmce ) for the iris dataset. We write a simple\nfunction that computes the measure on the basis of the predictions and subsequently wrap it\nin a  Measure  object. Then, we work with it as usual with the performance  function.\nSee the  R  documentation of the  makeMeasure  function for details on the\nvarious parameters.  ## Define the measure\nmy.mmce = function(task, model, pred, feats, extra.args) {\n  tb = table(pred$data$response, pred$data$truth)\n  1 - sum(diag(tb)) / sum(tb)\n}\n\n## Encapsulate the function with a Measure object\nmy.mmce = makeMeasure(id =  my.mmce , minimize = TRUE, classif = TRUE,\n  allowed.pred.types =  response , fun = my.mmce)\n\n## Create classification task and learner\ntask = makeClassifTask(data = iris, target =  Species )\nlrn = makeLearner( classif.lda )\nmod = train(lrn, task)\npred = predict(mod, newdata= iris)\n\n## Compare predicted and true label with our measure\nperformance(pred, measures = my.mmce)\n\n## Apparently the result coincides with the mlr implementaion\nperformance(pred, measures = mmce)", 
            "title": "Construct a performance measure"
        }, 
        {
            "location": "/create_measure/index.html#construct-a-measure-for-non-standard-misclassification-costs", 
            "text": "To create a measure that involves non-standard misclassification costs you can use\nthe  makeCostMeasure  function. In order to do this, you first need to define the cost\nmatrix you want to use and include all class labels. The cost matrix can then be\nwrapped in a  Measure  object and a prediction can be evaluated as usual with the performance  function. See the  R  documentation of the  makeCostMeasure  function for\ndetails on the various parameters.  ## Create misclassification cost matrix\nmcm = matrix(c(0, 2, 2, 3, 0, 2, 1, 1, 0), ncol = 3)\nrownames(mcm) = colnames(mcm) = c( setosa ,  versicolor ,  virginica )\n\n## Create classification task and learner\ntask = makeClassifTask(data = iris, target =  Species )\nlrn = makeLearner( classif.lda )\nmod = train(lrn, task)\npred = predict(mod, newdata = iris)\n\n## Encapsulate the cost matrix in a Measure object\nmy.costs = makeCostMeasure(id =  costs , minimize = TRUE, costs = mcm, task, aggregate = mean)\n\n## Compare predicted and true label with our measure\nperformance(pred, measures = my.costs)", 
            "title": "Construct a measure for non-standard misclassification costs"
        }, 
        {
            "location": "/create_measure/index.html#create-an-aggregation-function", 
            "text": "It is possible to create your own aggregation function by calling the\ninternal  mlr  function  makeAggregation .\nYou can use internal (not exported) functions of  R  packages by prefixing the\nname of the function with the name of the package, i.e.,  packagename:::function() .\nIn this case:  mlr:::makeAggregation(id =  some.name ,\n  fun = function (task, perf.test, perf.train, measure, group, pred) {\n    ## stuff you want to do with perf.test or perf.train\n  }\n)\n#  Aggregation function: some.name  Remember: It is important that the head of the function looks exactly as above! perf.test  and  perf.train  are both numerical vectors containing the measure values.\nIn the usual case (e.g. cross validation), the  perf.train  vector is empty.  Example: Evaluate the range of measures  Let's say you are interested in the range of the obtained measures:  my.range.aggr = mlr:::makeAggregation(id =  test.range ,\n  fun = function (task, perf.test, perf.train, measure, group, pred)\n    diff(range(perf.test))\n)  Now we can run a feature selection based on the first measure in the provided\nlist and see how the other measures turn out.  library(ggplot2)\nms1 = mmce\nms2 = setAggregation(ms1, my.range.aggr)\nms1min = setAggregation(ms1, test.min)\nms1max = setAggregation(ms1, test.max)\nrdesc = makeResampleDesc( CV , iters = 3)\nres = selectFeatures( classif.rpart , iris.task, rdesc, measures = list(ms1, ms2, ms1min, ms1max),\n  control = makeFeatSelControlExhaustive(), show.info = FALSE)\nperf.data = as.data.frame(res$opt.path)\np = ggplot(aes(x = mmce.test.mean, y = mmce.test.range, xmax = mmce.test.max, xmin = mmce.test.min,\n  color = as.factor(Sepal.Width), pch = as.factor(Petal.Width)), data = perf.data) +\n  geom_point(size = 4) +\n  geom_errorbarh(height = 0)\nprint(p)", 
            "title": "Create an aggregation function"
        }, 
        {
            "location": "/create_imputation/index.html", 
            "text": "Creating an Imputation Method\n\n\nFunction \nmakeImputeMethod\n allows to create your own imputation method.\nFor this purpose you need to specify a \nlearn\n function that extracts the necessary\ninformation and an \nimpute\n function that does the actual imputation.\nThe \nlearn\n and \nimpute\n functions both have at least the following arguments:\n\n\n\n\ndata\n is a \ndata frame\n with missing values in some features.\n\n\ncol\n indicates the feature to be imputed.\n\n\ntarget\n indicates the target variable(s) in a supervised learning task.\n\n\n\n\nLet's have a look at function \nimputeMean\n.\n\n\nimputeMean\n#\n function () \n#\n {\n#\n     makeImputeMethod(learn = function(data, target, col) mean(data[[col]], \n#\n         na.rm = TRUE), impute = simpleImpute)\n#\n }\n#\n \nbytecode: 0x7fa7dbbea4e8\n\n#\n \nenvironment: namespace:mlr\n\nmlr:::simpleImpute\n#\n function (data, target, col, const) \n#\n {\n#\n     if (is.na(const)) \n#\n         stopf(\nError imputing column '%s'. Maybe all input data was missing?\n, \n#\n             col)\n#\n     x = data[[col]]\n#\n     if (is.factor(x) \n const %nin% levels(x)) {\n#\n         levels(x) = c(levels(x), as.character(const))\n#\n     }\n#\n     replace(x, is.na(x), const)\n#\n }\n#\n \nbytecode: 0x7fa7dc5fdff8\n\n#\n \nenvironment: namespace:mlr\n\n\n\n\n\nThe \nlearn\n function calculates the mean of the non-missing observations in column \ncol\n.\nThe mean is passed via argument \nconst\n to the \nimpute\n function that replaces all \nNA's\n\nin feature \ncol\n.\n\n\nNow let's write a new imputation method: \nIn case of longitudinal data a frequently used technique is \nlast observation\ncarried forward\n (LOCF) where missing values are replaced by the most recent observed value.\n\n\nIn the \nR\n code below the \nlearn\n function determines the last observed value previous to each \nNA\n (\nvalues\n)\nas well as the corresponding number of consecutive \nNA's\n (\ntimes\n).\nThe \nimpute\n function generates a vector where the entries in \nvalues\n are replicated\naccording to \ntimes\n and replaces the \nNA's\n in feature \ncol\n.\n\n\nimputeLOCF = function() {\n    makeImputeMethod(\n      learn = function(data, target, col) {\n        x = data[[col]]\n        ind = is.na(x)\n        dind = diff(ind)\n        first = which(dind == 1)     # position of the last observed value previous to NA\n        last = which(dind == -1)     # position of the last of potentially several consecutive NA's\n        values = x[first]            # observed value previous to NA\n        times = last - first         # number of consecutive NA's\n        return(list(values = values, times = times))\n      },\n      impute = function(data, target, col, values, times) {\n        x = data[[col]]\n        replace(x, is.na(x), rep(values, times))\n      }\n    )\n}\n\n\n\n\nIn the following the missing values in features \nOzone\n and \nSolar.R\n in the \nairquality\n data set\nare imputed by LOCF.\n\n\ndata(airquality)\nimp = impute(airquality, cols = list(Ozone = imputeLOCF(), Solar.R = imputeLOCF()),\n  dummy.cols = c(\nOzone\n, \nSolar.R\n))\nhead(imp$data, 10)\n#\n    Ozone Solar.R Wind Temp Month Day Ozone.dummy Solar.R.dummy\n#\n 1     41     190  7.4   67     5   1       FALSE         FALSE\n#\n 2     36     118  8.0   72     5   2       FALSE         FALSE\n#\n 3     12     149 12.6   74     5   3       FALSE         FALSE\n#\n 4     18     313 11.5   62     5   4       FALSE         FALSE\n#\n 5     18     313 14.3   56     5   5        TRUE          TRUE\n#\n 6     28     313 14.9   66     5   6       FALSE          TRUE\n#\n 7     23     299  8.6   65     5   7       FALSE         FALSE\n#\n 8     19      99 13.8   59     5   8       FALSE         FALSE\n#\n 9      8      19 20.1   61     5   9       FALSE         FALSE\n#\n 10     8     194  8.6   69     5  10        TRUE         FALSE", 
            "title": "Create an Imputation Method"
        }, 
        {
            "location": "/create_imputation/index.html#creating-an-imputation-method", 
            "text": "Function  makeImputeMethod  allows to create your own imputation method.\nFor this purpose you need to specify a  learn  function that extracts the necessary\ninformation and an  impute  function that does the actual imputation.\nThe  learn  and  impute  functions both have at least the following arguments:   data  is a  data frame  with missing values in some features.  col  indicates the feature to be imputed.  target  indicates the target variable(s) in a supervised learning task.   Let's have a look at function  imputeMean .  imputeMean\n#  function () \n#  {\n#      makeImputeMethod(learn = function(data, target, col) mean(data[[col]], \n#          na.rm = TRUE), impute = simpleImpute)\n#  }\n#   bytecode: 0x7fa7dbbea4e8 \n#   environment: namespace:mlr \nmlr:::simpleImpute\n#  function (data, target, col, const) \n#  {\n#      if (is.na(const)) \n#          stopf( Error imputing column '%s'. Maybe all input data was missing? , \n#              col)\n#      x = data[[col]]\n#      if (is.factor(x)   const %nin% levels(x)) {\n#          levels(x) = c(levels(x), as.character(const))\n#      }\n#      replace(x, is.na(x), const)\n#  }\n#   bytecode: 0x7fa7dc5fdff8 \n#   environment: namespace:mlr   The  learn  function calculates the mean of the non-missing observations in column  col .\nThe mean is passed via argument  const  to the  impute  function that replaces all  NA's \nin feature  col .  Now let's write a new imputation method: \nIn case of longitudinal data a frequently used technique is  last observation\ncarried forward  (LOCF) where missing values are replaced by the most recent observed value.  In the  R  code below the  learn  function determines the last observed value previous to each  NA  ( values )\nas well as the corresponding number of consecutive  NA's  ( times ).\nThe  impute  function generates a vector where the entries in  values  are replicated\naccording to  times  and replaces the  NA's  in feature  col .  imputeLOCF = function() {\n    makeImputeMethod(\n      learn = function(data, target, col) {\n        x = data[[col]]\n        ind = is.na(x)\n        dind = diff(ind)\n        first = which(dind == 1)     # position of the last observed value previous to NA\n        last = which(dind == -1)     # position of the last of potentially several consecutive NA's\n        values = x[first]            # observed value previous to NA\n        times = last - first         # number of consecutive NA's\n        return(list(values = values, times = times))\n      },\n      impute = function(data, target, col, values, times) {\n        x = data[[col]]\n        replace(x, is.na(x), rep(values, times))\n      }\n    )\n}  In the following the missing values in features  Ozone  and  Solar.R  in the  airquality  data set\nare imputed by LOCF.  data(airquality)\nimp = impute(airquality, cols = list(Ozone = imputeLOCF(), Solar.R = imputeLOCF()),\n  dummy.cols = c( Ozone ,  Solar.R ))\nhead(imp$data, 10)\n#     Ozone Solar.R Wind Temp Month Day Ozone.dummy Solar.R.dummy\n#  1     41     190  7.4   67     5   1       FALSE         FALSE\n#  2     36     118  8.0   72     5   2       FALSE         FALSE\n#  3     12     149 12.6   74     5   3       FALSE         FALSE\n#  4     18     313 11.5   62     5   4       FALSE         FALSE\n#  5     18     313 14.3   56     5   5        TRUE          TRUE\n#  6     28     313 14.9   66     5   6       FALSE          TRUE\n#  7     23     299  8.6   65     5   7       FALSE         FALSE\n#  8     19      99 13.8   59     5   8       FALSE         FALSE\n#  9      8      19 20.1   61     5   9       FALSE         FALSE\n#  10     8     194  8.6   69     5  10        TRUE         FALSE", 
            "title": "Creating an Imputation Method"
        }, 
        {
            "location": "/example_tasks/index.html", 
            "text": "Example Tasks\n\n\nFor your convenience \nmlr\n provides pre-defined \nTask\ns for each type of learning problem.\nThese are used throughout this tutorial in order to get shorter and more readable code.\n\n\n\n\n\n\n\n\nTask\n\n\nTitle\n\n\n\n\n\n\n\n\n\n\nagri.task\n\n\nEuropean Union Agricultural Workforces clustering task\n\n\n\n\n\n\nbc.task\n\n\nWisconsin Breast Cancer classification task\n\n\n\n\n\n\nbh.task\n\n\nBoston Housing regression task\n\n\n\n\n\n\ncostiris.task\n\n\nIris cost-sensitive classification task\n\n\n\n\n\n\niris.task\n\n\nIris classification task\n\n\n\n\n\n\nlung.task\n\n\nNCCTG Lung Cancer survival task\n\n\n\n\n\n\nmtcars.task\n\n\nMotor Trend Car Road Tests clustering task\n\n\n\n\n\n\npid.task\n\n\nPimaIndiansDiabetes classification task\n\n\n\n\n\n\nsonar.task\n\n\nSonar classification task\n\n\n\n\n\n\nwpbc.task\n\n\nWisonsin Prognostic Breast Cancer (WPBC) survival task", 
            "title": "Example Tasks"
        }, 
        {
            "location": "/example_tasks/index.html#example-tasks", 
            "text": "For your convenience  mlr  provides pre-defined  Task s for each type of learning problem.\nThese are used throughout this tutorial in order to get shorter and more readable code.     Task  Title      agri.task  European Union Agricultural Workforces clustering task    bc.task  Wisconsin Breast Cancer classification task    bh.task  Boston Housing regression task    costiris.task  Iris cost-sensitive classification task    iris.task  Iris classification task    lung.task  NCCTG Lung Cancer survival task    mtcars.task  Motor Trend Car Road Tests clustering task    pid.task  PimaIndiansDiabetes classification task    sonar.task  Sonar classification task    wpbc.task  Wisonsin Prognostic Breast Cancer (WPBC) survival task", 
            "title": "Example Tasks"
        }, 
        {
            "location": "/integrated_learners/index.html", 
            "text": "Integrated Learners\n\n\nHere, the learning methods already integrated into \nmlr\n are listed.\n\n\nColumns \nNum.\n, \nFac.\n, \nNAs\n, and \nWeights\n indicate if a method can cope with\nnumerical and factor predictors, if \nNA\ns in the data are allowed and if observation\nweights are supported, respectively.\n\n\nColumn \nProps\n shows further properties of the learning methods.\n\nordered\n indicates that a method can deal with ordered factor features.\nFor \nclassification\n, you can see if binary and/or multi-class problems are supported.\nFor \nsurvival analysis\n, the censoring type is shown.\nFor example \nrcens\n means that the learning method can deal with right censored data.\nMoreover, the type of prediction is displayed, where \nprob\n indicates that probabilities\ncan be predicted.\nFor \nregression\n, \nse\n means that standard errors and the mean response can be predicted.\n\n\nClassification (54)\n\n\n\n\n\n\n\n\nID / Short Name\n\n\nName\n\n\nPackages\n\n\nNum.\n\n\nFac.\n\n\nNAs\n\n\nWeights\n\n\nProps\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nclassif.ada\n \nada\n\n\nada Boosting\n\n\nada\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nprob\ntwoclass\n\n\n\n\n\n\n\n\nclassif.bartMachine\n \nbartmachine\n\n\nBayesian Additive Regression Trees\n\n\nbartMachine\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nprob\ntwoclass\n\n\n'use_missing_data' has been set to TRUE by default to allow missing data support\n\n\n\n\n\n\nclassif.bdk\n \nbdk\n\n\nBi-Directional Kohonen map\n\n\nkohonen\n\n\nX\n\n\n\n\n\n\n\n\nmulticlass\nprob\ntwoclass\n\n\n\n\n\n\n\n\nclassif.binomial\n \nbinomial\n\n\nBinomial Regression\n\n\nstats\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nprob\ntwoclass\n\n\nDelegates to glm with freely choosable binomial link function via learner param 'link'.\n\n\n\n\n\n\nclassif.blackboost\n \nblackbst\n\n\nGradient Boosting With Regression Trees\n\n\nmboost\nparty\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nprob\ntwoclass\n\n\nsee ?ctree_control for possible breakage for nominal features with missingness\n\n\n\n\n\n\nclassif.boosting\n \nadabag\n\n\nAdabag Boosting\n\n\nadabag\nrpart\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nmulticlass\nprob\ntwoclass\n\n\nxval\n has been set to 0 by default for speed.\n\n\n\n\n\n\nclassif.bst\n \nbst\n\n\nGradient Boosting\n\n\nbst\n\n\nX\n\n\n\n\n\n\n\n\ntwoclass\n\n\nThe argument \nlearner\n has been renamed to \nLearner\n due to a name conflict with \nsetHyerPars\n. \nLearner\n has been set to \nlm\n by default.\n\n\n\n\n\n\nclassif.cforest\n \ncforest\n\n\nRandom forest based on conditional inference trees\n\n\nparty\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nmulticlass\nordered\nprob\ntwoclass\n\n\nsee ?ctree_control for possible breakage for nominal features with missingness\n\n\n\n\n\n\nclassif.ctree\n \nctree\n\n\nConditional Inference Trees\n\n\nparty\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nmulticlass\nordered\nprob\ntwoclass\n\n\nsee ?ctree_control for possible breakage for nominal features with missingness\n\n\n\n\n\n\nclassif.extraTrees\n \nextraTrees\n\n\nExtremely Randomized Trees\n\n\nextraTrees\n\n\nX\n\n\n\n\n\n\nX\n\n\nmulticlass\nprob\ntwoclass\n\n\n\n\n\n\n\n\nclassif.fnn\n \nfnn\n\n\nFast k-Nearest Neighbour\n\n\nFNN\n\n\nX\n\n\n\n\n\n\n\n\nmulticlass\ntwoclass\n\n\n\n\n\n\n\n\nclassif.gbm\n \ngbm\n\n\nGradient Boosting Machine\n\n\ngbm\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nmulticlass\nprob\ntwoclass\n\n\n\n\n\n\n\n\nclassif.geoDA\n \ngeoda\n\n\nGeometric Predictive Discriminant Analysis\n\n\nDiscriMiner\n\n\nX\n\n\n\n\n\n\n\n\nmulticlass\ntwoclass\n\n\n\n\n\n\n\n\nclassif.glmboost\n \nglmbst\n\n\nBoosting for GLMs\n\n\nmboost\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nprob\ntwoclass\n\n\nfamily\n has been set to \nBinomial()\n by default. Maximum number of boosting iterations is set via 'mstop', the actual number used for prediction is controlled by 'm'.\n\n\n\n\n\n\nclassif.glmnet\n \nglmnet\n\n\nGLM with Lasso or Elasticnet Regularization\n\n\nglmnet\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nmulticlass\nprob\ntwoclass\n\n\nFactors automatically get converted to dummy columns, ordered factors to integer\n\n\n\n\n\n\nclassif.hdrda\n \nhdrda\n\n\nHigh-Dimensional Regularized Discriminant Analysis\n\n\nsparsediscrim\n\n\nX\n\n\n\n\n\n\n\n\nprob\ntwoclass\n\n\n\n\n\n\n\n\nclassif.IBk\n \nibk\n\n\nk-Nearest Neighbours\n\n\nRWeka\n\n\nX\n\n\nX\n\n\n\n\n\n\nmulticlass\nprob\ntwoclass\n\n\n\n\n\n\n\n\nclassif.J48\n \nj48\n\n\nJ48 Decision Trees\n\n\nRWeka\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nmulticlass\nprob\ntwoclass\n\n\nNAs are directly passed to WEKA with \nna.action = na.pass\n\n\n\n\n\n\nclassif.JRip\n \njrip\n\n\nPropositional Rule Learner\n\n\nRWeka\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nmulticlass\nprob\ntwoclass\n\n\nNAs are directly passed to WEKA with \nna.action = na.pass\n\n\n\n\n\n\nclassif.kknn\n \nkknn\n\n\nk-Nearest Neighbor\n\n\nkknn\n\n\nX\n\n\nX\n\n\n\n\n\n\nmulticlass\nprob\ntwoclass\n\n\n\n\n\n\n\n\nclassif.knn\n \nknn\n\n\nk-Nearest Neighbor\n\n\nclass\n\n\nX\n\n\n\n\n\n\n\n\nmulticlass\ntwoclass\n\n\n\n\n\n\n\n\nclassif.ksvm\n \nksvm\n\n\nSupport Vector Machines\n\n\nkernlab\n\n\nX\n\n\nX\n\n\n\n\n\n\nmulticlass\nprob\ntwoclass\n\n\nKernel parameters have to be passed directly and not by using the kpar list in ksvm. Note that \nfit\n has been set to \nFALSE\n by default for speed.\n\n\n\n\n\n\nclassif.lda\n \nlda\n\n\nLinear Discriminant Analysis\n\n\nMASS\n\n\nX\n\n\nX\n\n\n\n\n\n\nmulticlass\nprob\ntwoclass\n\n\nLearner param 'predict.method' maps to 'method' in predict.lda.\n\n\n\n\n\n\nclassif.LiblineaRBinary\n \nliblinearbinary\n\n\nRegularized Binary Linear Predictive Models Estimation\n\n\nLiblineaR\n\n\nX\n\n\n\n\n\n\n\n\ntwoclass\n\n\nThis model subsumes the types 1,2,3,5.\n\n\n\n\n\n\nclassif.LiblineaRLogReg\n \nreglreg\n\n\nRegularized Logistic Regression\n\n\nLiblineaR\n\n\nX\n\n\n\n\n\n\n\n\nprob\ntwoclass\n\n\nThis model subsumes type 0,6,7.\n\n\n\n\n\n\nclassif.LiblineaRMultiClass\n \nmcsvc\n\n\nMulti-class Support Vector Classification by Crammer and Singer\n\n\nLiblineaR\n\n\nX\n\n\n\n\n\n\n\n\nmulticlass\ntwoclass\n\n\nThis model is type 4.\n\n\n\n\n\n\nclassif.linDA\n \nlinda\n\n\nLinear Discriminant Analysis\n\n\nDiscriMiner\n\n\nX\n\n\n\n\n\n\n\n\nmulticlass\ntwoclass\n\n\n\n\n\n\n\n\nclassif.logreg\n \nlogreg\n\n\nLogistic Regression\n\n\nstats\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nprob\ntwoclass\n\n\nDelegates to glm with family binomial/logit.\n\n\n\n\n\n\nclassif.lqa\n \nlqa\n\n\nFitting penalized Generalized Linear Models with the LQA algorithm\n\n\nlqa\n\n\nX\n\n\n\n\n\n\nX\n\n\nprob\ntwoclass\n\n\npenalty\n has been set to \nlasso\n and \nlambda\n to 0.1 by default.\n\n\n\n\n\n\nclassif.lssvm\n \nlssvm\n\n\nLeast Squares Support Vector Machine\n\n\nkernlab\n\n\nX\n\n\nX\n\n\n\n\n\n\nmulticlass\ntwoclass\n\n\nfitted\n has been set to \nFALSE\n by default for speed.\n\n\n\n\n\n\nclassif.lvq1\n \nlvq1\n\n\nLearning Vector Quantization\n\n\nclass\n\n\nX\n\n\n\n\n\n\n\n\nmulticlass\ntwoclass\n\n\n\n\n\n\n\n\nclassif.mda\n \nmda\n\n\nMixture Discriminant Analysis\n\n\nmda\n\n\nX\n\n\nX\n\n\n\n\n\n\nmulticlass\nprob\ntwoclass\n\n\nkeep.fitted\n has been set to \nFALSE\n by default for speed and we use start.method='lvq' for more robust behavior / less technical crashes\n\n\n\n\n\n\nclassif.multinom\n \nmultinom\n\n\nMultinomial Regression\n\n\nnnet\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nmulticlass\nprob\ntwoclass\n\n\n\n\n\n\n\n\nclassif.naiveBayes\n \nnbayes\n\n\nNaive Bayes\n\n\ne1071\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nmulticlass\nprob\ntwoclass\n\n\n\n\n\n\n\n\nclassif.nnet\n \nnnet\n\n\nNeural Network\n\n\nnnet\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nmulticlass\nprob\ntwoclass\n\n\nsize\n has been set to 3 by default.\n\n\n\n\n\n\nclassif.nodeHarvest\n \nnodeHarvest\n\n\nNode Harvest\n\n\nnodeHarvest\n\n\nX\n\n\nX\n\n\n\n\n\n\nprob\ntwoclass\n\n\n\n\n\n\n\n\nclassif.OneR\n \noner\n\n\n1-R Classifier\n\n\nRWeka\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nmulticlass\nprob\ntwoclass\n\n\nNAs are directly passed to WEKA with \nna.action = na.pass\n\n\n\n\n\n\nclassif.pamr\n \npamr\n\n\nNearest shrunken centroid\n\n\npamr\n\n\nX\n\n\n\n\n\n\n\n\nprob\ntwoclass\n\n\nthreshold for prediction (\nthreshold.predict\n) has been set to \n1\n by default\n\n\n\n\n\n\nclassif.PART\n \npart\n\n\nPART Decision Lists\n\n\nRWeka\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nmulticlass\nprob\ntwoclass\n\n\nNAs are directly passed to WEKA with \nna.action = na.pass\n\n\n\n\n\n\nclassif.plr\n \nplr\n\n\nLogistic Regression with a L2 Penalty\n\n\nstepPlr\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nprob\ntwoclass\n\n\nAIC and BIC penalty types can be selected via the new parameter \ncp.type\n\n\n\n\n\n\nclassif.plsdaCaret\n \nplsdacaret\n\n\nPartial Least Squares (PLS) Discriminant Analysis\n\n\ncaret\n\n\nX\n\n\n\n\n\n\n\n\nprob\ntwoclass\n\n\n\n\n\n\n\n\nclassif.probit\n \nprobit\n\n\nProbit Regression\n\n\nstats\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nprob\ntwoclass\n\n\nDelegates to glm with family binomial/probit.\n\n\n\n\n\n\nclassif.qda\n \nqda\n\n\nQuadratic Discriminant Analysis\n\n\nMASS\n\n\nX\n\n\nX\n\n\n\n\n\n\nmulticlass\nprob\ntwoclass\n\n\nLearner param 'predict.method' maps to 'method' in predict.lda.\n\n\n\n\n\n\nclassif.quaDA\n \nquada\n\n\nQuadratic Discriminant Analysis\n\n\nDiscriMiner\n\n\nX\n\n\n\n\n\n\n\n\nmulticlass\ntwoclass\n\n\n\n\n\n\n\n\nclassif.randomForest\n \nrf\n\n\nRandom Forest\n\n\nrandomForest\n\n\nX\n\n\nX\n\n\n\n\n\n\nmulticlass\nordered\nprob\ntwoclass\n\n\n\n\n\n\n\n\nclassif.randomForestSRC\n \nrfsrc\n\n\nRandom Forest\n\n\nrandomForestSRC\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nmulticlass\nprob\ntwoclass\n\n\n'na.action' has been set to 'na.impute' by default to allow missing data support\n\n\n\n\n\n\nclassif.rda\n \nrda\n\n\nRegularized Discriminant Analysis\n\n\nklaR\n\n\nX\n\n\nX\n\n\n\n\n\n\nmulticlass\nprob\ntwoclass\n\n\nestimate.error\n has been set to \nFALSE\n by default for speed.\n\n\n\n\n\n\nclassif.rFerns\n \nrFerns\n\n\nRandom ferns\n\n\nrFerns\n\n\nX\n\n\nX\n\n\n\n\n\n\nmulticlass\nordered\ntwoclass\n\n\n\n\n\n\n\n\nclassif.rpart\n \nrpart\n\n\nDecision Tree\n\n\nrpart\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nmulticlass\nordered\nprob\ntwoclass\n\n\nxval\n has been set to 0 by default for speed.\n\n\n\n\n\n\nclassif.rrlda\n \nrrlda\n\n\nRobust Regularized Linear Discriminant Analysis\n\n\nrrlda\n\n\nX\n\n\n\n\n\n\n\n\nmulticlass\ntwoclass\n\n\n\n\n\n\n\n\nclassif.sda\n \nsda\n\n\nShrinkage Discriminant Analysis\n\n\nsda\n\n\nX\n\n\n\n\n\n\n\n\nmulticlass\nprob\ntwoclass\n\n\n\n\n\n\n\n\nclassif.sparseLDA\n \nsparseLDA\n\n\nSparse Discriminant Analysis\n\n\nsparseLDA\nMASS\nelasticnet\n\n\nX\n\n\n\n\n\n\n\n\nmulticlass\nprob\ntwoclass\n\n\nArguments Q and stop are not yet provided as they depend on the task.\n\n\n\n\n\n\nclassif.svm\n \nsvm\n\n\nSupport Vector Machines (libsvm)\n\n\ne1071\n\n\nX\n\n\nX\n\n\n\n\n\n\nmulticlass\nprob\ntwoclass\n\n\n\n\n\n\n\n\nclassif.xyf\n \nxyf\n\n\nX-Y fused self-organising maps\n\n\nkohonen\n\n\nX\n\n\n\n\n\n\n\n\nmulticlass\nprob\ntwoclass\n\n\n\n\n\n\n\n\n\n\nRegression (45)\n\n\n\n\n\n\n\n\nID / Short Name\n\n\nName\n\n\nPackages\n\n\nNum.\n\n\nFac.\n\n\nNAs\n\n\nWeights\n\n\nProps\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nregr.bartMachine\n \nbartmachine\n\n\nBayesian Additive Regression Trees\n\n\nbartMachine\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n'use_missing_data' has been set to TRUE by default to allow missing data support\n\n\n\n\n\n\nregr.bcart\n \nbcart\n\n\nBayesian CART\n\n\ntgp\n\n\nX\n\n\nX\n\n\n\n\n\n\nse\n\n\n\n\n\n\n\n\nregr.bdk\n \nbdk\n\n\nBi-Directional Kohonen map\n\n\nkohonen\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.bgp\n \nbgp\n\n\nBayesian Gaussian Process\n\n\ntgp\n\n\nX\n\n\n\n\n\n\n\n\nse\n\n\n\n\n\n\n\n\nregr.bgpllm\n \nbgpllm\n\n\nBayesian Gaussian Process with jumps to the Limiting Linear Model\n\n\ntgp\n\n\nX\n\n\n\n\n\n\n\n\nse\n\n\n\n\n\n\n\n\nregr.blackboost\n \nblackbst\n\n\nGradient Boosting with Regression Trees\n\n\nmboost\nparty\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nsee ?ctree_control for possible breakage for nominal features with missingness\n\n\n\n\n\n\nregr.blm\n \nblm\n\n\nBayesian Linear Model\n\n\ntgp\n\n\nX\n\n\n\n\n\n\n\n\nse\n\n\n\n\n\n\n\n\nregr.brnn\n \nbrnn\n\n\nBayesian regularization for feed-forward neural networks\n\n\nbrnn\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.bst\n \nbst\n\n\nGradient Boosting\n\n\nbst\n\n\nX\n\n\n\n\n\n\n\n\n\n\nThe argument \nlearner\n has been renamed to \nLearner\n due to a name conflict with \nsetHyerPars\n\n\n\n\n\n\nregr.btgp\n \nbtgp\n\n\nBayesian Treed Gaussian Process\n\n\ntgp\n\n\nX\n\n\nX\n\n\n\n\n\n\nse\n\n\n\n\n\n\n\n\nregr.btgpllm\n \nbtgpllm\n\n\nBayesian Treed Gaussian Process with jumps to the Limiting Linear Model\n\n\ntgp\n\n\nX\n\n\nX\n\n\n\n\n\n\nse\n\n\n\n\n\n\n\n\nregr.btlm\n \nbtlm\n\n\nBayesian Treed Linear Model\n\n\ntgp\n\n\nX\n\n\nX\n\n\n\n\n\n\nse\n\n\n\n\n\n\n\n\nregr.cforest\n \ncforest\n\n\nRandom Forest Based on Conditional Inference Trees\n\n\nparty\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nordered\n\n\nsee ?ctree_control for possible breakage for nominal features with missingness\n\n\n\n\n\n\nregr.crs\n \ncrs\n\n\nRegression Splines\n\n\ncrs\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nse\n\n\n\n\n\n\n\n\nregr.ctree\n \nctree\n\n\nConditional Inference Trees\n\n\nparty\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nordered\n\n\nsee ?ctree_control for possible breakage for nominal features with missingness\n\n\n\n\n\n\nregr.cubist\n \ncubist\n\n\nCubist\n\n\nCubist\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\nregr.earth\n \nearth\n\n\nMultivariate Adaptive Regression Splines\n\n\nearth\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.elmNN\n \nelmNN\n\n\nExtreme Learning Machine for Single Hidden Layer Feedforward Neural Networks\n\n\nelmNN\n\n\nX\n\n\n\n\n\n\n\n\n\n\nnhid has been set to 1 and actfun has been set to \"sig\" by default\n\n\n\n\n\n\nregr.extraTrees\n \nextraTrees\n\n\nExtremely Randomized Trees\n\n\nextraTrees\n\n\nX\n\n\n\n\n\n\nX\n\n\n\n\n\n\n\n\n\n\nregr.fnn\n \nfnn\n\n\nFast k-Nearest Neighbor\n\n\nFNN\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.frbs\n \nfrbs\n\n\nFuzzy Rule-based Systems\n\n\nfrbs\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.gbm\n \ngbm\n\n\nGradient Boosting Machine\n\n\ngbm\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\ndistribution\n has been set to \ngaussian\n by default.\n\n\n\n\n\n\nregr.glmnet\n \nglmnet\n\n\nGLM with Lasso or Elasticnet Regularization\n\n\nglmnet\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nordered\n\n\nFactors automatically get converted to dummy columns, ordered factors to integer\n\n\n\n\n\n\nregr.IBk\n \nibk\n\n\nK-Nearest Neighbours\n\n\nRWeka\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.kknn\n \nkknn\n\n\nK-Nearest-Neighbor regression\n\n\nkknn\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.km\n \nkm\n\n\nKriging\n\n\nDiceKriging\n\n\nX\n\n\n\n\n\n\n\n\nse\n\n\nIn predict, we currently always use type = 'SK'. The extra param 'jitter' (default is FALSE) enables adding a very small jitter (order 1e-12) to the x-values before prediction, as predict.km reproduces the exact y-values of the training data points, when you pass them in, even if the nugget effect is turned on.\n\n\n\n\n\n\nregr.ksvm\n \nksvm\n\n\nSupport Vector Machines\n\n\nkernlab\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nKernel parameters have to be passed directly and not by using the kpar list in ksvm. Note that \nfit\n has been set to \nFALSE\n by default for speed.\n\n\n\n\n\n\nregr.laGP\n \nlaGP\n\n\nLocal Approximate Gaussian Process\n\n\nlaGP\n\n\nX\n\n\n\n\n\n\n\n\nse\n\n\n\n\n\n\n\n\nregr.lm\n \nlm\n\n\nSimple Linear Regression\n\n\nstats\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nse\n\n\n\n\n\n\n\n\nregr.mars\n \nmars\n\n\nMultivariate Adaptive Regression Splines\n\n\nmda\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.mob\n \nmob\n\n\nModel-based Recursive Partitioning  Yielding a Tree with Fitted Models Associated with each Terminal Node\n\n\nparty\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\n\n\n\n\n\n\nregr.nnet\n \nnnet\n\n\nNeural Network\n\n\nnnet\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\n\n\nsize\n has been set to 3 by default.\n\n\n\n\n\n\nregr.nodeHarvest\n \nnodeHarvest\n\n\nNode Harvest\n\n\nnodeHarvest\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.pcr\n \npcr\n\n\nPrincipal Component Regression\n\n\npls\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nmodel\n has been set to \nFALSE\n by default for speed.\n\n\n\n\n\n\nregr.penalized.lasso\n \nlasso\n\n\nLasso Regression\n\n\npenalized\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.penalized.ridge\n \nridge\n\n\nPenalized Ridge Regression\n\n\npenalized\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.plsr\n \nplsr\n\n\nPartial Least Squares Regression\n\n\npls\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.randomForest\n \nrf\n\n\nRandom Forest\n\n\nrandomForest\n\n\nX\n\n\nX\n\n\n\n\n\n\nordered\nse\n\n\n\n\n\n\n\n\nregr.randomForestSRC\n \nrfsrc\n\n\nRandom Forest\n\n\nrandomForestSRC\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nna.action' has been set to 'na.impute' by default to allow missing data support\n\n\n\n\n\n\nregr.rpart\n \nrpart\n\n\nDecision Tree\n\n\nrpart\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nordered\n\n\nxval\n has been set to 0 by default for speed.\n\n\n\n\n\n\nregr.rsm\n \nrsm\n\n\nResponse Surface Regression\n\n\nrsm\n\n\nX\n\n\n\n\n\n\n\n\n\n\nYou select the order of the regression by using modelfun = \"FO\" (first order), \"TWI\" (two-way interactions, this is with 1st oder terms!) and \"SO\" (full second order)\n\n\n\n\n\n\nregr.rvm\n \nrvm\n\n\nRelevance Vector Machine\n\n\nkernlab\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nKernel parameters have to be passed directly and not by using the kpar list in rvm.   Note that \nfit\n has been set to \nFALSE\n by default for speed.\n\n\n\n\n\n\nregr.slim\n \nslim\n\n\nSparse Linear Regression using Nonsmooth Loss Functions and L1 Regularization\n\n\nflare\n\n\nX\n\n\n\n\n\n\n\n\n\n\nlambda.idx has been set to 3 by default\n\n\n\n\n\n\nregr.svm\n \nsvm\n\n\nSupport Vector Machines (libsvm)\n\n\ne1071\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr.xyf\n \nxyf\n\n\nX-Y fused self-organising maps\n\n\nkohonen\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival analysis (10)\n\n\n\n\n\n\n\n\nID / Short Name\n\n\nName\n\n\nPackages\n\n\nNum.\n\n\nFac.\n\n\nNAs\n\n\nWeights\n\n\nProps\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nsurv.cforest\n \ncrf\n\n\nRandom Forest based on Conditional Inference Trees\n\n\nparty\nsurvival\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nordered\nrcens\n\n\nsee ?ctree_control for possible breakage for nominal features with missingness\n\n\n\n\n\n\nsurv.CoxBoost\n \ncoxboost\n\n\nCox Proportional Hazards Model with Componentwise Likelihood based Boosting\n\n\nCoxBoost\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nordered\nrcens\n\n\nFactors automatically get converted to dummy columns, ordered factors to integer\n\n\n\n\n\n\nsurv.coxph\n \ncoxph\n\n\nCox Proportional Hazard Model\n\n\nsurvival\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nprob\nrcens\n\n\n\n\n\n\n\n\nsurv.cvglmnet\n \ncvglmnet\n\n\nGLM with Regularization (Cross Validated Lambda)\n\n\nglmnet\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nordered\nrcens\n\n\nFactors automatically get converted to dummy columns, ordered factors to integer\n\n\n\n\n\n\nsurv.glmboost\n \nglmboost\n\n\nGradient Boosting with Componentwise Linear Models\n\n\nsurvival\nmboost\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nordered\nrcens\n\n\nfamily\n has been set to \nCoxPH()\n by default. Maximum number of boosting iterations is set via 'mstop', the actual number used for prediction is controlled by 'm'.\n\n\n\n\n\n\nsurv.glmnet\n \nglmnet\n\n\nGLM with Regularization\n\n\nglmnet\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nordered\nrcens\n\n\nFactors automatically get converted to dummy columns, ordered factors to integer\n\n\n\n\n\n\nsurv.optimCoxBoostPenalty\n \noptimCoxBoostPenalty\n\n\nCox Proportional Hazards Model with Componentwise Likelihood based Boosting, automatic tuning enabled\n\n\nCoxBoost\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nrcens\n\n\nFactors automatically get converted to dummy columns, ordered factors to integer\n\n\n\n\n\n\nsurv.penalized\n \npenalized\n\n\nPenalized Regression\n\n\npenalized\n\n\nX\n\n\nX\n\n\n\n\n\n\nordered\nrcens\n\n\nFactors automatically get converted to dummy columns, ordered factors to integer\n\n\n\n\n\n\nsurv.randomForestSRC\n \nrfsrc\n\n\nRandom Forests for Survival\n\n\nsurvival\nrandomForestSRC\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nordered\nrcens\n\n\n'na.action' has been set to 'na.impute' by default to allow missing data support\n\n\n\n\n\n\nsurv.rpart\n \nrpart\n\n\nSurvival Tree\n\n\nrpart\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nordered\nrcens\n\n\nxval\n has been set to 0 by default for speed.\n\n\n\n\n\n\n\n\nCluster analysis (6)\n\n\n\n\n\n\n\n\nID / Short Name\n\n\nName\n\n\nPackages\n\n\nNum.\n\n\nFac.\n\n\nNAs\n\n\nWeights\n\n\nProps\n\n\nNote\n\n\n\n\n\n\n\n\n\n\ncluster.cmeans\n \ncmeans\n\n\nFuzzy C-Means Clustering\n\n\ne1071\nclue\n\n\nX\n\n\n\n\n\n\n\n\nprob\n\n\nThe 'predict' method uses 'cl_predict' from the 'clue' package to compute the cluster memberships for new data. The default 'centers=2' is added so the method runs without setting params, but this must in reality of course be changed by the user.\n\n\n\n\n\n\ncluster.EM\n \nem\n\n\nExpectation-Maximization Clustering\n\n\nRWeka\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncluster.FarthestFirst\n \nfarthestfirst\n\n\nFarthestFirst Clustering Algorithm\n\n\nRWeka\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncluster.kmeans\n \nkmeans\n\n\nK-Means\n\n\nstats\nclue\n\n\nX\n\n\n\n\n\n\n\n\n\n\nThe 'predict' method uses 'cl_predict' from the 'clue' package to compute the cluster memberships for new data. The default 'centers=2' is added so the method runs without setting params, but this must in reality of course be changed by the user.\n\n\n\n\n\n\ncluster.SimpleKMeans\n \nsimplekmeans\n\n\nK-Means Clustering\n\n\nRWeka\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncluster.XMeans\n \nxmeans\n\n\nXMeans (k-means with automatic determination of k)\n\n\nRWeka\n\n\nX\n\n\n\n\n\n\n\n\n\n\nYou may have to install the XMeans Weka package: WPM('install-package', 'XMeans').", 
            "title": "Integrated Learners"
        }, 
        {
            "location": "/integrated_learners/index.html#integrated-learners", 
            "text": "Here, the learning methods already integrated into  mlr  are listed.  Columns  Num. ,  Fac. ,  NAs , and  Weights  indicate if a method can cope with\nnumerical and factor predictors, if  NA s in the data are allowed and if observation\nweights are supported, respectively.  Column  Props  shows further properties of the learning methods. ordered  indicates that a method can deal with ordered factor features.\nFor  classification , you can see if binary and/or multi-class problems are supported.\nFor  survival analysis , the censoring type is shown.\nFor example  rcens  means that the learning method can deal with right censored data.\nMoreover, the type of prediction is displayed, where  prob  indicates that probabilities\ncan be predicted.\nFor  regression ,  se  means that standard errors and the mean response can be predicted.  Classification (54)     ID / Short Name  Name  Packages  Num.  Fac.  NAs  Weights  Props  Note      classif.ada   ada  ada Boosting  ada  X  X   X  prob twoclass     classif.bartMachine   bartmachine  Bayesian Additive Regression Trees  bartMachine  X  X  X   prob twoclass  'use_missing_data' has been set to TRUE by default to allow missing data support    classif.bdk   bdk  Bi-Directional Kohonen map  kohonen  X     multiclass prob twoclass     classif.binomial   binomial  Binomial Regression  stats  X  X   X  prob twoclass  Delegates to glm with freely choosable binomial link function via learner param 'link'.    classif.blackboost   blackbst  Gradient Boosting With Regression Trees  mboost party  X  X  X  X  prob twoclass  see ?ctree_control for possible breakage for nominal features with missingness    classif.boosting   adabag  Adabag Boosting  adabag rpart  X  X  X   multiclass prob twoclass  xval  has been set to 0 by default for speed.    classif.bst   bst  Gradient Boosting  bst  X     twoclass  The argument  learner  has been renamed to  Learner  due to a name conflict with  setHyerPars .  Learner  has been set to  lm  by default.    classif.cforest   cforest  Random forest based on conditional inference trees  party  X  X  X  X  multiclass ordered prob twoclass  see ?ctree_control for possible breakage for nominal features with missingness    classif.ctree   ctree  Conditional Inference Trees  party  X  X  X  X  multiclass ordered prob twoclass  see ?ctree_control for possible breakage for nominal features with missingness    classif.extraTrees   extraTrees  Extremely Randomized Trees  extraTrees  X    X  multiclass prob twoclass     classif.fnn   fnn  Fast k-Nearest Neighbour  FNN  X     multiclass twoclass     classif.gbm   gbm  Gradient Boosting Machine  gbm  X  X  X  X  multiclass prob twoclass     classif.geoDA   geoda  Geometric Predictive Discriminant Analysis  DiscriMiner  X     multiclass twoclass     classif.glmboost   glmbst  Boosting for GLMs  mboost  X  X   X  prob twoclass  family  has been set to  Binomial()  by default. Maximum number of boosting iterations is set via 'mstop', the actual number used for prediction is controlled by 'm'.    classif.glmnet   glmnet  GLM with Lasso or Elasticnet Regularization  glmnet  X  X   X  multiclass prob twoclass  Factors automatically get converted to dummy columns, ordered factors to integer    classif.hdrda   hdrda  High-Dimensional Regularized Discriminant Analysis  sparsediscrim  X     prob twoclass     classif.IBk   ibk  k-Nearest Neighbours  RWeka  X  X    multiclass prob twoclass     classif.J48   j48  J48 Decision Trees  RWeka  X  X  X   multiclass prob twoclass  NAs are directly passed to WEKA with  na.action = na.pass    classif.JRip   jrip  Propositional Rule Learner  RWeka  X  X  X   multiclass prob twoclass  NAs are directly passed to WEKA with  na.action = na.pass    classif.kknn   kknn  k-Nearest Neighbor  kknn  X  X    multiclass prob twoclass     classif.knn   knn  k-Nearest Neighbor  class  X     multiclass twoclass     classif.ksvm   ksvm  Support Vector Machines  kernlab  X  X    multiclass prob twoclass  Kernel parameters have to be passed directly and not by using the kpar list in ksvm. Note that  fit  has been set to  FALSE  by default for speed.    classif.lda   lda  Linear Discriminant Analysis  MASS  X  X    multiclass prob twoclass  Learner param 'predict.method' maps to 'method' in predict.lda.    classif.LiblineaRBinary   liblinearbinary  Regularized Binary Linear Predictive Models Estimation  LiblineaR  X     twoclass  This model subsumes the types 1,2,3,5.    classif.LiblineaRLogReg   reglreg  Regularized Logistic Regression  LiblineaR  X     prob twoclass  This model subsumes type 0,6,7.    classif.LiblineaRMultiClass   mcsvc  Multi-class Support Vector Classification by Crammer and Singer  LiblineaR  X     multiclass twoclass  This model is type 4.    classif.linDA   linda  Linear Discriminant Analysis  DiscriMiner  X     multiclass twoclass     classif.logreg   logreg  Logistic Regression  stats  X  X   X  prob twoclass  Delegates to glm with family binomial/logit.    classif.lqa   lqa  Fitting penalized Generalized Linear Models with the LQA algorithm  lqa  X    X  prob twoclass  penalty  has been set to  lasso  and  lambda  to 0.1 by default.    classif.lssvm   lssvm  Least Squares Support Vector Machine  kernlab  X  X    multiclass twoclass  fitted  has been set to  FALSE  by default for speed.    classif.lvq1   lvq1  Learning Vector Quantization  class  X     multiclass twoclass     classif.mda   mda  Mixture Discriminant Analysis  mda  X  X    multiclass prob twoclass  keep.fitted  has been set to  FALSE  by default for speed and we use start.method='lvq' for more robust behavior / less technical crashes    classif.multinom   multinom  Multinomial Regression  nnet  X  X   X  multiclass prob twoclass     classif.naiveBayes   nbayes  Naive Bayes  e1071  X  X  X   multiclass prob twoclass     classif.nnet   nnet  Neural Network  nnet  X  X   X  multiclass prob twoclass  size  has been set to 3 by default.    classif.nodeHarvest   nodeHarvest  Node Harvest  nodeHarvest  X  X    prob twoclass     classif.OneR   oner  1-R Classifier  RWeka  X  X  X   multiclass prob twoclass  NAs are directly passed to WEKA with  na.action = na.pass    classif.pamr   pamr  Nearest shrunken centroid  pamr  X     prob twoclass  threshold for prediction ( threshold.predict ) has been set to  1  by default    classif.PART   part  PART Decision Lists  RWeka  X  X  X   multiclass prob twoclass  NAs are directly passed to WEKA with  na.action = na.pass    classif.plr   plr  Logistic Regression with a L2 Penalty  stepPlr  X  X   X  prob twoclass  AIC and BIC penalty types can be selected via the new parameter  cp.type    classif.plsdaCaret   plsdacaret  Partial Least Squares (PLS) Discriminant Analysis  caret  X     prob twoclass     classif.probit   probit  Probit Regression  stats  X  X   X  prob twoclass  Delegates to glm with family binomial/probit.    classif.qda   qda  Quadratic Discriminant Analysis  MASS  X  X    multiclass prob twoclass  Learner param 'predict.method' maps to 'method' in predict.lda.    classif.quaDA   quada  Quadratic Discriminant Analysis  DiscriMiner  X     multiclass twoclass     classif.randomForest   rf  Random Forest  randomForest  X  X    multiclass ordered prob twoclass     classif.randomForestSRC   rfsrc  Random Forest  randomForestSRC  X  X  X   multiclass prob twoclass  'na.action' has been set to 'na.impute' by default to allow missing data support    classif.rda   rda  Regularized Discriminant Analysis  klaR  X  X    multiclass prob twoclass  estimate.error  has been set to  FALSE  by default for speed.    classif.rFerns   rFerns  Random ferns  rFerns  X  X    multiclass ordered twoclass     classif.rpart   rpart  Decision Tree  rpart  X  X  X  X  multiclass ordered prob twoclass  xval  has been set to 0 by default for speed.    classif.rrlda   rrlda  Robust Regularized Linear Discriminant Analysis  rrlda  X     multiclass twoclass     classif.sda   sda  Shrinkage Discriminant Analysis  sda  X     multiclass prob twoclass     classif.sparseLDA   sparseLDA  Sparse Discriminant Analysis  sparseLDA MASS elasticnet  X     multiclass prob twoclass  Arguments Q and stop are not yet provided as they depend on the task.    classif.svm   svm  Support Vector Machines (libsvm)  e1071  X  X    multiclass prob twoclass     classif.xyf   xyf  X-Y fused self-organising maps  kohonen  X     multiclass prob twoclass      Regression (45)     ID / Short Name  Name  Packages  Num.  Fac.  NAs  Weights  Props  Note      regr.bartMachine   bartmachine  Bayesian Additive Regression Trees  bartMachine  X  X  X    'use_missing_data' has been set to TRUE by default to allow missing data support    regr.bcart   bcart  Bayesian CART  tgp  X  X    se     regr.bdk   bdk  Bi-Directional Kohonen map  kohonen  X         regr.bgp   bgp  Bayesian Gaussian Process  tgp  X     se     regr.bgpllm   bgpllm  Bayesian Gaussian Process with jumps to the Limiting Linear Model  tgp  X     se     regr.blackboost   blackbst  Gradient Boosting with Regression Trees  mboost party  X  X  X  X   see ?ctree_control for possible breakage for nominal features with missingness    regr.blm   blm  Bayesian Linear Model  tgp  X     se     regr.brnn   brnn  Bayesian regularization for feed-forward neural networks  brnn  X  X        regr.bst   bst  Gradient Boosting  bst  X      The argument  learner  has been renamed to  Learner  due to a name conflict with  setHyerPars    regr.btgp   btgp  Bayesian Treed Gaussian Process  tgp  X  X    se     regr.btgpllm   btgpllm  Bayesian Treed Gaussian Process with jumps to the Limiting Linear Model  tgp  X  X    se     regr.btlm   btlm  Bayesian Treed Linear Model  tgp  X  X    se     regr.cforest   cforest  Random Forest Based on Conditional Inference Trees  party  X  X  X  X  ordered  see ?ctree_control for possible breakage for nominal features with missingness    regr.crs   crs  Regression Splines  crs  X  X   X  se     regr.ctree   ctree  Conditional Inference Trees  party  X  X  X  X  ordered  see ?ctree_control for possible breakage for nominal features with missingness    regr.cubist   cubist  Cubist  Cubist  X  X  X       regr.earth   earth  Multivariate Adaptive Regression Splines  earth  X  X        regr.elmNN   elmNN  Extreme Learning Machine for Single Hidden Layer Feedforward Neural Networks  elmNN  X      nhid has been set to 1 and actfun has been set to \"sig\" by default    regr.extraTrees   extraTrees  Extremely Randomized Trees  extraTrees  X    X      regr.fnn   fnn  Fast k-Nearest Neighbor  FNN  X         regr.frbs   frbs  Fuzzy Rule-based Systems  frbs  X         regr.gbm   gbm  Gradient Boosting Machine  gbm  X  X  X  X   distribution  has been set to  gaussian  by default.    regr.glmnet   glmnet  GLM with Lasso or Elasticnet Regularization  glmnet  X  X   X  ordered  Factors automatically get converted to dummy columns, ordered factors to integer    regr.IBk   ibk  K-Nearest Neighbours  RWeka  X  X        regr.kknn   kknn  K-Nearest-Neighbor regression  kknn  X  X        regr.km   km  Kriging  DiceKriging  X     se  In predict, we currently always use type = 'SK'. The extra param 'jitter' (default is FALSE) enables adding a very small jitter (order 1e-12) to the x-values before prediction, as predict.km reproduces the exact y-values of the training data points, when you pass them in, even if the nugget effect is turned on.    regr.ksvm   ksvm  Support Vector Machines  kernlab  X  X     Kernel parameters have to be passed directly and not by using the kpar list in ksvm. Note that  fit  has been set to  FALSE  by default for speed.    regr.laGP   laGP  Local Approximate Gaussian Process  laGP  X     se     regr.lm   lm  Simple Linear Regression  stats  X  X   X  se     regr.mars   mars  Multivariate Adaptive Regression Splines  mda  X         regr.mob   mob  Model-based Recursive Partitioning  Yielding a Tree with Fitted Models Associated with each Terminal Node  party  X  X   X      regr.nnet   nnet  Neural Network  nnet  X  X   X   size  has been set to 3 by default.    regr.nodeHarvest   nodeHarvest  Node Harvest  nodeHarvest  X  X        regr.pcr   pcr  Principal Component Regression  pls  X  X     model  has been set to  FALSE  by default for speed.    regr.penalized.lasso   lasso  Lasso Regression  penalized  X  X        regr.penalized.ridge   ridge  Penalized Ridge Regression  penalized  X  X        regr.plsr   plsr  Partial Least Squares Regression  pls  X  X        regr.randomForest   rf  Random Forest  randomForest  X  X    ordered se     regr.randomForestSRC   rfsrc  Random Forest  randomForestSRC  X  X  X    na.action' has been set to 'na.impute' by default to allow missing data support    regr.rpart   rpart  Decision Tree  rpart  X  X  X  X  ordered  xval  has been set to 0 by default for speed.    regr.rsm   rsm  Response Surface Regression  rsm  X      You select the order of the regression by using modelfun = \"FO\" (first order), \"TWI\" (two-way interactions, this is with 1st oder terms!) and \"SO\" (full second order)    regr.rvm   rvm  Relevance Vector Machine  kernlab  X  X     Kernel parameters have to be passed directly and not by using the kpar list in rvm.   Note that  fit  has been set to  FALSE  by default for speed.    regr.slim   slim  Sparse Linear Regression using Nonsmooth Loss Functions and L1 Regularization  flare  X      lambda.idx has been set to 3 by default    regr.svm   svm  Support Vector Machines (libsvm)  e1071  X  X        regr.xyf   xyf  X-Y fused self-organising maps  kohonen  X          Survival analysis (10)     ID / Short Name  Name  Packages  Num.  Fac.  NAs  Weights  Props  Note      surv.cforest   crf  Random Forest based on Conditional Inference Trees  party survival  X  X  X  X  ordered rcens  see ?ctree_control for possible breakage for nominal features with missingness    surv.CoxBoost   coxboost  Cox Proportional Hazards Model with Componentwise Likelihood based Boosting  CoxBoost  X  X   X  ordered rcens  Factors automatically get converted to dummy columns, ordered factors to integer    surv.coxph   coxph  Cox Proportional Hazard Model  survival  X  X  X  X  prob rcens     surv.cvglmnet   cvglmnet  GLM with Regularization (Cross Validated Lambda)  glmnet  X  X   X  ordered rcens  Factors automatically get converted to dummy columns, ordered factors to integer    surv.glmboost   glmboost  Gradient Boosting with Componentwise Linear Models  survival mboost  X  X   X  ordered rcens  family  has been set to  CoxPH()  by default. Maximum number of boosting iterations is set via 'mstop', the actual number used for prediction is controlled by 'm'.    surv.glmnet   glmnet  GLM with Regularization  glmnet  X  X   X  ordered rcens  Factors automatically get converted to dummy columns, ordered factors to integer    surv.optimCoxBoostPenalty   optimCoxBoostPenalty  Cox Proportional Hazards Model with Componentwise Likelihood based Boosting, automatic tuning enabled  CoxBoost  X  X   X  rcens  Factors automatically get converted to dummy columns, ordered factors to integer    surv.penalized   penalized  Penalized Regression  penalized  X  X    ordered rcens  Factors automatically get converted to dummy columns, ordered factors to integer    surv.randomForestSRC   rfsrc  Random Forests for Survival  survival randomForestSRC  X  X  X   ordered rcens  'na.action' has been set to 'na.impute' by default to allow missing data support    surv.rpart   rpart  Survival Tree  rpart  X  X  X  X  ordered rcens  xval  has been set to 0 by default for speed.     Cluster analysis (6)     ID / Short Name  Name  Packages  Num.  Fac.  NAs  Weights  Props  Note      cluster.cmeans   cmeans  Fuzzy C-Means Clustering  e1071 clue  X     prob  The 'predict' method uses 'cl_predict' from the 'clue' package to compute the cluster memberships for new data. The default 'centers=2' is added so the method runs without setting params, but this must in reality of course be changed by the user.    cluster.EM   em  Expectation-Maximization Clustering  RWeka  X         cluster.FarthestFirst   farthestfirst  FarthestFirst Clustering Algorithm  RWeka  X         cluster.kmeans   kmeans  K-Means  stats clue  X      The 'predict' method uses 'cl_predict' from the 'clue' package to compute the cluster memberships for new data. The default 'centers=2' is added so the method runs without setting params, but this must in reality of course be changed by the user.    cluster.SimpleKMeans   simplekmeans  K-Means Clustering  RWeka  X         cluster.XMeans   xmeans  XMeans (k-means with automatic determination of k)  RWeka  X      You may have to install the XMeans Weka package: WPM('install-package', 'XMeans').", 
            "title": "Integrated Learners"
        }, 
        {
            "location": "/measures/index.html", 
            "text": "Implemented Performance Measures\n\n\nThe following tables show the performance measures available for the different types of\nlearning problems as well as general performance measures in alphabetical order.\n(See also the documentation about \nmeasures\n and \nmakeMeasure\n for available measures and\ntheir properties.)\n\n\nColumn \nMinimize\n indicates if the measure is minimized during, e.g., tuning or\nfeature selection.\n\nBest\n and \nWorst\n show the best and worst values the performance measure can attain.\nFor \nclassification\n, column \nMultiClass\n indicates if a measure is suitable for\nmulti-class problems. If not, the measure can only be used for binary classification problems.\n\n\nThe next six columns refer to information required to calculate the performance measure.\n\n\n\n\nPrediction\n: The \nPrediction\n object.\n\n\nTruth\n: The true values of the response variable(s) (for supervised learning).\n\n\nProbs\n: The predicted probabilities (might be needed for classification).\n\n\nModel\n: The \nWrappedModel\n (e.g., for calculating the training time).\n\n\nTask\n: The \nTask\n (relevant for cost-sensitive classification).\n\n\nFeats\n: The predicted data (relevant for clustering).\n\n\n\n\nAggregation\n shows the default \naggregation method\n tied to the measure.\n\n\nClassification\n\n\n\n\n\n\n\n\nMeasure\n\n\nNote\n\n\nMinimize\n\n\nBest\n\n\nWorst\n\n\nMultiClass\n\n\nPrediction\n\n\nTruth\n\n\nProbs\n\n\nModel\n\n\nTask\n\n\nFeats\n\n\nAggregation\n\n\n\n\n\n\n\n\n\n\nacc\n - Accuracy\n\n\n\n\n\n\n1\n\n\n0\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nauc\n - Area under the curve\n\n\n\n\n\n\n1\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nbac\n - Balanced accuracy\n\n\nMean of true positive rate and true negative rate.\n\n\n\n\n1\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nber\n - Balanced error rate\n\n\nMean of misclassification error rates on all individual classes.\n\n\nX\n\n\n0\n\n\n1\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nbrier\n - Brier score\n\n\n\n\nX\n\n\n0\n\n\n1\n\n\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nf1\n - F1 measure\n\n\n\n\n\n\n1\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nfdr\n - False discovery rate\n\n\n\n\nX\n\n\n0\n\n\n1\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nfn\n - False negatives\n\n\nAlso called misses.\n\n\nX\n\n\n0\n\n\nInf\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nfnr\n - False negative rate\n\n\n\n\nX\n\n\n0\n\n\n1\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nfp\n - False positives\n\n\nAlso called false alarms.\n\n\nX\n\n\n0\n\n\nInf\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nfpr\n - False positive rate\n\n\nAlso called false alarm rate or fall-out.\n\n\nX\n\n\n0\n\n\n1\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\ngmean\n - G-mean\n\n\nGeometric mean of recall and specificity.\n\n\n\n\n1\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\ngpr\n - Geometric mean of precision and recall\n\n\n\n\n\n\n1\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nmcc\n - Matthews correlation coefficient\n\n\n\n\n\n\n1\n\n\n-1\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nmmce\n - Mean misclassification error\n\n\n\n\nX\n\n\n0\n\n\n1\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nmulticlass.auc\n - Multiclass area under the curve\n\n\nCalls \npROC::multiclass.roc\n.\n\n\n\n\n1\n\n\n0\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nnpv\n - Negative predictive value\n\n\n\n\n\n\n1\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nppv\n - Positive predictive value\n\n\nAlso called precision.\n\n\n\n\n1\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\ntn\n - True negatives\n\n\nAlso called correct rejections.\n\n\n\n\nInf\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\ntnr\n - True negative rate\n\n\nAlso called specificity.\n\n\n\n\n1\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\ntp\n - True positives\n\n\n\n\n\n\nInf\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\ntpr\n - True positive rate\n\n\nAlso called hit rate or recall.\n\n\n\n\n1\n\n\n0\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\n\n\nRegression\n\n\n\n\n\n\n\n\nMeasure\n\n\nNote\n\n\nMinimize\n\n\nBest\n\n\nWorst\n\n\nPrediction\n\n\nTruth\n\n\nProbs\n\n\nModel\n\n\nTask\n\n\nFeats\n\n\nAggregation\n\n\n\n\n\n\n\n\n\n\nmae\n - Mean of absolute errors\n\n\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nmedae\n - Median of absolute errors\n\n\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nmedse\n - Median of squared errors\n\n\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nmse\n - Mean of squared errors\n\n\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nrmse\n - Root mean square error\n\n\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.sqrt.of.mean\n\n\n\n\n\n\nsae\n - Sum of absolute errors\n\n\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\nsse\n - Sum of squared errors\n\n\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\n\n\nSurvival analysis\n\n\n\n\n\n\n\n\nMeasure\n\n\nNote\n\n\nMinimize\n\n\nBest\n\n\nWorst\n\n\nPrediction\n\n\nTruth\n\n\nProbs\n\n\nModel\n\n\nTask\n\n\nFeats\n\n\nAggregation\n\n\n\n\n\n\n\n\n\n\ncindex\n - Concordance index\n\n\n\n\n\n\n1\n\n\n0\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\n\n\nCluster analysis\n\n\n\n\n\n\n\n\nMeasure\n\n\nNote\n\n\nMinimize\n\n\nBest\n\n\nWorst\n\n\nPrediction\n\n\nTruth\n\n\nProbs\n\n\nModel\n\n\nTask\n\n\nFeats\n\n\nAggregation\n\n\n\n\n\n\n\n\n\n\ndb\n - Davies-Bouldin cluster separation measure\n\n\nSee \n?clusterSim::index.DB\n.\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\n\n\n\n\n\n\n\n\nX\n\n\ntest.mean\n\n\n\n\n\n\ndunn\n - Dunn index\n\n\nSee \n?clValid::dunn\n.\n\n\n\n\nInf\n\n\n0\n\n\nX\n\n\n\n\n\n\n\n\n\n\nX\n\n\ntest.mean\n\n\n\n\n\n\nG1\n - Calinski-Harabasz pseudo F statistic\n\n\nSee \n?clusterSim::index.G1\n.\n\n\n\n\nInf\n\n\n0\n\n\nX\n\n\n\n\n\n\n\n\n\n\nX\n\n\ntest.mean\n\n\n\n\n\n\nG2\n - Baker and Hubert adaptation of Goodman-Kruskal's gamma statistic\n\n\nSee \n?clusterSim::index.G2\n.\n\n\n\n\nInf\n\n\n0\n\n\nX\n\n\n\n\n\n\n\n\n\n\nX\n\n\ntest.mean\n\n\n\n\n\n\nsilhouette\n - Rousseeuw's silhouette internal cluster quality index\n\n\nSee \n?clusterSim::index.S\n.\n\n\n\n\nInf\n\n\n0\n\n\nX\n\n\n\n\n\n\n\n\n\n\nX\n\n\ntest.mean\n\n\n\n\n\n\n\n\nCost-sensitive classification\n\n\n\n\n\n\n\n\nMeasure\n\n\nNote\n\n\nMinimize\n\n\nBest\n\n\nWorst\n\n\nPrediction\n\n\nTruth\n\n\nProbs\n\n\nModel\n\n\nTask\n\n\nFeats\n\n\nAggregation\n\n\n\n\n\n\n\n\n\n\nmcp\n - Misclassification penalty\n\n\nAverage difference between costs of oracle and model prediction.\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\n\n\n\n\n\n\nX\n\n\n\n\ntest.mean\n\n\n\n\n\n\nmeancosts\n - Mean costs of the predicted choices\n\n\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\n\n\n\n\n\n\nX\n\n\n\n\ntest.mean\n\n\n\n\n\n\n\n\nGeneral performance measures\n\n\n\n\n\n\n\n\nMeasure\n\n\nNote\n\n\nMinimize\n\n\nBest\n\n\nWorst\n\n\nPrediction\n\n\nTruth\n\n\nProbs\n\n\nModel\n\n\nTask\n\n\nFeats\n\n\nAggregation\n\n\n\n\n\n\n\n\n\n\nfeatperc\n - Percentage of original features used for model\n\n\nUseful for feature selection.\n\n\nX\n\n\n0\n\n\n1\n\n\nX\n\n\n\n\n\n\nX\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\ntimeboth\n - timetrain + timepredict\n\n\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\n\n\n\n\nX\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\ntimepredict\n - Time of predicting test set\n\n\n\n\nX\n\n\n0\n\n\nInf\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\ntest.mean\n\n\n\n\n\n\ntimetrain\n - Time of fitting the model\n\n\n\n\nX\n\n\n0\n\n\nInf\n\n\n\n\n\n\n\n\nX\n\n\n\n\n\n\ntest.mean", 
            "title": "Implemented Performance Measures"
        }, 
        {
            "location": "/measures/index.html#implemented-performance-measures", 
            "text": "The following tables show the performance measures available for the different types of\nlearning problems as well as general performance measures in alphabetical order.\n(See also the documentation about  measures  and  makeMeasure  for available measures and\ntheir properties.)  Column  Minimize  indicates if the measure is minimized during, e.g., tuning or\nfeature selection. Best  and  Worst  show the best and worst values the performance measure can attain.\nFor  classification , column  MultiClass  indicates if a measure is suitable for\nmulti-class problems. If not, the measure can only be used for binary classification problems.  The next six columns refer to information required to calculate the performance measure.   Prediction : The  Prediction  object.  Truth : The true values of the response variable(s) (for supervised learning).  Probs : The predicted probabilities (might be needed for classification).  Model : The  WrappedModel  (e.g., for calculating the training time).  Task : The  Task  (relevant for cost-sensitive classification).  Feats : The predicted data (relevant for clustering).   Aggregation  shows the default  aggregation method  tied to the measure.  Classification     Measure  Note  Minimize  Best  Worst  MultiClass  Prediction  Truth  Probs  Model  Task  Feats  Aggregation      acc  - Accuracy    1  0  X  X  X      test.mean    auc  - Area under the curve    1  0   X  X  X     test.mean    bac  - Balanced accuracy  Mean of true positive rate and true negative rate.   1  0   X  X      test.mean    ber  - Balanced error rate  Mean of misclassification error rates on all individual classes.  X  0  1  X  X  X      test.mean    brier  - Brier score   X  0  1   X  X  X     test.mean    f1  - F1 measure    1  0   X  X      test.mean    fdr  - False discovery rate   X  0  1   X  X      test.mean    fn  - False negatives  Also called misses.  X  0  Inf   X  X      test.mean    fnr  - False negative rate   X  0  1   X  X      test.mean    fp  - False positives  Also called false alarms.  X  0  Inf   X  X      test.mean    fpr  - False positive rate  Also called false alarm rate or fall-out.  X  0  1   X  X      test.mean    gmean  - G-mean  Geometric mean of recall and specificity.   1  0   X  X      test.mean    gpr  - Geometric mean of precision and recall    1  0   X  X      test.mean    mcc  - Matthews correlation coefficient    1  -1   X  X      test.mean    mmce  - Mean misclassification error   X  0  1  X  X  X      test.mean    multiclass.auc  - Multiclass area under the curve  Calls  pROC::multiclass.roc .   1  0  X  X  X  X     test.mean    npv  - Negative predictive value    1  0   X  X      test.mean    ppv  - Positive predictive value  Also called precision.   1  0   X  X      test.mean    tn  - True negatives  Also called correct rejections.   Inf  0   X  X      test.mean    tnr  - True negative rate  Also called specificity.   1  0   X  X      test.mean    tp  - True positives    Inf  0   X  X      test.mean    tpr  - True positive rate  Also called hit rate or recall.   1  0   X  X      test.mean     Regression     Measure  Note  Minimize  Best  Worst  Prediction  Truth  Probs  Model  Task  Feats  Aggregation      mae  - Mean of absolute errors   X  0  Inf  X  X      test.mean    medae  - Median of absolute errors   X  0  Inf  X  X      test.mean    medse  - Median of squared errors   X  0  Inf  X  X      test.mean    mse  - Mean of squared errors   X  0  Inf  X  X      test.mean    rmse  - Root mean square error   X  0  Inf  X  X      test.sqrt.of.mean    sae  - Sum of absolute errors   X  0  Inf  X  X      test.mean    sse  - Sum of squared errors   X  0  Inf  X  X      test.mean     Survival analysis     Measure  Note  Minimize  Best  Worst  Prediction  Truth  Probs  Model  Task  Feats  Aggregation      cindex  - Concordance index    1  0  X  X      test.mean     Cluster analysis     Measure  Note  Minimize  Best  Worst  Prediction  Truth  Probs  Model  Task  Feats  Aggregation      db  - Davies-Bouldin cluster separation measure  See  ?clusterSim::index.DB .  X  0  Inf  X      X  test.mean    dunn  - Dunn index  See  ?clValid::dunn .   Inf  0  X      X  test.mean    G1  - Calinski-Harabasz pseudo F statistic  See  ?clusterSim::index.G1 .   Inf  0  X      X  test.mean    G2  - Baker and Hubert adaptation of Goodman-Kruskal's gamma statistic  See  ?clusterSim::index.G2 .   Inf  0  X      X  test.mean    silhouette  - Rousseeuw's silhouette internal cluster quality index  See  ?clusterSim::index.S .   Inf  0  X      X  test.mean     Cost-sensitive classification     Measure  Note  Minimize  Best  Worst  Prediction  Truth  Probs  Model  Task  Feats  Aggregation      mcp  - Misclassification penalty  Average difference between costs of oracle and model prediction.  X  0  Inf  X     X   test.mean    meancosts  - Mean costs of the predicted choices   X  0  Inf  X     X   test.mean     General performance measures     Measure  Note  Minimize  Best  Worst  Prediction  Truth  Probs  Model  Task  Feats  Aggregation      featperc  - Percentage of original features used for model  Useful for feature selection.  X  0  1  X    X    test.mean    timeboth  - timetrain + timepredict   X  0  Inf  X    X    test.mean    timepredict  - Time of predicting test set   X  0  Inf  X       test.mean    timetrain  - Time of fitting the model   X  0  Inf     X    test.mean", 
            "title": "Implemented Performance Measures"
        }, 
        {
            "location": "/filter_methods/index.html", 
            "text": "Integrated Filter Methods\n\n\nThe following table shows the available methods for calculating the feature importance.\nColumns \nClassif\n, \nRegr\n and \nSurv\n indicate if classification, regression or survival\nanalysis problems are supported.\nColumns \nFac.\n and \nNum.\n show if a particular method can deal with factor and\nnumeric features.\n\n\n\n\n\n\n\n\nID\n\n\nPackage\n\n\nDescription\n\n\nClassif\n\n\nRegr\n\n\nSurv\n\n\nFac.\n\n\nNum.\n\n\n\n\n\n\n\n\n\n\nanova.test\n\n\n\n\nANOVA Test for binary and multiclass classification tasks\n\n\nX\n\n\n\n\n\n\n\n\nX\n\n\n\n\n\n\ncarscore\n\n\ncare\n\n\nCAR scores\n\n\n\n\nX\n\n\n\n\n\n\nX\n\n\n\n\n\n\ncforest.importance\n\n\nparty\n\n\nPermutation importance of random forest fitted in package 'party'\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nchi.squared\n\n\nFSelector\n\n\nChi-squared statistic of independence between feature and target\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\ngain.ratio\n\n\nFSelector\n\n\nEntropy-based gain ratio between feature and target\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\ninformation.gain\n\n\nFSelector\n\n\nEntropy-based information gain between feature and target\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\nkruskal.test\n\n\n\n\nKurskal Test for binary and multiclass classification tasks\n\n\nX\n\n\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\nlinear.correlation\n\n\nFSelector\n\n\nPearson correlation between feature and target\n\n\n\n\nX\n\n\n\n\n\n\nX\n\n\n\n\n\n\nmrmr\n\n\nmRMRe\n\n\nMinimum redundancy, maximum relevance filter\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\noneR\n\n\nFSelector\n\n\noneR assocation rule\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\nrank.correlation\n\n\nFSelector\n\n\nSpearman's correlation between feature and target\n\n\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\nrelief\n\n\nFSelector\n\n\nRELIEF algorithm\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\nrf.importance\n\n\nrandomForestSRC\n\n\nImportance of random forests\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nrf.min.depth\n\n\nrandomForestSRC\n\n\nMinimal depth of random forest fitted in package 'randomForestSRC'\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nsymmetrical.uncertainty\n\n\nFSelector\n\n\nEntropy-based symmetrical uncertainty between feature and target\n\n\nX\n\n\nX\n\n\n\n\nX\n\n\nX\n\n\n\n\n\n\nunivariate\n\n\n\n\nConstruct a simple performance filter using a mlr learner\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nvariance\n\n\n\n\nA simple variance filter\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\nX", 
            "title": "Integrated Filter Methods"
        }, 
        {
            "location": "/filter_methods/index.html#integrated-filter-methods", 
            "text": "The following table shows the available methods for calculating the feature importance.\nColumns  Classif ,  Regr  and  Surv  indicate if classification, regression or survival\nanalysis problems are supported.\nColumns  Fac.  and  Num.  show if a particular method can deal with factor and\nnumeric features.     ID  Package  Description  Classif  Regr  Surv  Fac.  Num.      anova.test   ANOVA Test for binary and multiclass classification tasks  X     X    carscore  care  CAR scores   X    X    cforest.importance  party  Permutation importance of random forest fitted in package 'party'  X  X  X  X  X    chi.squared  FSelector  Chi-squared statistic of independence between feature and target  X  X   X  X    gain.ratio  FSelector  Entropy-based gain ratio between feature and target  X  X   X  X    information.gain  FSelector  Entropy-based information gain between feature and target  X  X   X  X    kruskal.test   Kurskal Test for binary and multiclass classification tasks  X    X  X    linear.correlation  FSelector  Pearson correlation between feature and target   X    X    mrmr  mRMRe  Minimum redundancy, maximum relevance filter  X  X  X  X  X    oneR  FSelector  oneR assocation rule  X  X   X  X    rank.correlation  FSelector  Spearman's correlation between feature and target   X   X  X    relief  FSelector  RELIEF algorithm  X  X   X  X    rf.importance  randomForestSRC  Importance of random forests  X  X  X  X  X    rf.min.depth  randomForestSRC  Minimal depth of random forest fitted in package 'randomForestSRC'  X  X  X  X  X    symmetrical.uncertainty  FSelector  Entropy-based symmetrical uncertainty between feature and target  X  X   X  X    univariate   Construct a simple performance filter using a mlr learner  X  X  X  X  X    variance   A simple variance filter  X  X  X   X", 
            "title": "Integrated Filter Methods"
        }
    ]
}